[{"content":"TL; DR 本篇文章記錄使用 bitnami helm chart 安裝 kafka Kraft mode 的對外集群。\n準備 values.yaml 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 global: storageClass: ceph-csi-rbd-hdd heapOpts: \u0026#34;-Xmx6g -Xms6g\u0026#34; listeners: interbroker: name: INTERNAL containerPort: 9092 protocol: PLAINTEXT controller: name: CONTROLLER containerPort: 9093 protocol: PLAINTEXT client: name: CLIENT containerPort: 9095 protocol: PLAINTEXT external: containerPort: 9094 protocol: PLAINTEXT name: EXTERNAL controller: replicaCount: 3 persistence: size: 50Gi broker: replicaCount: 3 persistence: size: 300Gi externalAccess: enabled: true controller: forceExpose: false service: type: NodePort ports: external: 9094 nodePorts: - 30494 - 30594 - 30694 useHostIPs: true broker: service: type: NodePort ports: external: 9094 nodePorts: - 30194 - 30294 - 30394 useHostIPs: true volumePermissions: enabled: true rbac: create: true kraft: clusterId: M2VhY2Q3NGQ0NGYzNDg2YW 部署 1 helm upgrade --install -name kafka bitnami/kafka --namespace kafka -f values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 Kafka can be accessed by consumers via port 9095 on the following DNS name from within your cluster: kafka.kafka.svc.cluster.local Each Kafka broker can be accessed by producers via port 9095 on the following DNS name(s) from within your cluster: kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-1.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-broker-0.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-1.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-2.kafka-broker-headless.kafka.svc.cluster.local:9095 為 Kafka 設定 Load Balance 部署完畢後可以看到每個 broker / controller 皆使用 NodePort 對外開放，可以為 kafka 設定 Load Balance 以提供外部存取的 client 透過統一的入口點存取。\n目前 Kubernetes 外部已經為集群建立一個 Nginx，我們可以直接使用這台 server 也為 Kafka 的 TCP 流量設置負載轉發。其設定檔如下，將所有會被開到 Node Port 的 Worker Node 節點一次設置上去：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 upstream tcp9094 { server 172.20.37.36:30194 max_fails=3 fail_timeout=30s; server 172.20.37.36:30294 max_fails=3 fail_timeout=30s; server 172.20.37.36:30394 max_fails=3 fail_timeout=30s; server 172.20.37.36:30494 max_fails=3 fail_timeout=30s; server 172.20.37.36:30594 max_fails=3 fail_timeout=30s; server 172.20.37.36:30694 max_fails=3 fail_timeout=30s; server 172.20.37.37:30194 max_fails=3 fail_timeout=30s; server 172.20.37.37:30294 max_fails=3 fail_timeout=30s; server 172.20.37.37:30394 max_fails=3 fail_timeout=30s; server 172.20.37.37:30494 max_fails=3 fail_timeout=30s; server 172.20.37.37:30594 max_fails=3 fail_timeout=30s; server 172.20.37.37:30694 max_fails=3 fail_timeout=30s; server 172.20.37.38:30194 max_fails=3 fail_timeout=30s; server 172.20.37.38:30294 max_fails=3 fail_timeout=30s; server 172.20.37.38:30394 max_fails=3 fail_timeout=30s; server 172.20.37.38:30494 max_fails=3 fail_timeout=30s; server 172.20.37.38:30594 max_fails=3 fail_timeout=30s; server 172.20.37.38:30694 max_fails=3 fail_timeout=30s; server 172.20.37.39:30194 max_fails=3 fail_timeout=30s; server 172.20.37.39:30294 max_fails=3 fail_timeout=30s; server 172.20.37.39:30394 max_fails=3 fail_timeout=30s; server 172.20.37.39:30494 max_fails=3 fail_timeout=30s; server 172.20.37.39:30594 max_fails=3 fail_timeout=30s; server 172.20.37.39:30694 max_fails=3 fail_timeout=30s; server 172.20.37.40:30194 max_fails=3 fail_timeout=30s; server 172.20.37.40:30294 max_fails=3 fail_timeout=30s; server 172.20.37.40:30394 max_fails=3 fail_timeout=30s; server 172.20.37.40:30494 max_fails=3 fail_timeout=30s; server 172.20.37.40:30594 max_fails=3 fail_timeout=30s; server 172.20.37.40:30694 max_fails=3 fail_timeout=30s; server 172.20.37.41:30194 max_fails=3 fail_timeout=30s; server 172.20.37.41:30294 max_fails=3 fail_timeout=30s; server 172.20.37.41:30394 max_fails=3 fail_timeout=30s; server 172.20.37.41:30494 max_fails=3 fail_timeout=30s; server 172.20.37.41:30594 max_fails=3 fail_timeout=30s; server 172.20.37.41:30694 max_fails=3 fail_timeout=30s; server 172.20.37.42:30194 max_fails=3 fail_timeout=30s; server 172.20.37.42:30294 max_fails=3 fail_timeout=30s; server 172.20.37.42:30394 max_fails=3 fail_timeout=30s; server 172.20.37.42:30494 max_fails=3 fail_timeout=30s; server 172.20.37.42:30594 max_fails=3 fail_timeout=30s; server 172.20.37.42:30694 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } 部署 Kafka UI 1 2 3 4 5 6 7 8 9 10 11 yamlApplicationConfig: kafka: clusters: - name: platform bootstrapServers: kafka-broker-headless:9092 auth: type: disabled management: health: ldap: enabled: false 1 helm install -name kafka-ui kafka-ui/kafka-ui -f values.yaml --namespace kafka ","date":"2024-06-01T11:35:00+08:00","permalink":"http://localhost:1313/zh-tw/p/k8s-bitnami-kafka-kraft-helm-chart/","title":"[Kubernetes] 使用 bitnami helm chart 安裝 kafka kraft 集群"},{"content":"簡介 官方定義:\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n― offical site Kubernetes 又稱為 k8s，最初由 google 用 golang 開發而後釋出的專案。用於操作自動化容器，包括部署，調度和節點集群間擴展。\n架構 上圖為一個簡易的 Kubernetes Cluster，通常一個 Cluster 中會有多個 Master 作為備援，但為了簡化我們只顯示一個。\n運作方式 使用者透過 User Command（kubectl）建立 Pod 時經過使用者身份的認證後，再將指令傳遞到 Master Node 中的 API Server，API Server 會把指令備份到 etcd 。接下來 controller-manager 會從 API Server 收到需要創建一個新的 Pod 的訊息，並檢查如果資源許可，就會建立一個新的 Pod。最後 Scheduler 在定期訪問 API Server 時，會詢問 controller-manager 是否有建置新的 Pod，如果發現新建立的 Pod 時，Scheduler 就會負責把 Pod 配送到最適合的一個 Node 上面。\n節點與組件 Master Node 為 Kubernetes 叢集的控制台，負責管理集群、協調所有活動，包含的元件如下：\nAPI Server API Server 管理 Kubernetes 的所有 api interface，用來和集群中的各節點通訊並進行操作。\nScheduler Scheduler 是 Pods 調度員，監視新建立但還沒有被指定要跑在哪個 Worker Node 上的 Pod，並根據每個 Node 上面資源去協調出一個最適合放置的對象給該 Pod。\nController Manager 負責管理並運行 Kubernetes controller 的組件，controller 是許多負責監視 Cluster 狀態的 Process，又可分為下列不同的種類\nNode controller - 負責通知與回應節點的狀態 Replication controller - 負責每個複寫系統內維持設定的 Pod 數量 End-Point controller - 負責端點的服務發布 Service Account \u0026amp; Token controller - 負責創建服務帳戶與新生成的 Namespace 的 API 存取 Token etcd 用來存放 Kubernetes Cluster 的資料作為備份，當 Master 因為某些原因而故障時，我們可以透過 etcd 幫我們還原 Kubernetes 的狀態。\nWorker Node 為 Kubernetes 的 runtime 執行環境，包含的元件如下：\nPod Kubernetes pod 是 Kubernetes 管理的最小單元，裡面包含一個或多個 container，可視為一個應用程式的邏輯主機。 同一個 Pod 中的 Containers 共享相同資源及網路，彼此透過 local port number 溝通。pod 運行在私有隔離的網絡上，默認情況下在同一集群的其他 pod 和 service 中可見，但是外部不可見，需要藉助 service 暴露給外部。\nKubelet Kubelet 接受 API server 的命令，用來啟動 pod 並監測狀態，確保所有 container 都在運行。它每隔幾秒鐘向 master node 提供一次 heartbeat。如果 replication controller 未收到該消息，則將該節點標記為不正常。\nKube Proxy 進行網路連線的 forwarding，負責將 request 轉發到正確的 container。\nResource https://blog.sensu.io/how-kubernetes-works https://medium.com/@C.W.Hu/kubernetes-basic-concept-tutorial-e033e3504ec0 https://ithelp.ithome.com.tw/articles/10202135 ","date":"2022-09-07T21:13:00Z","permalink":"http://localhost:1313/zh-tw/p/kubernets-basic/","title":"Kubernetes Introduction"},{"content":"本篇文章記錄怎麼使用 cert-manager 為對外的 istio gateway 加上 https。\n憑證分類 自簽憑證：某些不需要被公開存取、但希望達到資料傳輸能加密的內部服務，可以使用自簽憑證，Client 去存取的時候自己帶上 CA 憑證去驗證即可，例如 HashiCorp Vault, AWS RDS TLS 連線\u0026hellip;等。\n第三方 CA 機構簽發憑證：如果是公開的網路服務，就必須透過正規的 CA 機構來簽發，如需要收費的 Digicert, SSL.com, Symantec\u0026hellip;等，或是免費的 Let’s Encrypt。\ncert-manager cert-manager 是基於 Kubernetes 所開發的憑證管理工具，它可以可以幫忙發出來自各家的 TLS 憑證，例如上面所提到的 ACME (Let’s Encrypt), HashiCorp Vault, Venafi 或是自己簽發的憑證，而且它還可以確保 TLS 憑證一直維持在有效期限內。\nAbove Reference\nInstall 1 2 3 4 5 6 $ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.4.0/cert-manager.yaml $ kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c6866597-zw7kh 1/1 Running 0 2m cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m Issuer Issuer 用來頒發憑證，分為兩種資源類型：\nissuer：只作用於特定 namespace ClusterIssuer：作用於整個 k8s 集群 cert-manager 有支援幾種的 issuer type：\nCA: 使用 x509 keypair 產生 certificate，存在 kubernetes secret Self Signed: 自簽 certificate ACME: 從 ACME (ex. Let\u0026rsquo;s Encrypt) server 取得 ceritificate Vault: 從 Vault PKI backend 頒發 certificate Venafi: Venafi Cloud Above Refenrence\nCreate Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: # 創建的簽發機構的名稱，後面創建證書的時候會引用 name: letsencrypt-prod spec: acme: # 證書快過期的時候會有郵件提醒，不過 cert-manager 會利用 acme 協議自動給我們重新頒發證書來續期 email: ulahsieh@nexaiot.com privateKeySecretRef: # Name of a secret used to store the ACME account private key 指示此簽發機構的私鑰將要存儲到哪個 Secret 中 name: letsencrypt-prod # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory solvers: # 指示簽發機構使用 HTTP-01 的方式進行 acme 協議(還可以用 DNS 方式，acme 協議的目的是證明這台機器和域名都是屬於你的，然後才准許給你頒發證書) - http01: ingress: class: istio 備註：\nletsencrypt 可以看到網路上有兩個不同名字的設定，一個是 letsencrypt-staging 用於測試，一個是 letsencrypt-prod 用於 production. 1 2 $ kubectl apply -f clusterIssuer.yaml $ kubectl describe clusterissuers.cert-manager.io 做完的時候發生了 server misbehaving 錯誤\n1 2 3 4 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ErrInitIssuer 8m2s (x3 over 8m7s) cert-manager Error initializing issuer: Get \u0026#34;https://acme-v02.api.letsencrypt.org/directory\u0026#34;: dial tcp: lookup acme-v02. api.letsencrypt.org on 10.96.0.10:53: server misbehaving 查了一下發現是 dns 解析的問題，便去自建的 dns server 改 /etc/named.conf 檔，發現在 option 中少加了 forwarders 欄位。\nforwarders 是指當本 DNS 解析不了的域名，要轉給誰來解析的意思，通常轉給再上一層，也就是外網本身的 DNS，簡單來說可直接使用 8.8.8.8，並添加 allow-query any;，讓集群內的網段都能來使用。\n改好之後回到 k8s 集群再次查看 clusterissuer 是否可以建成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 [root@k8sm1 cert-manager]# kubectl describe clusterissuers.cert-manager.io Name: letsencrypt-prod Namespace: Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: cert-manager.io/v1 Kind: ClusterIssuer Metadata: Creation Timestamp: 2021-06-21T12:12:47Z Generation: 1 Resource Version: 104200852 Self Link: /apis/cert-manager.io/v1/clusterissuers/letsencrypt-prod UID: f0e9ecc6-9a50-491c-af78-b88670885e18 Spec: Acme: Email: ulahsieh@nexaiot.com Preferred Chain: Private Key Secret Ref: Name: letsencrypt-prod Server: https://acme-v02.api.letsencrypt.org/directory Solvers: http01: Ingress: Class: istio Status: Acme: Last Registered Email: ulahsieh@nexaiot.com Uri: https://acme-v02.api.letsencrypt.org/acme/acct/127768449 Conditions: Last Transition Time: 2021-06-21T12:13:35Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u0026lt;none\u0026gt; Create Certificate 透過 Issuer 申請 Certificate 憑證\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # cat istio-cert.yaml apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: istio-cert namespace: istio-system spec: commonName: convergence.nexmasa.com dnsNames: - convergence.nexmasa.com secretName: istio-cert issuerRef: name: letsencrypt-prod kind: ClusterIssuer 說明：\nspec.secretName 指示證書最終存到哪個 Secret 中 spec.issuerRef.kind 值為 ClusterIssuer 說明簽發機構不在本namespace 下，而是在全局 spec.issuerRef.name 我們創建的簽發機構的 Issuer 名稱 spec.dnsNames 指示該證書的可以用於哪些域名 踩坑囉 部屬 certificate 後一直卡在跟 letsencrypt issueing 這塊，錯誤訊息是:\n1 2 3 4 5 6 $ kubectl describe certificate -n istio-system istio-cert $ kubectl get event -n istio-system 30s Normal Issuing certificate/istio-cert Issuing certificate as Secret does not exist 29s Normal Generated certificate/istio-cert Stored new private key in temporary Secret resource \u0026#34;istio-cert-hrrgb\u0026#34; 29s Normal Requested certificate/istio-cert Created new CertificateRequest resource \u0026#34;istio-cert-89tj8\u0026#34; 3s Warning Failed certificate/istio-cert The certificate request has failed to complete and will be retried: Failed to wait for order resource \u0026#34;istio- cert-89tj8-2838533447\u0026#34; to become ready: order is in \u0026#34;invalid\u0026#34; state: 試了很久，發現官網有寫 debug 過程，才發現要去看 challenge 的 log\nhttps://cert-manager.io/docs/faq/acme/\n1 2 3 4 5 6 7 8 $ kubectl describe challenges.acme.cert-manager.io -n istio-system istio-cert-22gl9-2838533447-3373762545 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Started 32m cert-manager Challenge scheduled for processing Normal Presented 32m cert-manager Presented challenge using HTTP-01 challenge mechanism Warning Failed 32m cert-manager Accepting challenge authorization failed: acme: authorization error for convergence.nexmasa.com: 400 urn:ietf:params:acme:error: dns: DNS problem: NXDOMAIN looking up A for convergence.nexmasa.com - check that a DNS record exists for this domain 然後找了一下解答發現，letsencrypt 只適合用在網際網路存取到的 DNS 啊 \u0026hellip; = =\nhttps://github.com/jetstack/cert-manager/issues/3543\nThis error comes from Let\u0026rsquo;s Encrypt which cannot reach your domain (it tries to as .int is a public known TLD). In order for Let\u0026rsquo;s Encrypt to work they need to have public access to verify the ownership of your domain. For internal only domains you might want to look into using an internal CA.\nhttps://serverfault.com/questions/1048678/check-that-a-dns-record-exists-for-this-domain\ndomain names that are in the global DNS tree\n不過這邊還是可以記錄幾篇可以參考使用 letsencrypt 的文章\nhttps://www.qikqiak.com/k8strain/istio/cert-manager/ https://medium.com/intelligentmachines/istio-https-traffic-secure-your-service-mesh-using-ssl-certificate-ac20ec2b6cd6 改建自簽憑證 建立 Issuer 1 2 3 4 5 6 7 8 9 kubectl apply -f \u0026lt;(echo \u0026#34; apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: istio-system spec: selfSigned: {} \u0026#34;) 建立 Certificate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f \u0026lt;(echo \u0026#39; apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-crt namespace: istio-system spec: secretName: tls-secret duration: 175200h renewBefore: 12h issuerRef: kind: Issuer name: selfsigned commonName: \u0026#34;convergence.nexmasa.com\u0026#34; isCA: true dnsNames: - \u0026#34;convergence.nexmasa.com\u0026#34; \u0026#39;) 查看憑證及金鑰的有效性 1 kubectl get secrets/tls-secret -n istio-system -o \u0026#34;jsonpath={.data[\u0026#39;tls\\.crt\u0026#39;]}\u0026#34; | base64 -d | openssl x509 -text -noout 1 kubectl get secrets/tls-secret -n istio-system -o \u0026#34;jsonpath={.data[\u0026#39;tls\\.key\u0026#39;]}\u0026#34; | base64 -D | openssl rsa -check 為服務加上憑證 修改 istio gateway，加上 https 的 protocol 並指定上面建立的 Secret Name。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: converg-api-gw namespace: converg-api spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: Port-80-M55Sa number: 80 protocol: HTTP - hosts: - \u0026#39;*\u0026#39; port: name: https number: 443 protocol: HTTPS tls: credentialName: tls-secret mode: SIMPLE ","date":"2022-05-19T09:11:00Z","permalink":"http://localhost:1313/zh-tw/p/add-https-to-istio-gw/","title":"為對外的 istio gateway 加上 https"},{"content":"kubernetes 1.22 版之後，就不再支持 Docker 作為 container runtime 以及管理容器及鏡像的工具了。可以使用 containerd 取代 docker 的 container runtime；以及 crictl 作為 CRI(Container Runtime Interface)，另外 podman 也可以用來管理容器和鏡像。本篇記錄基於 containerd \u0026amp; crictl 使用 kubeadm 部屬 Kubernetes 集群的過程。\n系統環境配置 (所有節點) 最小系統資源需求 每台機器 4 GiB 以上 RAM master control plane 節點至少需要有兩個以上的 vCPU 集群中所有機器之間的完整網絡連接 (can be private or public) Server Type Hostname Spec master node.ulatest.com 4 vCPU, 8G RAM worker rockyw.ulatest.com 8 vCPU, 16G RAM worker rockyw2.ulatest.com 8 vCPU, 16G RAM 配置 /etc/hosts 1 2 3 4 5 6 7 8 9 10 cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.1.5.130 nexdata nexdata.ulatest.com 10.1.5.146 node node.ulatest.com 10.1.5.147 rockyw rockyw.ulatest.com 10.1.5.148 rockyw2 rockyw2.ulatest.com 10.1.5.130 nfs nfs.ulatest.com 更新軟體套件 1 yum update -y 系統配置 停用防火牆 1 2 systemctl stop firewalld systemctl disable firewalld 關閉 SELINUX 1 2 3 4 5 6 7 8 9 10 11 12 13 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted 關閉 swap 1 2 3 4 # Turn off swap swapoff -a # comment out the line of swap\u0026#39;s mount point sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab 配置 kernel module 自動加載 1 2 3 4 5 6 7 8 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # 執行以下命令使配置生效 modprobe overlay modprobe br_netfilter 調整 kernel 參數 Kubernetes 的核心是依靠 netfilter kernel module 來設定低級別的集群 IP 負載均衡，需要兩個關鍵的 module：IP轉發和橋接。\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysctl.d/kubernetes.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 EOF # 執行以下命令使配置生效 sysctl -p /etc/sysctl.d/kubernetes.conf 以上操作含意如下：\n開啟 iptables 對 bridge 的數據進行處理 開啟數據包轉發功能（實現 vxlan） 禁止使用 swap 空間，只有當系統 OOM 時才允許使用它 開啟 ipvs module Kube-Proxy 是 Kubernetes 用來控制 Service 轉發過程的一個元件，預設會使用 iptables 作為 Kubernetes Service 的底層實現方式，而此模式最主要的問題是在服務多的時候產生太多的 iptables 規則，大規模情況下有明顯的性能問題。可以透過參數變化的方式要求 Kube-Proxy 使用 ipvs。開啟 ipvs 的前提條件是加載以下的 kernal module：\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack 上面腳本創建了的 /etc/sysconfig/modules/ipvs.modules 文件，保證在節點重啟後能自動加載所需模塊。使用 lsmod | grep -e ip_vs -e nf_conntrack 命令查看是否已經正確加載所需的內核模塊。\n接下來還需要確保各個節點上已經安裝了 ipset 軟件包，以及管理工具 ipvsadm 便於查看 ipvs 的代理規則。\n1 yum install -y ipset ipvsadm 如果以上前提條件如果不滿足，則即使 kube-proxy 的配置開啟了 ipvs 模式，也會退回到 iptables 模式。\n安裝 containerd \u0026amp; crictl (所有節點) 安裝 containerd 1 2 3 4 5 yum install -y yum-utils # 使用 docker.ce 作為 containerd 的 repo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y containerd.io systemctl enable containerd 生成 containerd 的配置文件:\n1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 根據 Kubernetes 文檔 Container runtimes 中的內容，對於使用 systemd 作為 init system 的 Linux 發行版，使用 systemd 作為容器的 cgroup driver 可以確保服務器節點在資源緊張的情況更加穩定，因此這裡配置各個節點上 containerd 的 cgroup driver 為 systemd。\n如果檔案中[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] 區塊下，沒有 SystemdCgroup 的選項，下： 1 sed -i \u0026#39;s|\\(\\s\\+\\)\\[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options\\]|\\1\\[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options\\]\\n\\1 SystemdCgroup = true|g\u0026#39; /etc/containerd/config.toml 如果檔案中[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] 區塊下，有 SystemdCgroup = false 的選項，下： 1 sed -i \u0026#39;s/ SystemdCgroup = false/ SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml 修改完畢後 config 內容會如下：\n1 2 3 4 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true 重啟 containerd 已應用 config\n1 systemctl restart containerd 安裝 crictl 1 2 3 yum install -y wget tar wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.23.0/crictl-v1.23.0-linux-amd64.tar.gz tar zxvf crictl-v1.23.0-linux-amd64.tar.gz -C /usr/local/bin 設定 container runtime interface 為 containerd\n1 2 3 4 5 6 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF 測試\n1 2 crictl images IMAGE TAG IMAGE ID SIZE 如果上方 CRI 沒有指定的話，會出現以下錯誤\n安裝 kubernetes 套件 (所有節點) 新增 kubernetes repo 1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF 在更新 yum 源後，使用 yum makecache 生成緩存，將套件包訊息提前在本地 cache 一份，用來提高搜索安裝套件的速度。\n1 yum makecache -y 通過 yum list 命令可以查看當前源的穩定版本，目前的穩定版本是 1.23.4-0。安裝 kubeadm 便會將 kubelet、kubectl 等依賴一併安裝。\n1 yum list kubeadm 1 yum install -y kubeadm-1.23.4-0 配置命令參數自動補全功能 1 2 3 4 yum install -y bash-completion echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc echo \u0026#39;source \u0026lt;(kubeadm completion bash)\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc 啟動kubelet 服務 1 2 systemctl enable kubelet systemctl restart kubelet 配置節點 kubeadm 部署 master 節點 準備配置文件 1 2 kubeadm config print init-defaults \u0026gt; kubeadm-init.yaml vim kubeadm.yaml 更改以下配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.1.5.146 # 改為 master node IP bindPort: 6443 nodeRegistration: criSocket: unix:///run/containerd/containerd.sock # 改為 containerd Unix socket 地址 imagePullPolicy: IfNotPresent name: rockym # 指定節點名稱 taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: 1.23.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # 指定 pod 子網 cidr，在設定 calico 時會用到 scheduler: {} 叢集初始化 1 kubeadm init --config=kubeadm-init.yaml 完成後按照提示將 /etc/kubernetes/admin.conf 複製到 $HOME/.kube/config\n1 2 3 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 並複製下面那一串加入指令以便其他 node 加入 (兩個小時過期)\n1 2 kubeadm join 10.1.5.146:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fc56685aedecf323023637a6e02cc1584cfc88bfeb0690dc0e2a1feca278f008 或是之後使用 kubeadm token create \u0026ndash;print-join-command 建立新的。\n以上就完成 master 節點的部屬，可以使用 kubectl command 確認。\n因目前網路尚未設置，所以 coredns 狀態為 Pending 是正常的。\n安裝 calico 1 curl -s https://docs.projectcalico.org/manifests/calico.yaml | kubectl apply -f - 安裝完畢後就可以發現節點已經部屬完成了。\n加入工作節點 在各工作節點上直接輸入上方的 join command\n1 2 kubeadm join 10.1.5.146:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fc56685aedecf323023637a6e02cc1584cfc88bfeb0690dc0e2a1feca278f008 就可大功告成了~\n註釋\n後記 有些截圖裡面可以發現原本的 master node 的 hostname 本來叫 rockym 的，可是在加入 master node 節點的時候的名字忘記改 (冏) 導致 master node 強迫改名為 node \u0026hellip; 求助谷歌大神，發現改節點名稱最乾淨且簡單的方式就是刪掉節點後重新加入，但不巧地是我要改的節點就是唯一一個的 master node =__= 只好折衷將錯就錯改 hostname，不知道後續會不會發生問題，先記錄一下。\nReference kube-proxy https://kubernetes.io/zh/docs/concepts/services-networking/service/#proxy-mode-ipvs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd ","date":"2022-03-13T14:56:00Z","permalink":"http://localhost:1313/zh-tw/p/install-kubernetes-123-on-rocky-linux/","title":"在 Rocky Linux 8 安裝 Kubernetes 1.23 (containerd as cri)"}]