[{"content":"TL; DR è¶è‘—å­¸æœŸçµæŸå¾Œï¼Œåœ¨å…­æœˆçš„æœ€å¾Œä¸€å€‹ç¦®æ‹œçŠ’è³è‡ªå·±åœ¨è·ç¢©ä¸€çµæŸå¾Œä¾†å ´é¦™æ¸¯å››å¤©ä¸‰å¤œéŠ (ï½¡Ã­ _ Ã¬ï½¡)\nè¡Œå‰ é€™æ¬¡å¾å››æœˆæè­°ã€åˆ°äº”æœˆåº•çœŸæ­£æ±ºå®šå‡ºç™¼ï¼Œç¸½å…±æ‹–äº†ä¸€å€‹æœˆ XD æœ€å¾Œçµ‚æ–¼åœ¨å‡ºç™¼å‰çš„ä¸€å€‹æœˆæ‰æ‰¾å¥½æ©Ÿç¥¨è·Ÿé£¯åº—ï¼Œé£¯åº—æ˜¯åœ¨å»æ¨¹æ—ä¸Šç­çš„è·¯ä¸Šçœ‹åˆ°ä¾¿å®œã€å¾Œä¾†ä½äº†ä¹‹å¾Œè¶…æ»¿æ„çš„ OTTO Hotelï¼Œå…©æ™š $5400ã€‚å¾Œä¾†é‚„æ±ºå®šç‚ºäº†è›‹å¡”å¤šä½ä¸€æ™šï¼Œè€ŒåƒæŒ‘è¬é¸çš„ 168 youth hostelï¼Œä¸€æ™š $1500ï¼Œåªèƒ½èªªå°å¾—èµ·åƒ¹æ ¼ (Â´ãƒ»Ã…ãƒ»`) æ©Ÿç¥¨çš„è©±å¯æƒœæ²’æœ‰æ³¨æ„åˆ°é€™å¹¾å¹´éƒ½æœƒæœ‰çš„åœ‹æ³°è²·ä¸€é€ä¸€çš„æ´»å‹•ï¼Œå°±ç”¨å¤§æ¦‚é«˜é›„é¦™æ¸¯å¿«é‹ $6800ã€å°åŒ—é•·æ¦® $7900 çš„ä¾†å›åƒ¹æ ¼è²·äº†ã€‚\nç„¶å¾Œç”¨äº†åå¹´çš„è­·ç…§ä¹Ÿåœ¨ä»Šå¹´å³å°‡åˆ°æœŸäº†ï¼Œç¿»èˆŠè­·ç…§é‚„ç¿»åˆ°ä¹‹å‰å»ç¾åœ‹åŠ æ‹¿å¤§çš„å‡ºå…¥å¢ƒç« ï¼Œæ™‚é–“çœŸçš„éå¾—é£›å¿« ( â€¢Ìá½¤â€¢Ì€) äº”æœˆåº•è«‹äº†ä¸€å€‹ä¸‹åˆå»è¾¦æ–°è­·ç…§ï¼Œæ²’æƒ³åˆ°å¤–äº¤éƒ¨å°±åœ¨é’å³¶æ±è·¯ï¼Œå‰›å¥½é‚„é‡åˆ°å¾ˆä¹…é•çš„å¤§å‹é›†æœƒ ï¼ˆé’é³¥é‹å‹•ï¼‰ã€‚é«˜ä¸­çš„æ™‚å€™ä¸€ç›´å¾ˆæœ‰å°è±¡ç­å°å°æ–¼æˆ‘å€‘ç«Ÿç„¶ä¸çŸ¥é“è¡Œæ”¿é™¢é•·æ˜¯èª°è€Œé©šè¨ï¼Œç„¶å¾Œè·Ÿæˆ‘å€‘èªªèº«ç‚ºå…¬æ°‘å°±æ˜¯éœ€è¦æœ‰å…¬æ°‘è²¬ä»»ã€‚é‚£æ™‚å€™ä¸æ‡‚æ”¿æ²»æ˜¯ä»€éº¼ï¼Œä½†æ²’æƒ³åˆ°åå¹¾å¹´å¾Œçš„ç¾åœ¨ä¹Ÿæœƒå¦‚æ­¤æŠ•èº«å…¶ä¸­ã€‚åŸä¾†é€™å°±æ˜¯é•·å¤§ï¼\næ¥è‘—åœ¨å‡ºç™¼å‰å°±åˆæ¿€ç™¼äº† J çš„å±¬æ€§è³ªï¼Œç™¼ç¾äº†å»è¶£é€™å€‹å¥½ç”¨çš„è¡Œç¨‹å®‰æ’ APPï¼Œç„¶å¾Œæ¯å¤©ç‹‚çœ‹éŠè¨˜åˆ†äº«æ‰¾æ™¯é»è·Ÿç¾é£Ÿï¼Œæœ€å¾Œå…±èª¿æ•´äº†ä¸‰æ¬¡çš„è¡Œç¨‹æœ€å¾Œæ‰åº•å®šå¥½ ğŸ¤£ DAY1 é€™æ¬¡çµ‚æ–¼ä¸æ˜¯ç´…çœ¼ç­æ©Ÿï¼Œæ—©ä¸Šå…­é»å‡ºé–€æ­å…¬è»Šå»åæ©Ÿæ·ç¶½ç¶½æœ‰é¤˜ã€‚è‡ªå¾ä¹‹å‰å»ç¾åœ‹é¦–æ¬¡è‡ªå·±åé£›æ©Ÿå¾Œå°±å†ä¹Ÿæ²’ç¨è‡ªåéï¼Œå§Šç”šè‡³æ˜¯ç¬¬ä¸€æ¬¡ï¼Œæ‰€ä»¥å„è‡ªå¾é«˜é›„è·Ÿæ¡ƒæ©Ÿå‡ºç™¼çš„æˆ‘å€‘ä¸€è·¯çš„è·Ÿå°æ–¹èªªç¾åœ¨åœ¨å“ªå€‹éšæ®µäº† XD æš‘å‡çš„äººæ½®æœç„¶ä¸å¯å°è¦·ï¼Œé€™æ™‚æ®µçš„é£›æ©Ÿç­æ¬¡å¤ªå¤šäº†ï¼Œå°è‡´èª¤é»äº†åŠå°æ™‚æ‰èµ·é£› ğŸ¥¹\nå¦å¤–é‚„æœ‰ä¸€å€‹å°æ’æ›²ï¼Œå…¥å¢ƒæ™‚å› ç‚ºç©ºå§åœ¨é£›æ©Ÿä¸Šèªªå¯ä»¥ä¸ç”¨å¡«å…¥å¢ƒå°å¡ï¼Œæ‰€ä»¥åœ¨æµ·é—œé–˜å£å‰æ²’å¡«å®Œå…¨éƒ¨çš„æ¬„ä½å°±æ€¥æ€¥å¿™å¿™åœ°å»æ’éšŠï¼Œæ®Šä¸çŸ¥æµ·é—œé‚„æ˜¯è·Ÿæˆ‘è¦äº†ï¼Œå†æ®Šä¸çŸ¥äº¤å‡ºæ²’å¡«å®Œæ•´çš„å°å¡é‚„æ˜¯é †åˆ©å…¥å¢ƒäº† á•• ( á› ) á•—\nå§Šå…ˆå¹«æˆ‘æ‹¿å¥½è¡Œæï¼Œæœƒåˆå¾Œåœ¨æ©Ÿå ´å¤§å»³é ˜äº‹å…ˆåœ¨ klook è²·å¥½çš„å…«é”é€šï¼Œå†å»è²·æ©Ÿå ´è·Ÿå¸‚å€ä¾†å›çš„åŸå·´å¿«ç·š A21 \u0026amp; A25 å¥—ç¥¨ HKD$60ï¼Œå¾æ©Ÿå ´åˆ°å¸‚å€çš„ç­æ¬¡å¾ˆå¹¸é‹ååˆ°äº†å¹¾ä¹æ˜¯ç›´é”çš„ A25ï¼Œæ¯”åæ·é‹æ–¹ä¾¿ï¼ˆä¸éœ€æ‹‰è¡Œæè½‰è»Šï¼‰ï¼Œè€Œä¸”é‚„æ¯”æ·é‹ä¾¿å®œå¾ˆå¤šï¼\nå·¦ä¸Šï¼šæ¡ƒæ©Ÿæ»¿æ»¿çš„checkinäººæ½®ã€å³ä¸Šï¼šä¾†è‡ªå§çš„checkinå›å ±ã€ä¸‹ï¼šA25ç©ºç©ºçš„èˆ’é©è»Šå»‚\nHashtag b åˆ°äº†é£¯åº—æ‰€åœ¨çš„å°–æ²™å’€å¾Œï¼Œå…ˆé †é“å»äº† hashtag B è²·åƒå±¤è›‹å¡”ï¼Œå¹¸é‹çš„æ˜¯ä¸ç”¨æ’å¹¾çµ„å°±å¾ˆå¿«è²·åˆ°äº†ï¼è²·äº†å…©å€‹è›‹å¡”è·Ÿä¸€å€‹é–‹å¿ƒæœé…¥ æ¥è‘—å»é£¯åº— checkinï¼Œå¤§æ¦‚åœ¨é£¯åº—å‰ç¹äº†å…©åœˆæ‰ç™¼ç¾å…¥å£ XDD ä¸€é€²é£¯åº—å°±è¦ºå¾—é¸å°äº†ï¼Œæˆ¿é–“ä¸å¤§ï¼Œä½†å°æ–¼å¯¸åœŸå¯¸é‡‘çš„é¦™æ¸¯ä¾†èªªå·²ç¶“å¾ˆèˆ’é©äº†ï¼Œæœ€æ£’çš„æ˜¯éš”éŸ³åšå¾—å¾ˆå¥½ï¼Œå…©æ™šä½ä¸‹ä¾†æ²’æ‰¾åˆ°ä¸€å€‹ç¼ºé»ï¼ ç¨ä½œä¼‘æ¯å¾Œä¾¿å‡ºç™¼åŸæœ¬çš„é¦–ç«™å…­å®‰å±…åƒé£¯ï¼Œè·¯ä¸Šé¦–æ¬¡é«”é©—äº†å®å®è»Šã€‚åŸæœ¬æƒ³é«”é©—å‚³çµ±çš„æ¸¯å¼é£²èŒ¶æ¨è»Šçš„ï¼Œæ®Šä¸çŸ¥åªé–‹æ—©ä¸Šï¼Œå¹¸å¥½é‚„æœ‰å‚™æ¡ˆï¼Œé‚„æ˜¯æ’è¡Œç¨‹çš„æ™‚å€™çŒ¶è±«å¾ˆä¹…æœ€å¾Œæ¨æ£„çš„ç”Ÿè¨˜ç²¥å“ï¼ ç”Ÿè¨˜ç²¥å“å°ˆå®¶ é»äº†ç‰›è‚‰è·Ÿé­šçš„å£å‘³å„ä¸€ç¢—ï¼Œç²¥æœ¬èº«ç…®åˆ°é€£ç±³ç²’éƒ½çœ‹ä¸è¦‹ï¼Œå–èµ·ä¾†å¾ˆæ¸…çˆ½åˆå¾ˆé¦™ï¼åº—å®¶åè½æ–¼å··å¼„ï¼Œé–€å£å¾ˆä¸èµ·çœ¼ï¼Œæ‰€ä»¥é‚„å·®é»èµ°åˆ°é¦¬è·¯ä¸Šçš„åŒåéºµåº— ğŸ¤£ åŸæœ¬çœ‹ç¶²è·¯è©•åƒ¹éƒ½æ˜¯éœ€è¦æ’éšŠçš„ï¼Œä½†å› ç‚ºæ˜¯å¹³æ—¥ã€åˆæ˜¯å…©é»æ™‚çš„éç”¨é¤æ™‚é–“ï¼Œæ‰€ä»¥é™¤äº†ä¸ç”¨æ’éšŠä¹‹å¤–ï¼Œé€£é¤é»éƒ½æ‰“äº†æŠ˜ï¼ ä¸­ç’°è¡—å¸‚-æª¸æª¬ç‹ã€é™³æ„é½‹ã€è˜­èŠ³åœ’ã€æ³°æ˜Œé¤…å®¶ å»æ­ä¸­ç’°åŠå±±æ‰‹æ‰¶æ¢¯çš„è·¯ä¸Šå…ˆå»æª¸æª¬ç‹è·Ÿé™³æ„é½‹è²·äº†ä¼´æ‰‹ç¦®ï¼Œæ¥è‘—é †åˆ©çš„æ­åˆ°å¾€ä¸Šçš„æ‰‹æ‰¶æ¢¯å…å»çˆ¬å¡çš„è¾›è‹¦ï¼Œå–äº†è˜­èŠ³åœ’çš„çµ²è¥ªå¥¶èŒ¶ï¼ˆå¤–å¸¶ç„¡éœ€æ’éšŠï¼ï¼‰ã€æ„çŒ¶æœªç›¡çš„é‚„åœ¨éš”å£ LINLEE è²·äº†å‡æª¸èŒ¶ï¼Œå†ä¾†ä¸€é¡†æ³°æ˜Œé¤…å®¶çš„å‚³çµ±è›‹å¡”ï¼ˆè²·å®Œé‚„åˆ°å°å··çš„æŸé–“å’–å•¡å»³å¤–çš„çŸ³æ¤…ä¸Šåè‘—äº«ç”¨ï¼‰ åœ¨googlemapsä¸Šæ‰¾åˆ°çŸ³æ¤…çš„ç¢ºåˆ‡ä½ç½®ï¼Œæ„Ÿè¬ Green Waffle Diner çš„è²¢ç» XDD\nè·ææ´»é“ wall muralã€crazy of art æ²’æœ‰åœ¨é å®šè¡Œç¨‹å…§ï¼Œä½†èµ°æ²’å¹¾æ­¥ç™¼ç¾äº†ä¸€å€‹å°åœ°æ¨™ã€‚ å¤§é¤¨ - SHARISHARI èˆŠè­¦ç½²ç›£ç„å¤è¹Ÿçš„éºå€è®Šæˆè—è¡“å±•è¦½é¤¨ï¼Œå±•ç¤ºäº†é¦™æ¸¯æ®–æ°‘åœ°æ™‚æœŸçš„å»ºç¯‰é¢¨æ ¼å’Œæ­·å²ã€‚ ç„¶å¾Œæ²’ç´°é€›å¾ˆä¹…å°±è·‘åˆ°éš”å£çš„å£è¢‹åå–®ä¹‹ä¸€çš„ SHARISHARI åƒå†° XD é»äº†ä¼¯çˆµå¥¶èŒ¶å£å‘³ï¼Œä»½é‡åè¶³ï¼Œè£¡é¢çš„é»ƒè±†ç²‰å¾ˆé¦™ã€è£¡é¢é‚„åŠ äº†å¥¶å‡è·Ÿé¤…ä¹¾å±‘ï¼Œæ•´ç†ä¾†èªªé‚„ä¸éŒ¯ï¼ çŸ³æ¿è¡— èµ°ä¸­ç’°ç µç”¸ä¹è¡—å»æ¸¯å£çš„è·¯ä¸Šçš„ä¸€å€‹æ‹ç…§æ™¯é» - çŸ³æ¿è¡—ï¼Œé€™æ™‚å€™è…³å·²ç¶“æœ‰è…«è„¹çš„æ„Ÿè¦ºäº†ï¼Œé‚„è¨˜å¾—åœ¨èµ°é€™æ¢çš„æ™‚å€™ï¼Œå› ç‚ºæ˜¯ä¸‹å¡ã€åˆé€™éº¼å¤šçŸ³æ¿åšç‚ºéšœç¤™ï¼Œæˆ‘è·Ÿå§éƒ½åœ¨å“€åš ğŸ˜‚ å¤©æ˜Ÿå°è¼ªã€ç¶­å¤šåˆ©äºæ¸¯ æ¥ä¸‹ä¾†æ˜¯æˆ‘æœ€æœŸå¾…çš„è¡Œç¨‹ï¼Œå¾ä¸­ç’°åå°æ¸¡è¼ªå›å°–æ²™å’€ï¼å¤©æ˜Ÿå°è¼ªä¸€æ¬¡åªéœ€è¦ HKD$5 ç”¨å…«é”é€šé€šè¡Œä¸Šèˆ¹å¾Œååˆ†é˜å°±å¯ä»¥ç›´é”å°–æ²™å’€ï¼Œä¸€é‚Šå¹é¢¨ä¸€é‚Šæ¬£è³æ¸¯å£ç¾æ™¯ï¼Œéå¸¸æ¨è–¦ï¼ åœ¨å»æ¸¯å£çš„è·¯ä¸Šæœƒç¶“éé•·é•·çš„å¤©æ©‹ï¼Œæ­£å¥½é‚„é‡åˆ°ä¸‹ç­æ™‚é–“ï¼Œçœ‹åˆ°è¨±å¤šçš„æ­£è£ä¸Šç­æ—é€šå‹¤ï¼Œæ‰é«”æœƒåˆ°èº«è™•åœ¨å•†æ¥­é‡åœ°ä¹‹ä¸­çš„ç¹å¿™ã€‚å¦å¤–é‚„æœ‰è·¯éæ²’æƒ³åƒä¸­å¤§çš„é¦™æ¸¯æ‘©å¤©è¼ª ğŸ¡\nä¸‹èˆ¹å¾Œé‚„æ­£å¥½è¶•ä¸Šå…«é»çš„ç‡ˆå…‰ç§€ï¼Œå‰›å¥½æŒ‘åˆ°å¥½ä½ç½®æ¬£è³é€™ä¸–ç•Œä¸‰å¤§å¤œæ™¯ä¹‹ä¸€çš„ç¶­å¤šåˆ©äºæ¸¯ï¼ å¦¹è¨˜å¤§æ’æª” å›åˆ°é£¯åº—ä¼‘æ¯å¾Œï¼Œä¹é»é‚„æ˜¯å …æŒå‡ºå»åƒæ™šé¤ï¼Œé€™é–“å¦¹è¨˜å¤§æ’æª”åŸæœ¬ä¸åœ¨æˆ‘åšåŠŸèª²çš„åå–®å…§ï¼Œä½†æ˜¯å‰›å¥½çœ‹åˆ°ç‚ºäº†å»é¦™æ¸¯è€ŒåŠ å…¥çš„ line ç¤¾ç¾¤è£¡é¢çš„äººå¤§åŠ›è®šè³ï¼Œè€Œå¾ˆå·§çš„å°±åè½åœ¨é£¯åº—çš„æ­£å‰æ–¹ï¼Œä¾¿ä¹Ÿé †ä¾¿ä¾†è©¦è©¦é¦™æ¸¯çš„å¤§æ’æª”é¤å»³ï¼ é»äº†ä¸€å€‹å…¬ä»”éºµã€ç‚’é’èœè·Ÿçƒ¤ä¹³é´¿ï¼Œçµæœæœ€å¥½åƒçš„æ˜¯ç‡™é’èœï¼Œå…¶æ¬¡æ˜¯çƒ¤ä¹³é´¿ XDD å…¬ä»”éºµå£å‘³å¤ªé‡äº†æ²’è¾¦æ³•é †åˆ©åƒå®Œ DAY2 æ–°èˆˆé£Ÿå®¶ ç¬¬ä¸€ç«™æ—©åˆé¤ä¾†å …å°¼åœ°åŸçš„æ¸¯å¼é£²èŒ¶æ–°èˆˆé£Ÿå®¶ï¼Œé‚„æ²’11é»æ‰€ä»¥ä¸éœ€è¦æ’éšŠï¼Œä½†è£¡é¢ä¹Ÿå¹¾ä¹æ˜¯æ»¿åº§çš„æƒ…æ³ã€‚å› ç‚ºç”¨é¤å€å°åˆ°æ²’è¾¦æ³•å¾æ¨è»Šé»é¤ï¼Œä¹Ÿå› ç‚ºåº§ä½æœ‰é™ï¼Œäººæ•¸å°‘çš„è©±çµ•å°æœƒéœ€è¦ä½µæ¡Œï¼Œä½†æ•´é«”é¤é»å¥½åƒï¼Œé»é¤çš„æœå‹™é˜¿å§¨æ…‹åº¦ä¹Ÿè·Ÿç¶²è·¯ä¸Šèªªçš„å®Œå…¨ä¸ä¸€æ¨£ï¼Œéå¸¸å‹å–„ï¼åƒé£½å‡ºä¾†é‚„å¾ˆå¹¸é‹çš„ç™¼ç¾å¤–é¢å·²ç¶“é–‹å§‹åœ¨æ’éšŠäº†ï¼Œå†æ¬¡è®šå˜†æˆ‘çš„æ’ç¨‹ d(`ï½¥âˆ€ï½¥)b çŸ³ç‰†æ¨¹ç¾¤ã€å°ç´…æ›¸ç±ƒçƒå ´ åƒé£½å¾Œèµ°ä¾†é™„è¿‘çš„å…©å€‹æ™¯é»ï¼Œä¸€å€‹æ˜¯çŸ³ç‰†æ¨¹ç¾¤ï¼Œå¦å¤–ä¸€å€‹æ˜¯å»å¹´å°ç´…æ›¸çˆ†ç´…çš„ç±ƒçƒå ´æ‹ç…§æ™¯é»ã€‚åœ¨æŠµé”ç±ƒçƒå ´å‰ï¼Œé‚„ä»¥ç‚ºæ˜¯ç›´æ¥å¯ä»¥é çœºçš„é¢¨æ™¯ï¼ŒçµæœæŠµé”å¾Œç™¼ç¾é¢¨æ™¯å¾—éš”è‘—è—çƒç¶²æ‹ XDD èƒ½ç™¼ç¾çš„äººçœŸçš„å¾ˆå²å®³ æ€ªç¸å¤§æ¨“ è®Šå½¢é‡‘å‰›çš„å–æ™¯åœ°æ€ªç¸å¤§æ¨“æ˜¯ç”±å¤šæ£Ÿå¤§å»ˆçµ„æˆçš„ï¼Œå¾å¤–è§€çœ‹ä¸Šå»å¦‚åŒè¢«å·¨å¤§çš„æ€ªç¸åå™¬ï¼Œå› è€Œå¾—å…¶åã€‚æ¯å±¤äº¤éŒ¯ï¼Œæ˜¯å…¸å‹çš„é¦™æ¸¯è¸å±…ï¼Œå¯†å¯†éº»éº»çš„çª—æˆ¶è£¡è—è‘—ç„¡æ•¸äººçš„æ•…äº‹èˆ‡æ—¥å¸¸ã€‚ BlueHouse æ’è¡Œç¨‹æ™‚ç„¡æ„é–“çœ‹åˆ°çš„åœ°æ¨™ BlueHouseï¼Œè—å±‹ä»¥å‰æ˜¯ä¸€æ‰€é†«é™¢ï¼Œç¾ä»Šåˆ—ç‚ºé¦™æ¸¯ä¸€ç´šæ­·å²å»ºç¯‰ç‰©ã€‚ä»¤äººé©šå–œçš„æ˜¯é€™é‚Šçš„å»ºç¯‰ç«Ÿç„¶ä¸åƒ…åªæœ‰è—è‰²ï¼Œå‘¨é‚Šçš„å»ºç¯‰ç‰©æ¯ä¸€æ£Ÿéƒ½æ“æœ‰ç¨ç‰¹çš„é¡è‰²ï¼Œèˆ‡è—å±‹ç›¸äº’è¼æ˜ ï¼Œå½¢æˆä¸€å¹…å……æ»¿æ´»åŠ›çš„åŸå¸‚é¢¨æ™¯ã€‚ æ–°å…‰æˆ²é™¢ã€é¦™æ¸¯è¡—æ™¯ã€è·¯ä¸Šç¾é£Ÿ åœ¨å¾å …å°¼åœ°åŸå›åˆ°ä¸­ç’°ã€å†å»éŠ…é‘¼ç£ä¸Šä»¥åŠå›åˆ°ä¸­ç’°çš„è·¯ä¸Šè²·çš„å°é£Ÿç‰©æœ‰ï¼š\nifc mall è£¡é¢çš„ FineFood æ‹¿ç ´å´™è›‹ç³•é… % Arabica å’–å•¡ï¼Œç‰¹åˆ¥åˆ°é€™é–“è³¼ç‰©å•†å ´è£¡é¢å»æ‰¾è«–å£‡ä¸Šæ¨è–¦çš„å¸è‹‘é…’åº—çš„ FineFood æ«ƒä½è²·æ‹¿ç ´å´™ï¼Œé †ä¾¿é‚„è²·äº†ä¸€ç›’é»‘é¹½å£å‘³çš„è´è¶é…¥ï¼Œé€™é‚ŠçœŸçš„å®Œå…¨ä¸ç”¨æ’éšŠï¼Œæœ‰å¤ éš±å¯†ï¼ ç™¾äº‹å‰é¤…åº—çš„è´è¶é…¥è·Ÿæ‹¿ç ´å´™æ´¾ï¼Œè´è¶é…¥æ˜¯ä¼´æ‰‹ç¦®ï¼Œæ‹¿ç ´å´™è›‹ç³•å›åˆ°é£¯åº—åƒçš„å¿ƒå¾—æ˜¯æœç„¶é‚„æ˜¯æ‰¾ä¸åˆ°å°æ™‚å€™åœ¨çš‡æœé…’åº—åƒçš„å‘³é“ï¼Œä½†æœ‰æ™‚å€™æˆ‘åœ¨æƒ³ï¼Œé•·å¤§å¾Œæ˜æ˜åƒéé€™éº¼å¤šä¸ä¸€æ¨£çš„æ‹¿ç ´å´™ï¼Œç¸½é‚„æ˜¯è¦ºå¾—çš‡æœçš„æœ€æ£’ï¼Œä¹Ÿæˆ–è¨±æ˜¯å› ç‚ºå°æ™‚å€™åƒçš„æ™‚å€™æ˜¯æš‘å‡åœ¨å¤§é™¸ç©çš„ç¾å¥½æ™‚å…‰ï¼Œæ‰€ä»¥æ‰é‚£éº¼é›£ä»¥è¢«å–ä»£ã€‚ æ–°å…‰æˆ²é™¢æ—çš„æ¶¼èŒ¶ï¼ŒèŒ¶æ˜¯ç”¨ç¢—å…¬è£å¥½ç„¶å¾Œå°±ç«™åœ¨åº—å®¶å‰é¢å–å®Œï¼Œå¾ˆç‰¹åˆ¥çš„é«”é©—ï¼ç„¶å¾Œå› ç‚ºé‚£æ™‚å€™æ˜¯æ„Ÿå†’æœ«æœŸï¼Œå¯æ˜¯é€™å…©å¤©åˆæ”¾ç¸±çš„åƒäº†ä¸€å †å†°å“ï¼Œæ‰€ä»¥ç‚ºäº†å½Œè£œå–‰åš¨é»äº†åŒ–ç—°èŒ¶ (â°â–¿â°) éŠ…é‘¼ç£åº—çš„ BakeHouseï¼Œç¶“éæ²’äººåœ¨æ’éšŠçš„å¦ä¸€å®¶ç†±é–€è›‹å¡”åº—ï¼Œå°±é¦¬ä¸Šå„è²·äº†ä¸€é¡†ç«™åœ¨é–€å£åƒï¼Œå‰›å‡ºçˆçš„è›‹å¡”çœŸçš„å¾ˆè®šï¼ æ¥è‘—æ˜¯åœ¨éŠ…é‘¼ç£èµ°çš„ä¸€å †è·¯ä¸Šæ‹åˆ°çš„çµ•ç¾è¡—æ™¯ï¼Œé¦™æ¸¯çœŸçš„æ˜¯åº§ç¥å¥‡åˆçŸ›ç›¾çš„åŸå¸‚ã€‚æ“æ“ çš„äººæ½®æ“ æ»¿äº†æ¯ä¸€å¯¸åœŸåœ°ï¼Œå»åˆèƒ½åœ¨è»Šæ°´é¦¬é¾çš„è¡—é“ä¸Šæ„Ÿå—åˆ°å¯¬é—Šçš„è‡ªç”±ã€‚å¤è€çš„å»ºç¯‰å’Œç¾ä»£çš„é«˜æ¨“å¤§å»ˆä¸¦è‚©è€Œç«‹ï¼Œå‰é¢æ‰çœ‹åˆ°ç°æš—å¯†é›†çš„ç¤¾å€ã€é¦¬ä¸Šåˆæœ‰è‰²å½©é£½å’Œçš„è¡—æ™¯ã€‚æœçœŸæ˜¯è¦è‡ªç”±è¡Œæ‰èƒ½æ·±å…¥æ„Ÿå—åˆ°åŸå¸‚çš„é­…åŠ›ã€‚ è­šä»”ä¸‰å“¥ç±³ç·š æ™šé¤åƒé£¯åº—é™„è¿‘è­šä»”ä¸‰å“¥çš„éæ©‹ç±³ç·šï¼Œå…©å€‹äººä¸€ç¢—å†åŠ ä¸ŠåœŸåŒªé›ç¿¼ã€‚çµ‚æ–¼å¯ä»¥ç†è§£ç‚ºä»€éº¼é¦™æ¸¯äººé€™éº½å–œæ­¡åƒäº†ï¼æ¦®ç™»æˆ‘é¦™æ¸¯è¡Œçš„å‰ä¸‰ï¼ å»Ÿè¡—å¤œå¸‚ã€æ¾³æ´²ç‰›å¥¶å…¬å¸ åƒé£½å¾Œåå…¬è»Šå»å»Ÿè¡—å¤œå¸‚é€›é€›ï¼Œå¤œå¸‚ç›¸è¼ƒæ–¼å°ç£éœè‰²å¾ˆå¤šï¼Œæ²’æœ‰ç‰¹åˆ¥çš„é£Ÿç‰©ã€‚å¹¸å¥½é‚„å¯ä»¥é †ä¾¿å»é™„è¿‘çš„æ¾³æ´²ç‰›å¥¶å…¬å¸åƒå€‹ç‡‰å¥¶ç•¶é»å¿ƒã€‚ç‡‰å¥¶é»å†°çš„ï¼Œåƒèµ·ä¾†å°±æ˜¯å¾ˆæ¿ƒéƒçš„å¥¶é…ªï¼Œå¥½å¥½åƒï¼ å›ç¨‹é‚„é †ä¾¿å»é€›äº† Balenoï¼Œå§Šè²·äº†ä¸€ä»¶æ’æ±— Tæ¤ï¼Œé€™é–“ç°¡ç›´å°±æ˜¯å°ç£çš„ä½ä¸¹å¥´å˜› XD\nDAY3 ä»Šå¤©è¦å¾å¾ˆæ£’çš„ OTTO é€€æˆ¿äº† T_T æ—©ä¸Šè¶å§åœ¨æº–å‚™çš„æ™‚å€™ï¼Œå…ˆå»çå¦®å°ç†Šæ‰›å››ç›’é¤…ä¹¾ç•¶ä¼´æ‰‹ç¦®ã€‚ä¹é»åŠå°±åˆ°äº†ï¼Œæ²’æƒ³åˆ°äººå·²ç¶“æ’äº†å¥½å¤§ä¸€åœˆï¼Œä¸éå¹¸å¥½çµå¸³æµç¨‹å¾ˆå¿«ï¼Œæ‰€ä»¥åé»ä¸€åˆ°æ²’å¤šä¹…å°±é †åˆ©è²·åˆ°äº†ï¼ é€€æˆ¿å¾Œå†å›åˆ°çå¦®å°ç†Šçš„åŒä¸€æ£Ÿä½å•†å¤§æ¨“ XD ç¬¬äºŒé–“é’æ—…ä¹Ÿåœ¨åŒä¸€æ£Ÿï¼Œå› ç‚ºä¸‹åˆä¸‰é»æ‰èƒ½å…¥ä½ï¼Œå¹¸å¥½å¯ä»¥å…ˆå…è²»å¯„æ”¾è¡Œæ ãƒ½(â—Â´âˆ€`â—)ï¾‰\néº¥æ–‡è¨˜éºµå®¶ ä¸å°å¿ƒæ²’æ³¨æ„åˆ°é–‹åº—æ™‚é–“ï¼Œé‚„æ²’ 12 é»å°±åˆ°äº†éº¥æ–‡è¨˜ï¼Œåœ¨é™„è¿‘æ™ƒæ™ƒå›å»å¾Œå‰›å¥½ç‡Ÿæ¥­ï¼Œé»äº†é®®è¦é›²åã€ç‰›è…©æ’ˆéºµé‚„æœ‰èŠ¥è˜­èœï¼Œé®®è¦è·Ÿç¶²è·¯è©•åƒ¹æ•˜è¿°ä¸€æ¨£çš„æ–°é®®å¥½åƒï¼Œå¦å¤–èŠ¥è˜­èœæ„å¤–é®®è„†ï½ åƒé£½å‡ºä¾†å¾Œä¸€æ¨£ç™¼ç¾åœ¨æ’éšŠäº†ï¼Œå„ªç§€çš„æ™‚é–“ç®¡ç†èƒ½åŠ› (ââÌ´Ì›á´—âÌ´Ì›â)â€¼\nä½³ä½³ç”œå“ æ­£é¤åƒå®Œå¾Œä¾†éš”å£åƒç”œå“ï¼Œé›–ç„¶éœ€è¦æ’éšŠï¼Œä¸éä¹Ÿå‰›å¥½åœ¨ç­‰çš„åŒæ™‚ä¸‹èµ·ä¸€å ´é›¨ï¼Œå¯æƒœçš„æ˜¯æ¥Šæç”˜éœ²è³£å®Œäº†ï¼Œæ‰€ä»¥é»äº†èŠéº»ç³Šæ‹¼æä»éœ²é‚„æœ‰æœ¨ç“œéŠ€è€³ã€‚èŠéº»ç³Šå¾ˆé¦™ï¼Œä½†æ˜¯æ˜¯å¯ä»¥æƒ³åƒçš„å‘³é“ã€‚ å½©è™¹é‚¨ç±ƒçƒå ´ ä¾†åˆ°å½©è™¹ç«™çš„å½©è™¹é‚¨ï¼Œç±ƒçƒå ´å¾ˆç‰¹åˆ¥çš„ä½åœ¨åœè»Šä¸Šé ‚æ¨“ï¼Œåœ¨é«˜åº¦çš„ç±ƒçƒå ´å‰›å¥½æ˜¯ä¸€å€‹å¾ˆå¥½çš„å¹³å°å°‡äº”é¡å…­è‰²çš„å¤§æ¨“æ”¶é€²ç›¸æ©Ÿç•«é¢è£¡ï¼ç„¶å¾ŒåŠŸèª²æ²’åšä»”ç´°ï¼Œåœ¨ç¤¾å€è£¡ç¹å…©åœˆæ²’æ‰¾åˆ°ç±ƒçƒå ´åˆ°åº•åœ¨å“ªï¼Œå°±åœ¨æ”¾æ£„ä¹‹éš›ï¼Œçœ‹åˆ°æœ‰å¾ˆæ˜é¡¯æ˜¯éŠå®¢çš„äººçˆ¬ä¸Šåœè»Šå ´é ‚æ¨“ï¼Œæ‰ç™¼ç¾åŸä¾†åœ¨ä¸Šé¢ áƒš(Â´â€¢Ğ´â€¢ Ì€áƒš\næ—ºè§’çš„å„ç¨®è¡— ä¾†åˆ°æ—ºè§’çš„å„ç¨®è¡—ä¸Šåƒæ±è¥¿\nå•Šä¸€æª¸æª¬èŒ¶ã€èŒ¶æ•‘æ˜Ÿçƒè‹¦ç“œæª¸æª¬èŒ¶ï¼Œåœ¨é¦™æ¸¯å˜—è©¦å„ç¨®ç‰Œå­çš„æª¸æª¬èŒ¶ XD çµè«–æ˜¯ç¬¬ä¸€å¤©å–çš„æ—é‡Œæœ€å¥½å–ï¼Œå•Šä¸€å®Œå…¨æ²’å‘³é“å•Šï¼ç„¶å¾Œè‹¦ç“œæª¸æª¬èŒ¶è »ç‰¹åˆ¥çš„ã€‚ èŠ±åœ’è¡—å¸‚å ´å¥‡è¶£é¤…å®¶ï¼Œé€™é–“æ˜¯ç„¡æ„é–“åœ¨å¸‚å ´è£¡é¢ç¶“éçš„ç”Ÿæ„å¾ˆå¥½çš„é¤…åº—ï¼Œè²·äº†ç´…è±†éº»ç³¬é¤…è·Ÿå…‰é…¥é¤…ï¼Œç´…è±†éº»ç³¬é¤…ä¸­è¦ä¸­çŸ©ï¼Œå…‰é…¥é¤…çš„éƒ¨ä»½åƒä¸å¤ªæ‡‚ XDD é‚„è¨˜å¾—åƒä¸å®Œæ‹¿ä¾†ç•¶éš”å¤©å¸¶è›‹å¡”å›å»å¡ç¸«éš™çš„é˜²æ’å¢Š =v= æ™¶è¯å†°å»³è è˜¿æ²¹ï¼Œä¸æ„§æ˜¯ç¶²ä¸Šè©•è«–ä¸­é¦™æ¸¯æœ€ä½³è è˜¿æ²¹ï¼ç›´æ¥ä¸ç”¨æ’éšŠå°±å¤–å¸¶è²·åˆ°ä¸€é¡†å‰›å‡ºçˆçš„ï¼Œå¤–è¡¨è è˜¿é…¥çš„éƒ¨åˆ†æ¥µè„†ï¼Œå¥¶æ²¹å¾ˆé¦™ï¼å¥½å¥½åƒ (à¹‘Â´ã…‚`à¹‘) è›‹ä»”è¨˜é›è›‹ä»”ï¼Œé¦™æ¸¯çš„ç‰¹è‰²å°åƒï¼Œä¸­è¦ä¸­çŸ©çš„å‘³é“è·Ÿå£æ„Ÿï¼Œåƒèµ·ä¾†å°±è·Ÿåœ¨å°ç£åƒçš„ä¸€æ¨£ ^_^ å¯Œè±ªé›ªç³•ï¼Œæ²’æœ‰ç‰¹åˆ¥ç´€éŒ„é›ªç³•è»Šå‡ºæ²’çš„ä½ç½®ï¼Œä½†çœŸçš„å¾ˆå®¹æ˜“é‡åˆ°ï¼Œé€™é‚Šæ‡‰è©²æ˜¯å‰›å¥½å¸æ©Ÿè‡¨æ™‚åœè»Šå›ä¾†ï¼Œè¢«å‰é¢çš„å®¢äººæ””æˆªï¼Œæˆ‘å€‘è·Ÿè‘—æ’ï¼Œè²·å®Œå°±é–‹èµ°äº† ğŸ¤£ å‚³çµ±çš„é¦™è‰é›ªç³•ï¼Œä½†åƒèµ·ä¾†æ„å¤–æ¸…çˆ½ï½ ç´€éŒ„æ—ºè§’çš„ç†±é¬§è¡—é ­ï¼Œé‚„æœ‰åœ¨æ—ºè§’ç‰¹åœ°èµ°äº†ä¸€å€‹ä¸Šæµ·è¡— 618 çš„æ™¯é»ï¼Œé€™é‚Šæœ‰é»åƒæ¾å±±æ–‡å‰µï¼Œä¸€æ¨“é‚„æœ‰æ¨‚åœ˜åœ¨æ¼”å¥ï¼Œæ•´æ£Ÿæ¨“éƒ½æ˜¯è²©è³£ä¸€äº›æ–‡é’çš„å°ç‰©ã€‚ å¸è‹‘æ‹¿é¤… å¾æ—ºè§’ç›´æ¥æ­å…¬è»Šå›åˆ°å°–æ²™å’€çš„å¸è‹‘é…’åº—æ‹¿ä¸€å€‹ç¦®æ‹œå‰åœ¨ç¶²è·¯ä¸Šè¨‚å¥½çš„è´è¶é…¥ï½è²·äº†å…©ç›’å·§å…‹åŠ›è·Ÿå…©è¢‹åŸå‘³ï¼Œå·§å…‹åŠ›å¾ˆè²´ï¼Œä½†å›å®¶äº«ç”¨å¾Œæ‰ç†è§£ä»–çš„åƒ¹å€¼ï¼Œé¤…ä¹¾çš„éƒ¨åˆ†å¾ˆè„†ï¼Œä½†ä¸Šé¢çš„å·§å…‹åŠ›å±…ç„¶æ˜¯æ¿•æ½¤çš„ã€ç”œåº¦å‰›å¥½ï¼Œé‚„å¥½æœ‰è²·å…©ç›’ï¼\nåœ¨å»çš„è·¯ä¸Šå‰›å¥½å¯ä»¥æ¬£è³ç™½å¤©çš„ç¶­å¤šåˆ©äºæ¸¯å£ï½ 168 Hostel å‚æ™šå›åˆ°é’æ—…ï¼Œçµ‚æ–¼å¯ä»¥ checkin çœ‹çœ‹å…§è£åˆ°åº•é•·æ€æ¨£äº†ï¼Œé¦–å…ˆé’æ—…æ˜¯åœ¨ä¸€æ£Ÿå¾ˆæœ‰å¹´ä»£æ„Ÿçš„ä½å•†å¤§æ¨“è£¡é¢ï¼Œå¤šæœ‰å¹´ä»£æ„Ÿå¯ä»¥åƒè€ƒçå¦®å°ç†Šæ’éšŠçš„ç…§ç‰‡ç’°å¢ƒ XD å¹¸å¥½æ—©ä¸Šä¾†æ¢éè·¯ï¼Œä¸é›£æ‰¾åˆ°ä½ç½®ã€‚é’æ—…å…§éƒ¨é›–ä¹Ÿçœ‹èµ·ä¾†è€èˆŠï¼Œä½†æ‰€å¹¸ç’°å¢ƒå¾ˆæ•´æ½”ï¼Œé‚„æœ‰é£²æ°´æ©Ÿå¯ä»¥ä½¿ç”¨ã€‚ ç‚¯è¨˜ç‡’è‡˜ åŸæœ¬é€™è¶Ÿè¡Œç¨‹è¦åƒæœ‰åçš„ç”˜ç‰Œç‡’éµï¼Œä½†ç¬¬äºŒå¤©çœ‹åˆ°ç¶“éæ™‚çš„äººæ½®å°±æœæ–·æ”¾æ£„ï¼Œæ‰€ä»¥ç•™æœ€å¾Œä¸€å€‹æ™šä¸Šåœ¨é™„è¿‘æ‰¾ç‡’è‡˜åº—åƒã€‚é»äº†å…©ä»½é›™æ‹¼é¸äº†å››ç¨®ä¸ä¸€æ¨£çš„ç‡’è‡˜ï¼Œå‰ç‡’ã€æ²¹é›ã€è…©ä»”ã€ç‡’éµï¼Œç‡’éµæœç„¶æ˜¯æœ€å²å®³çš„ï¼é£¯ä¹Ÿæ˜¯ç²’ç²’åˆ†æ˜ï¼ŒåŠ ä¸Šéµæ²¹å¾ˆé¦™å¾ˆå¥½åƒã€‚ DAY4 BakeHouse ç•™åˆ°ç¬¬å››å¤©çš„æœ€çµ‚ç›®çš„å°±æ˜¯å»æ—…é¤¨é™„è¿‘çš„ bakehouse å¸¶è›‹å¡”å›å°ç£ï¼ï¼ï¼æ‰€ä»¥æ—©ä¸Šå…­é»åŠèµ·åºŠï¼Œä¸åˆ°ä¸ƒé»å°±å»åº—é–€å£æ’éšŠäº†ï¼Œåˆ°çš„æ™‚å€™å·²ç¶“æœ‰å…©å€‹äººåœ¨é–€å£ç­‰å€™äº†ï¼Œç­‰åˆ°å…«é»ä¸€é–‹é–€ï¼Œå¾Œé¢ç›®æ¸¬æ‡‰è©²æ’äº† 30 å€‹äººæœ‰ï¼Œæœç„¶é‚„æ˜¯æ—©èµ·çš„é³¥å…’æœ‰èŸ²åƒ ğŸ¥¹ è²·äº†å…©ç›’å¤–åŠ ä¸€é¡†å¯é Œç•¶æ—©é¤\nå›ç¨‹ - A21ã€é¦™æ¸¯æ©Ÿå ´ç¿ åœ’ è²·å®Œè›‹å¡”å°±åˆ°é£¯åº—å°é¢çš„è»Šç«™ç‰Œç­‰ A21 å›æ©Ÿå ´ï¼Œå‡ºé–€å°±ç™¼ç¾ä¸‹èµ·å¤§é›¨ï¼Œå¹¸å¥½è¦å›ç¨‹å•¦ï¼å›ç¨‹çš„ A21 äººå°±éå¸¸å¤šï¼Œé‚„å¥½æˆ‘å€‘ä¸Šè»Šçš„å°–æ²™å’€ç«™åœ¨å‰æ®µï¼Œé‚„æœ‰ä½ç½®å¯ä»¥ååˆ°æ©Ÿå ´ã€‚\nåˆ°æ©Ÿå ´å¾Œå»åƒäº†æœ€å¾Œä¸€é¤æ¸¯å¼ï¼Œç¿ åœ’ï¼Œå¥¶æ²¹è±¬ä»”åŒ…å‰›å‡ºçˆå¤–è¡¨é…¥è„†ã€å…§è£¡é¬†è»Ÿï¼Œç„¶å¾Œéºµé£Ÿé»äº†ç‰›è‚‰æ²³ç²‰é‚„æœ‰é…¸è¾£è¦ç±³ç²‰ï¼Œæ¹¯é ­éƒ½å¾ˆå¥½å–ï¼Œé‚„æœ‰æ©Ÿæœƒçµ•å°è¦å†é»é…¸è¾£è¦ç±³ç²‰ï¼ï¼ï¼ THE END å®Œç¾çš„çµæŸé¦™æ¸¯çš„ç¾é£Ÿä¹‹æ—…å•¦ï½è¡Œç¨‹æ’å¾—éƒ½å¾ˆæ»¿æ„ï¼Œè©²åƒçš„éƒ½æœ‰åƒåˆ°ï¼Œæ²’æœ‰æ’å¤ªå¤šéšŠï¼Œä¸é¬†ä¸ç·Šçš„æ™¯é»ï¼Œå¾ˆæ£’ï¼ â™¥(Â´âˆ€` )\n","date":"2024-11-01T15:00:00+08:00","image":"http://localhost:1313/p/hongkong-2024/cover_hu7953674859307773612.jpg","permalink":"http://localhost:1313/p/hongkong-2024/","title":"å››å¤©ä¸‰å¤œé¦™æ¸¯éŠè¨˜ğŸ¥®"},{"content":"TL; DR æœ¬æ–‡è¨˜éŒ„åœ¨ Kubernetes é›†ç¾¤ä¸Šéƒ¨ç½² Apache Streampark çš„æ­¥é©Ÿï¼ŒStreampark æ˜¯ä¸€å€‹é–‹æºé …ç›®ï¼Œç°¡åŒ–äº†åœ¨ Kubernetes ä¸Šéƒ¨ç½²å’Œç®¡ç† Flink æ‡‰ç”¨ç¨‹åºã€‚\nå®¢è£½åŒ– Streampark image ä¸»è¦è¦é”æˆä»¥ä¸‹ç›®çš„\nå› éœ€è¦åœ¨ Streampark ä¸­å»ºç«‹ flink é›†ç¾¤ï¼Œå»ºç«‹é›†ç¾¤éœ€æŒ‡å®š FLINK_HOMEï¼Œä½†é è¨­çš„ image æ²’æœ‰åŒ…å« flinkï¼Œæ‰€ä»¥éœ€è¦å°‡æœƒç”¨åˆ°çš„ flink ç‰ˆæœ¬ä¸€æ¬¡åŒ…å¥½ã€‚ ç”±æ–¼ Apache å°ˆæ¡ˆèˆ‡ mysql jdbc é©…å‹•çš„ license ä¸ç›¸å®¹ï¼Œé ˆé¡å¤–ä¸‹è¼‰ mysql jdbc é©…å‹•ï¼Œä¸¦å°‡å…¶æ”¾åœ¨ $STREAMPARK_HOME/lib ä¸‹ã€‚ å‰å¾€å®˜ç¶²ä¸‹è¼‰æœ€æ–°ç‰ˆæœ¬ https://streampark.apache.org/download/ ï¼Œä¸¦æº–å‚™ä»¥ä¸‹ Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 FROM alpine:3.20 as deps-stage COPY apache-streampark_*-*-bin.tar.gz / WORKDIR / RUN tar zxvf apache-streampark_*-*-bin.tar.gz \\ \u0026amp;\u0026amp; mv apache-streampark_*-*-bin streampark FROM apache/flink:1.17.2 as flink-1.17 FROM apache/flink:1.18.1 as flink-1.18 FROM apache/flink:1.19.1 as flink-1.19 FROM docker:dind WORKDIR /streampark COPY --from=deps-stage /streampark /streampark ENV NODE_VERSION=16.1.0 ENV NPM_VERSION=7.11.2 RUN apk add openjdk8 \\ \u0026amp;\u0026amp; apk add maven \\ \u0026amp;\u0026amp; apk add wget \\ \u0026amp;\u0026amp; apk add vim \\ \u0026amp;\u0026amp; apk add bash \\ \u0026amp;\u0026amp; apk add curl ENV JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk ENV MAVEN_HOME=/usr/share/java/maven-3 ENV PATH $JAVA_HOME/bin:$PATH ENV PATH $MAVEN_HOME/bin:$PATH RUN wget \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; tar zxvf \u0026#34;node-v$NODE_VERSION-linux-x64.tar.gz\u0026#34; -C /usr/local --strip-components=1 \\ \u0026amp;\u0026amp; rm \u0026#34;node-v$NODE_VERSION-linux-x64.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs \\ \u0026amp;\u0026amp; curl -LO https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl \\ \u0026amp;\u0026amp; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl RUN mkdir -p ~/.kube # Copy Flink installations from base images COPY --from=flink-1.17 /opt/flink /streampark/flink-1.17 COPY --from=flink-1.18 /opt/flink /streampark/flink-1.18 COPY --from=flink-1.19 /opt/flink /streampark/flink-1.19 # Download and install JDBC connector RUN wget -P /streampark/lib https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.31/mysql-connector-j-8.0.31.jar EXPOSE 10000 æ‰“åŒ…é¡åƒ\n1 docker buildx build -t streampark-flink:2.1.4 . éƒ¨ç½² MySQL ::: info Streampark ä½¿ç”¨ h2, pgsql, mysql å…¶ä¸­ä¸€å€‹ Database å­˜æ”¾ metadataï¼Œæœ¬ä¾‹ä½¿ç”¨ MySQLã€‚ :::\næº–å‚™ init.sql æª”æ¡ˆ\nåœ¨ä¸‹è¼‰çš„ Streampark ç›®éŒ„ä¸­æ‰¾ mysql çš„ DDL æª”\nä¸»è¦éœ€è¦å»ºç«‹çš„æ˜¯ mysql-schema.sql ä»¥åŠ mysql-data.sqlï¼Œåˆä½µå…©å€‹ sql æª”åˆä½µï¼Œä¸¦ä»¥ init.sql å‘½åã€‚\n1 cat ./incubator-streampark-2.1.4/streampark-console/streampark-console-service/src/main/assembly/script/schema/mysql-schema.sql ./incubator-streampark-2.1.4/streampark-console/streampark-console-service/src/main/assembly/script/data/mysql-data.sql \u0026gt; init.sql å»ºç«‹ mysql init.sql çš„ configMaps\n1 kubectl create configmap mysql-initdb-config --from-file=init.sql -n streampark å»ºç«‹ mysql statefulset\nstreampark-mysql-sts.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 --- apiVersion: v1 kind: Service metadata: name: mysql-service namespace: streampark spec: type: NodePort selector: app: mysql ports: - protocol: TCP port: 3306 targetPort: 3306 nodePort: 30306 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: streampark spec: serviceName: \u0026#39;mysql-service\u0026#39; replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: securityContext: fsGroup: 1000 runAsUser: 1000 containers: - name: mysql image: mysql:8.3 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#39;streampark\u0026#39; - name: MYSQL_USER value: \u0026#39;streampark\u0026#39; - name: MYSQL_PASSWORD value: \u0026#39;streampark\u0026#39; volumeMounts: - mountPath: /var/lib/mysql name: mysql-storage - mountPath: /docker-entrypoint-initdb.d name: initdb-scripts volumes: - name: initdb-scripts configMap: name: mysql-initdb-config volumeClaimTemplates: - metadata: name: mysql-storage namespace: streampark spec: accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] storageClassName: \u0026#39;ceph-block\u0026#39; resources: requests: storage: 10Gi 1 kubectl apply -f streampark-mysql-sts.yaml å»ºç«‹ secret ä»¥ä¾›å¾Œé¢ Streampark éƒ¨ç½²ä½¿ç”¨\n1 2 3 kubectl create secret generic streampark-mysql \\ --namespace=streampark \\ --from-literal=mysql-root-password=streampark Streampark éƒ¨ç½² é€é helm ç”¢ç”Ÿéƒ¨ç½²æª” åœ¨ä¸Šä¸€æ­¥é©Ÿä¸‹è¼‰çš„ Streampark ä¸­åŒ…å«äº† helm chart å¯ç”¨æ–¼ Kubernetes çš„éƒ¨ç½²ï¼Œ\né€²å…¥ helm/streampark ç›®éŒ„ä¿®æ”¹ values.yaml ï¼Œä»¥è¨»è§£èªªæ˜æœ¬æ–‡ä¿®æ”¹çš„åœ°æ–¹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 image: repository: \u0026#39;harbor.sdsp-stg.com/base/streampark-flink\u0026#39; pullPolicy: \u0026#39;IfNotPresent\u0026#39; tag: \u0026#39;2.1.4\u0026#39; pullSecret: \u0026#39;\u0026#39; rbac: create: true spec: container: env: [ { name: TZ, value: \u0026#39;Asia/Taipei\u0026#39; }, { name: LANG, value: en_US.UTF-8 }, # æŒ‡å®šè¦ä½¿ç”¨çš„ DOCKER HOSTï¼Œè©³æƒ…è«‹åƒè€ƒéƒ¨ç½²éšæ®µèªªæ˜ { name: DOCKER_HOST, value: \u0026#39;tcp://172.21.18.51:2375\u0026#39; }, { name: LANGUAGE, value: en_US:en }, { name: LC_ALL, value: en_US.UTF-8 }, ] replicaCount: 1 containerPort: 10000 name: rest affinity: {} nodeSelector: {} tolerations: [] # åŠ å¤§ resource limit çš„è³‡æºé™åˆ¶ resources: { limits: { memory: \u0026#39;8Gi\u0026#39;, cpu: \u0026#39;4\u0026#39; }, requests: { memory: \u0026#39;1Gi\u0026#39;, cpu: \u0026#39;1\u0026#39; }, } livenessProbe: enabled: true initialDelaySeconds: \u0026#39;90\u0026#39; periodSeconds: \u0026#39;30\u0026#39; timeoutSeconds: \u0026#39;20\u0026#39; failureThreshold: \u0026#39;3\u0026#39; successThreshold: \u0026#39;1\u0026#39; readinessProbe: enabled: true initialDelaySeconds: \u0026#39;90\u0026#39; periodSeconds: \u0026#39;30\u0026#39; timeoutSeconds: \u0026#39;20\u0026#39; failureThreshold: \u0026#39;3\u0026#39; successThreshold: \u0026#39;1\u0026#39; # å®šç¾© ingressï¼Œä¸¦ä½¿ç”¨ Prefix çš„æ–¹å¼å»ºç«‹ï¼Œå¦å¤–ç§»é™¤ä¸å¿…è¦çš„ annotation (å¾ŒçºŒéœ€è¦æ‰‹å‹•ä¿®æ”¹ yaml åŠ ä¸Š ingressClassName ingress: enabled: true host: \u0026#39;streampark.sdsp-stg.com\u0026#39; path: \u0026#39;/\u0026#39; pathType: \u0026#39;Prefix\u0026#39; # å› ä½¿ç”¨ ingress å°å¤–é–‹æ”¾ï¼Œå°±å°‡ service æ”¹ç‚º ClusterIP service: type: \u0026#39;ClusterIP\u0026#39; name: \u0026#39;streampark-service\u0026#39; streamParkDefaultConfiguration: create: true append: true streamParkServiceAccount: create: true annotations: {} name: \u0026#39;streampark\u0026#39; è¨­å®š ./streampark/conf/streampark-console-config ç›®éŒ„ä¸‹çš„é…ç½®æª”\nä¸»è¦ç§»é™¤äº†ç”¨ä¸åˆ°çš„ application-h2.yml ã€ application-pgsql.yml ã€ application-sso.yml æª”æ¡ˆ\nä¸¦ä¸”ä¿®æ”¹ application.yml ä»¥åŠ application-mysql.yml å…§å®¹\n1 2 3 4 # application.yml ä¿®æ”¹ database ç‚º mysqlï¼Œå…¶æ–¼ä¿æŒé è¨­ spring: profiles: active: mysql 1 2 3 4 5 6 7 # application-mysql.yml æ›´æ–°é€£ç·šè³‡è¨Š spring: datasource: username: root password: streampark driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://mysql-service.streampark.svc.cluster.local:3306/streampark?useSSL=false\u0026amp;useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;allowPublicKeyRetrieval=false\u0026amp;useJDBCCompliantTimezoneShift=true\u0026amp;useLegacyDatetimeCode=false\u0026amp;serverTimezone=GMT%2B8 ç”¢ç”Ÿ YAML æª”\n1 helm template streampark/ -n streampark -f streampark/values.yaml --output-dir ./result ç”¢ç”Ÿçš„ YAML æª”æœƒæ”¾åœ¨ result ç›®éŒ„ä¸‹ã€‚\nå®¢è£½åŒ–éƒ¨ç½²æª” ç”¢ç”Ÿ kube config çš„ configmaps ä¾› Streampark ä½¿ç”¨ Kubernetes éƒ¨ç½² flink cluster\n1 kubectl create configmap my-kube-config --from-file=$HOME/.kube/config -n streampark ç·¨è¼¯ result/ingress.yaml ï¼ŒæŒ‡å®š ingressClassName\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: streampark namespace: streampark labels: app.kubernetes.io/name: streampark app.kubernetes.io/version: \u0026#39;2.1.4\u0026#39; app.kubernetes.io/managed-by: Helm helm.sh/chart: streampark-2.1.4 spec: ingressClassName: nginx rules: - host: streampark.sdsp-stg.com http: paths: - backend: service: name: streampark-service port: name: rest path: / pathType: Prefix ç·¨è¼¯ result/streampark.yaml æ–°å¢ initContainers æ¸¬è©¦ MySQL è³‡æ–™åº«é€£ç·šã€ç¶å…¥ kube configã€ä»¥åŠæŒä¹…åŒ–å°ˆæ¡ˆç›®éŒ„ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 apiVersion: apps/v1 kind: Deployment metadata: name: streampark namespace: streampark labels: app.kubernetes.io/name: streampark app.kubernetes.io/version: \u0026#39;2.1.4\u0026#39; app.kubernetes.io/managed-by: Helm helm.sh/chart: streampark-2.1.4 spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: streampark template: metadata: labels: app.kubernetes.io/name: streampark spec: serviceAccountName: streampark initContainers: - name: db-service-check image: mysql:8.3 command: - sh - \u0026#39;-c\u0026#39; - mysqladmin ping -h mysql-service.streampark.svc.cluster.local -uroot -p\u0026#39;${MYSQL_ROOT_PASSWORD}\u0026#39; --silent env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: streampark-mysql key: mysql-root-password containers: - image: harbor.sdsp-stg.com/base/streampark-flink:2.1.4 name: streampark imagePullPolicy: Always ports: - name: rest containerPort: 10000 protocol: TCP env: - name: TZ value: Asia/Taipei - name: LANG value: en_US.UTF-8 - name: LANGUAGE value: en_US:en - name: LC_ALL value: en_US.UTF-8 - name: DOCKER_HOST value: tcp://172.21.18.51:2375 securityContext: privileged: false command: [ \u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;bash ./bin/streampark.sh start_docker\u0026#39;, ] livenessProbe: exec: command: [ \u0026#39;curl\u0026#39;, \u0026#39;-s\u0026#39;, \u0026#39;http://localhost:10000/actuator/health/liveness\u0026#39;, ] initialDelaySeconds: 90 periodSeconds: 30 timeoutSeconds: 20 successThreshold: 1 failureThreshold: 3 readinessProbe: exec: command: [ \u0026#39;curl\u0026#39;, \u0026#39;-s\u0026#39;, \u0026#39;http://localhost:10000/actuator/health/readiness\u0026#39;, ] initialDelaySeconds: 90 periodSeconds: 30 timeoutSeconds: 20 successThreshold: 1 failureThreshold: 3 volumeMounts: - name: streampark-default-config-volume mountPath: /streampark/conf - name: kube-config-volume mountPath: /root/.kube - name: streampark-storage-volume mountPath: /opt resources: limits: cpu: \u0026#39;4\u0026#39; memory: 8Gi requests: cpu: \u0026#39;1\u0026#39; memory: 1Gi volumes: - name: streampark-default-config-volume configMap: name: streampark-console-config items: - key: application.yml path: application.yml - key: application-mysql.yml path: application-mysql.yml - key: logback-spring.xml path: logback-spring.xml - key: kerberos.yml path: kerberos.yml - key: spy.properties path: spy.properties - key: ValidationMessages.properties path: ValidationMessages.properties - name: kube-config-volume configMap: name: my-kube-config items: - key: config path: config - name: streampark-storage-volume persistentVolumeClaim: claimName: streampark-pvc éƒ¨ç½² åœ¨éƒ¨ç½²å‰éœ€è¦å…ˆæ‰¾ä¸€å°æœ‰ docker çš„æ©Ÿå™¨å°‡ docker daemon å…¬é–‹å‡ºä¾†ï¼ŒåŸå› æ˜¯å› ç‚ºç•¶ä½¿ç”¨ streampark application mode éƒ¨ç½² flink çš„æ™‚å€™ï¼Œéœ€è¦è¨­å®š DOCKE_HOSTï¼Œä¸¦ä¸”é‚„éœ€è¦é€é docker å»åŸ·è¡Œæ‰“åŒ…ä»¥åŠæ¨é€é¡åƒã€‚\né€²å…¥åˆ°æ¬²å…¬é–‹ä½¿ç”¨çš„ docker hostï¼Œç·¨è¼¯ /etc/docker/daemon.json\n1 2 3 { \u0026#34;hosts\u0026#34;: [\u0026#34;tcp://0.0.0.0:2375\u0026#34;,\u0026#34;unix:///var/run/docker.sock\u0026#34;] } æ¥è‘—é‡å•Ÿ docker sudo systemctl restart dockerï¼Œå°±å¯ä»¥ç™¼ç¾åˆ° docker post è¢«å…¬é–‹äº†ã€‚\nå›åˆ° Kubernetes é–‹å§‹æ­£å¼éƒ¨ç½²\n1 kubectl apply -f ./result ","date":"2024-08-09T10:10:17+08:00","permalink":"http://localhost:1313/p/data-apache-streampark-k8s-deployment/","title":"Apache Streampark Deployment on Kubernetes"},{"content":"TL; DR ç›®å‰çš„ Kubernetes HA æ¶æ§‹æ˜¯ä½¿ç”¨ Nginx ç•¶ Loadbalancerï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨ Kubespray éƒ¨ç½²æ™‚å¯ä»¥è¨­å®š api server çš„ endpoint ç‚º external LBã€‚\ninventory/sample/group_vars/all/all.yml\n1 2 3 4 5 ## External LB example config ## apiserver_loadbalancer_domain_name: \u0026#34;elb.some.domain\u0026#34; # loadbalancer_apiserver: # address: 1.2.3.4 # port: 1234 æ‰€ä»¥åœ¨éƒ¨ç½² ansible playbook å‰éœ€è¦å¤šä¿®æ”¹ group_vars\n1 vi inventory/mycluster/group_vars/all/all.yml å°‡ä¸Šé¢çš„ External LB example config æ‹¿æ‰è¨»è§£ï¼Œä¸¦è¨­å®š\n1 2 3 4 5 apiserver_loadbalancer_domain_name: \u0026#34;k8s.sdsp-dev.com\u0026#34; # è¨­å®š nginx IP ä»¥åŠè½‰ç™¼çš„ port loadbalancer_apiserver: address: 172.20.37.19 port: 6443 è£œå…… nginx çš„ stream config\nReference https://www.youtube.com/watch?v=u_1f3WyvtQE ","date":"2024-06-10T11:36:21+08:00","permalink":"http://localhost:1313/p/k8s-kubespray-exteranl-loadbalancer/","title":"Kubespray Setup for External Load Balancer"},{"content":"Introduction Kafka ç”Ÿæ…‹ç³»é™¤äº† Kafka Broker å¤–ï¼Œé‚„æœ‰ Kafka Connect, Kafka Bridge, Mirror Maker â€¦ ç­‰ï¼Œé€é Operator ä¾†æ¶è¨­æœƒæ¯”éœ€è¦å¯«å¤šçš„ manifest æˆ–è£å¤šå€‹ helm chart ä¾†çš„å¥½ç”¨ã€‚æœ¬ç¯‡é¸æ“‡ GitHub ä¸Š Kafka Operator project æ˜Ÿæ˜Ÿæ•¸æœ€å¤šçš„ Strimziï¼Œä¸‹è¡¨ç‚º Stimzi æä¾›å¯ä»¥éƒ¨å±¬çš„è³‡æºåˆ—è¡¨ã€‚\nStrimzi Operator Deployment Best Practice å°‡ Strimzi Operator å®‰è£åœ¨å…¶ç®¡ç†çš„ Kafka Cluster åŠå…¶ä»– Kafka component ä¸åŒçš„ namespace ä¸­ï¼Œä»¥ç¢ºä¿è³‡æºå’Œé…ç½®çš„æ˜ç¢ºåˆ†é›¢ã€‚ ä¸€åº§ Kubernetes åªå®‰è£å–®ä¸€ Strimzi Operator ä¾†ç®¡ç†æ‰€æœ‰ Kafka å¯¦ä¾‹ã€‚ æ›´æ–° Strimzi Operator å’Œæ”¯æ´çš„ Kafka ç‰ˆæœ¬ï¼Œä»¥åæ˜ æœ€æ–°çš„åŠŸèƒ½å’Œå¢å¼·åŠŸèƒ½ã€‚ å®‰è£ Operator 1 2 kubectl create ns kafka kubectl create -f \u0026#39;https://strimzi.io/install/latest?namespace=kafka\u0026#39; -n kafka éƒ¨ç½² kafka cluster 1 2 3 4 5 wget https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.41.0/strimzi-0.41.0.tar.gz tar zxvf strimzi-0.41.0.tar.gz cd strimzi-0.41.0 cp examples/kafka/kraft/kafka.yaml . vi kafka.yaml ä¿®æ”¹éƒ¨ç½²æ–‡ä»¶ï¼ŒKRaft æ¨¡å¼éƒ¨ç½² Kafka å¢é›†éœ€è¦ä½¿ç”¨ KafkaNodePool è³‡æºï¼Œæ‰€ä»¥ä¸Šé¢å…©å€‹ node pool çš„ yaml æ˜¯å¿…é ˆéƒ¨ç½²çš„è³‡æºã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: controller labels: strimzi.io/cluster: my-cluster spec: replicas: 3 roles: - controller storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi kraftMetadata: shared deleteClaim: false class: ceph-csi-rbd-hdd # æ›¿æ›æˆç¾æœ‰çš„ storage class --- apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: broker labels: strimzi.io/cluster: my-cluster spec: replicas: 3 roles: - broker storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi kraftMetadata: shared deleteClaim: false class: ceph-csi-rbd-hdd # æ›¿æ›æˆç¾æœ‰çš„ storage class --- apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata: name: my-cluster annotations: strimzi.io/node-pools: enabled strimzi.io/kraft: enabled spec: kafka: version: 3.7.1 metadataVersion: 3.7-IV4 listeners: - name: plain port: 9092 type: internal tls: false - name: external port: 9094 type: nodeport # æ–°å¢å°å¤– nodeport æœå‹™ tls: false configuration: bootstrap: nodePort: 32100 # æŒ‡å®š bootstrap å ç”¨çš„ nodeportï¼Œbroker å¦‚ä¸ä¸€ä¸€æŒ‡å®šçš„è©±ï¼ŒOperator æœƒè‡ªå‹•æŒ‡æ´¾ brokers: - broker: 0 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8091 - broker: 1 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8092 - broker: 2 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8093 config: offsets.topic.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 default.replication.factor: 3 min.insync.replicas: 2 entityOperator: topicOperator: {} userOperator: {} é–‹å§‹éƒ¨ç½²\n1 kubectl -n kafka apply -f kafka.yaml æŸ¥çœ‹ kafka ä»¥åŠ kafka node pool customized resource\nç‚º Kafka è¨­å®š Load Balance Nginx Load Balance çš„è² è¼‰è½‰ç™¼è¦å‰‡é ˆå°‡ Bootstrap ä»¥åŠ Brokers æ‰€å ç”¨çš„æ‰€æœ‰ Worker Node çš„ Node Portã€‚\nä»¥ä¸‹æ˜¯ Bootstrap port çš„è¨­å®šç¯„ä¾‹ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 upstream tcp9094 { server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } éƒ¨ç½²æ¶æ§‹ è¨»é‡‹\nåœ¨è¨­ç½® external çš„è¨­å®šæ™‚ï¼Œéœ€è¦ä¹ŸæŠŠæ¯å€‹ broker çš„ nodeport æŒ‡å®šå¥½ï¼Œå¦å‰‡ç•¶å®¢æˆ¶ç«¯å­˜åœ¨æ–¼ç¶²æ®µå¤–éƒ¨æ™‚ï¼Œæœƒé‡åˆ° Disconnected from node 1 due to timeout çš„éŒ¯èª¤ã€‚\nä¾‹å¦‚ï¼š\n1 2 3 4 5 [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 2 disconnected. [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Cancelled in-flight API_VERSIONS request with correlation id 2 due to node 2 being disconnected (elapsed time since creation: 7ms, elapsed time since send: 7ms, request timeout: 3600000ms) [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 1 disconnected. [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Cancelled in-flight API_VERSIONS request with correlation id 3 due to node 1 being disconnected (elapsed time since creation: 4ms, elapsed time since send: 4ms, request timeout: 3600000ms) [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 0 disconnected. æˆ–æ˜¯\n1 2 3 4 5 6 7 8 9 10 11 [2024-09-18 16:28:17,666] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 4 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 5 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 6 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 7 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,670] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 8 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:47,774] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 12 on topic-partition mocktest-0, retrying (2147483645 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:29:17,883] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 15 on topic-partition mocktest-0, retrying (2147483644 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation ","date":"2024-06-01T12:45:10+08:00","permalink":"http://localhost:1313/p/k8s-strimzi-kafka-kraft-cluster/","title":"[Kubernetes] ä½¿ç”¨ Strimzi å®‰è£ Kafka Kraft é›†ç¾¤"},{"content":"TL; DR æœ¬ç¯‡æ–‡ç« è¨˜éŒ„ä½¿ç”¨ bitnami helm chart å®‰è£ kafka Kraft mode çš„å°å¤–é›†ç¾¤ã€‚\næº–å‚™ values.yaml æ–‡ä»¶ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 global: storageClass: ceph-csi-rbd-hdd heapOpts: \u0026#34;-Xmx6g -Xms6g\u0026#34; listeners: interbroker: name: INTERNAL containerPort: 9092 protocol: PLAINTEXT controller: name: CONTROLLER containerPort: 9093 protocol: PLAINTEXT client: name: CLIENT containerPort: 9095 protocol: PLAINTEXT external: containerPort: 9094 protocol: PLAINTEXT name: EXTERNAL controller: replicaCount: 3 persistence: size: 50Gi broker: replicaCount: 3 persistence: size: 300Gi externalAccess: enabled: true controller: forceExpose: false service: type: NodePort ports: external: 9094 nodePorts: - 30494 - 30594 - 30694 useHostIPs: true broker: service: type: NodePort ports: external: 9094 nodePorts: - 30194 - 30294 - 30394 useHostIPs: true volumePermissions: enabled: true rbac: create: true kraft: clusterId: M2VhY2Q3NGQ0NGYzNDg2YW éƒ¨ç½² 1 helm upgrade --install -name kafka bitnami/kafka --namespace kafka -f values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 Kafka can be accessed by consumers via port 9095 on the following DNS name from within your cluster: kafka.kafka.svc.cluster.local Each Kafka broker can be accessed by producers via port 9095 on the following DNS name(s) from within your cluster: kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-1.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-broker-0.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-1.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-2.kafka-broker-headless.kafka.svc.cluster.local:9095 ç‚º Kafka è¨­å®š Load Balance éƒ¨ç½²å®Œç•¢å¾Œå¯ä»¥çœ‹åˆ°æ¯å€‹ broker / controller çš†ä½¿ç”¨ NodePort å°å¤–é–‹æ”¾ï¼Œå¯ä»¥ç‚º kafka è¨­å®š Load Balance ä»¥æä¾›å¤–éƒ¨å­˜å–çš„ client é€éçµ±ä¸€çš„å…¥å£é»å­˜å–ã€‚\nç›®å‰ Kubernetes å¤–éƒ¨å·²ç¶“ç‚ºé›†ç¾¤å»ºç«‹ä¸€å€‹ Nginxï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥ä½¿ç”¨é€™å° server ä¹Ÿç‚º Kafka çš„ TCP æµé‡è¨­ç½®è² è¼‰è½‰ç™¼ã€‚å…¶è¨­å®šæª”å¦‚ä¸‹ï¼Œå°‡æ‰€æœ‰æœƒè¢«é–‹åˆ° Node Port çš„ Worker Node ç¯€é»ä¸€æ¬¡è¨­ç½®ä¸Šå»ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 upstream tcp9094 { server 172.20.37.36:30194 max_fails=3 fail_timeout=30s; server 172.20.37.36:30294 max_fails=3 fail_timeout=30s; server 172.20.37.36:30394 max_fails=3 fail_timeout=30s; server 172.20.37.36:30494 max_fails=3 fail_timeout=30s; server 172.20.37.36:30594 max_fails=3 fail_timeout=30s; server 172.20.37.36:30694 max_fails=3 fail_timeout=30s; server 172.20.37.37:30194 max_fails=3 fail_timeout=30s; server 172.20.37.37:30294 max_fails=3 fail_timeout=30s; server 172.20.37.37:30394 max_fails=3 fail_timeout=30s; server 172.20.37.37:30494 max_fails=3 fail_timeout=30s; server 172.20.37.37:30594 max_fails=3 fail_timeout=30s; server 172.20.37.37:30694 max_fails=3 fail_timeout=30s; server 172.20.37.38:30194 max_fails=3 fail_timeout=30s; server 172.20.37.38:30294 max_fails=3 fail_timeout=30s; server 172.20.37.38:30394 max_fails=3 fail_timeout=30s; server 172.20.37.38:30494 max_fails=3 fail_timeout=30s; server 172.20.37.38:30594 max_fails=3 fail_timeout=30s; server 172.20.37.38:30694 max_fails=3 fail_timeout=30s; server 172.20.37.39:30194 max_fails=3 fail_timeout=30s; server 172.20.37.39:30294 max_fails=3 fail_timeout=30s; server 172.20.37.39:30394 max_fails=3 fail_timeout=30s; server 172.20.37.39:30494 max_fails=3 fail_timeout=30s; server 172.20.37.39:30594 max_fails=3 fail_timeout=30s; server 172.20.37.39:30694 max_fails=3 fail_timeout=30s; server 172.20.37.40:30194 max_fails=3 fail_timeout=30s; server 172.20.37.40:30294 max_fails=3 fail_timeout=30s; server 172.20.37.40:30394 max_fails=3 fail_timeout=30s; server 172.20.37.40:30494 max_fails=3 fail_timeout=30s; server 172.20.37.40:30594 max_fails=3 fail_timeout=30s; server 172.20.37.40:30694 max_fails=3 fail_timeout=30s; server 172.20.37.41:30194 max_fails=3 fail_timeout=30s; server 172.20.37.41:30294 max_fails=3 fail_timeout=30s; server 172.20.37.41:30394 max_fails=3 fail_timeout=30s; server 172.20.37.41:30494 max_fails=3 fail_timeout=30s; server 172.20.37.41:30594 max_fails=3 fail_timeout=30s; server 172.20.37.41:30694 max_fails=3 fail_timeout=30s; server 172.20.37.42:30194 max_fails=3 fail_timeout=30s; server 172.20.37.42:30294 max_fails=3 fail_timeout=30s; server 172.20.37.42:30394 max_fails=3 fail_timeout=30s; server 172.20.37.42:30494 max_fails=3 fail_timeout=30s; server 172.20.37.42:30594 max_fails=3 fail_timeout=30s; server 172.20.37.42:30694 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } éƒ¨ç½² Kafka UI 1 2 3 4 5 6 7 8 9 10 11 yamlApplicationConfig: kafka: clusters: - name: platform bootstrapServers: kafka-broker-headless:9092 auth: type: disabled management: health: ldap: enabled: false 1 helm install -name kafka-ui kafka-ui/kafka-ui -f values.yaml --namespace kafka ","date":"2024-06-01T11:35:00+08:00","permalink":"http://localhost:1313/p/k8s-bitnami-kafka-kraft-helm-chart/","title":"[Kubernetes] ä½¿ç”¨ bitnami helm chart å®‰è£ kafka kraft é›†ç¾¤"},{"content":"å»ºç«‹ PostgreSQL HA å¢é›† ç›´æ¥é€é bitnami çš„ postgresql-ha helm chart å®‰è£\n1 helm repo add bitnami https://charts.bitnami.com/bitnami å¯ä»¥é€éæŒ‡å®š values.yaml å®‰è£\n1 helm install pgsql -f values.yaml bitnami/postgresql-ha -n platform æˆ–æ˜¯ç›´æ¥åœ¨å®‰è£æ™‚æŒ‡å®šè¦æ›¿æ›çš„åƒæ•¸ï¼Œå› ç‚ºè¦ä¿®æ”¹çš„åƒæ•¸åªæœ‰ storage classï¼Œæ‰€ä»¥ç›´æ¥ç”¨æ­¤å‘½ä»¤å®‰è£\n1 helm install pgsql bitnami/postgresql-ha -n platform --set global.storageClass=rook-ceph-block å®‰è£å®Œæˆå¾Œæœƒè·³æç¤ºï¼Œå¯ä»¥å–å¾— DB çš„ç™»å…¥å¯†ç¢¼ï¼Œä»¥åŠèªªæ˜å¦‚ä½•é€£ç·šã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 To get the password for \u0026#34;postgres\u0026#34; run: export POSTGRES_PASSWORD=$(kubectl get secret --namespace platform pgsql-postgresql-ha-postgresql -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) To get the password for \u0026#34;repmgr\u0026#34; run: export REPMGR_PASSWORD=$(kubectl get secret --namespace platform pgsql-postgresql-ha-postgresql -o jsonpath=\u0026#34;{.data.repmgr-password}\u0026#34; | base64 -d) To connect to your database run the following command: kubectl run pgsql-postgresql-ha-client --rm --tty -i --restart=\u0026#39;Never\u0026#39; --namespace platform --image docker.io/bitnami/postgresql-repmgr:16.0.0-debian-11-r15 --env=\u0026#34;PGPASSWORD=$POSTGRES_PASSWORD\u0026#34; \\ --command -- psql -h pgsql-postgresql-ha-pgpool -p 5432 -U postgres -d postgres To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace platform svc/pgsql-postgresql-ha-pgpool 5432:5432 \u0026amp; psql -h 127.0.0.1 -p 5432 -U postgres -d postgres update values.yaml å¦‚æœå®‰è£å¾Œæœ‰éœ€è¦ä¿®æ”¹è¨­å®šï¼Œå¯ä»¥å†æ”¹å®Œ values.yaml å¾Œç”¨ upgrade æŒ‡ä»¤é€²è¡Œæ›´æ–°ã€‚\n1 helm upgrade -f values.yaml pgsql bitnami/postgresql-ha -n platform å»ºç«‹ PGADMIN4 UI postgresql-ha helm chart ä¸åŒ…å« PGADIMN UIï¼Œå¦å¤–ä½¿ç”¨ yaml éƒ¨ç½²\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 apiVersion: apps/v1 kind: Deployment metadata: name: pgadmin namespace: platform spec: replicas: 1 selector: matchLabels: app: pgadmin template: metadata: labels: app: pgadmin spec: containers: - env: - name: PGADMIN_DEFAULT_EMAIL value: asus.sdsp@gmail.com - name: PGADMIN_DEFAULT_PASSWORD value: \u0026#34;!QAZxsw2\u0026#34; - name: PGADMIN_LISTEN_PORT value: \u0026#34;8001\u0026#34; - name: PGADMIN_CONFIG_WTF_CSRF_CHECK_DEFAULT value: \u0026#34;False\u0026#34; - name: PGADMIN_CONFIG_WTF_CSRF_ENABLED value: \u0026#34;False\u0026#34; - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION value: \u0026#34;False\u0026#34; image: dpage/pgadmin4:7.8 imagePullPolicy: IfNotPresent name: pgadmin ports: - name: tcp8001 containerPort: 8001 --- kind: Service apiVersion: v1 metadata: namespace: platform name: pgadmin spec: type: ClusterIP ports: - name: http protocol: TCP port: 8001 targetPort: tcp8001 selector: app: pgadmin --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: pgadmin namespace: platform annotations: kubernetes.io/ingress.class: nginx #cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;3600\u0026#34; nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;3600\u0026#34; spec: tls: - hosts: - pgadmin.sdsp-stg.com secretName: domain-cert-sdsp-stg.com-prod rules: - host: pgadmin.sdsp-stg.com http: paths: - path: / pathType: Prefix backend: service: name: pgadmin port: number: 8001 Error: The CSRF tokens do not match å¦‚æœ PGADMIN å•Ÿç”¨æ™‚é‡åˆ°ä»¥ä¸‹éŒ¯èª¤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2023-11-15 03:03:39,842: ERROR pgadmin: 400 Bad Request: The CSRF tokens do not match. Traceback (most recent call last): File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 261, in protect validate_csrf(self._get_csrf_token()) File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 115, in validate_csrf raise ValidationError(\u0026#34;The CSRF tokens do not match.\u0026#34;) wtforms.validators.ValidationError: The CSRF tokens do not match. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \u0026#34;/venv/lib/python3.11/site-packages/flask/app.py\u0026#34;, line 1821, in full_dispatch_request rv = self.preprocess_request() ^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;/venv/lib/python3.11/site-packages/flask/app.py\u0026#34;, line 2313, in preprocess_request rv = self.ensure_sync(before_func)() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 229, in csrf_protect self.protect() File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 264, in protect self._error_response(e.args[0]) File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 307, in _error_response raise CSRFError(reason) å¯ä»¥åŠ ä¸Šä¸‰å€‹ç’°å¢ƒè®Šæ•¸è§£æ±ºï¼š\nPGADMIN_CONFIG_WTF_CSRF_CHECK_DEFAULT=\u0026ldquo;False\u0026rdquo; PGADMIN_CONFIG_WTF_CSRF_ENABLED=\u0026ldquo;False\u0026rdquo; PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION=\u0026ldquo;False\u0026rdquo; ","date":"2024-05-25T10:57:04+08:00","permalink":"http://localhost:1313/p/k8s-install-ha-postgresql/","title":"[Kubernetes] å®‰è£é«˜å¯ç”¨ PostgreSQL"},{"content":"å®‰è£ kubectl å¯ç›´æ¥åƒè€ƒå®˜ç¶²ï¼Œä¾ç…§ç’°å¢ƒé¸æ“‡æƒ³è¦çš„å®‰è£æ–¹æ³•å®‰è£ã€‚\næº–å‚™ config æª”æ¡ˆ å°‡å…©åº§ k8s configï¼Œå­˜åœ¨ä»»æ„ç›®éŒ„ä¸‹\n1 2 3 $ mkdir k8s-config $ vi dev $ vi stg å°‡å¤šå€‹ config åˆä½µ 1 2 $ export KUBECONFIG=~/k8s-config/dev:~/k8s-config/stg $ kubectl config view --flatten å°‡æŒ‡ä»¤è¿”å›çš„ config è¤‡è£½åˆ° kubernetes çš„é è¨­ config ç›®éŒ„ .kube ä¸‹\n1 $ vi ~/.kube/config ä¸¦é‡æ–°ä¿®æ”¹ç’°å¢ƒè®Šæ•¸\n1 $ export KUBECONFIG=~/.kube/config æˆåŠŸä½¿ç”¨ å°±èƒ½ç›´æ¥é€é kubectl config æŒ‡ä»¤ç®¡ç† context äº†\n1 $ kubectl config get-contexts åˆ‡æ›å­˜å– Kubernets å¢é›†\n1 $ kubectl config use-context kubernetes-admin@kubernetes-dev æç¤º\nå…¶å¯¦ config ä¸¦ä¸ä¸€å®šè¦åˆåœ¨ä¸€èµ·ï¼Œåˆ†é–‹ç„¶å¾Œä½¿ç”¨ç’°å¢ƒè®Šæ•¸æŒ‡å®šå¤šå€‹ config ä¹Ÿå¯ä»¥ã€‚çœ‹è‡ªå·±çš„ç¿’æ…£~\n","date":"2024-05-21T10:45:00+08:00","permalink":"http://localhost:1313/p/k8s-kubectl-access-multiple-cluster/","title":"[Kubernetes] kubectl å­˜å–å¤šå¢é›†"},{"content":"TL; DR ç´€éŒ„ä½¿ç”¨ Helm Chart å®‰è£ Ingress Nginx Controller ä»¥åŠ Cert Manager ä¸¦è¨­ç½® TLS æ†‘è­‰çš„éç¨‹ã€‚\nIngress Nginx Controller æº–å‚™ helm chart 1 2 3 4 5 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm pull ingress-nginx/ingress-nginx tar -zxvf ingress-nginx-xxx.tgz cd ingress-nginx ç·¨è¼¯ values.yaml è¨­å®š kind é¡å‹æ›´æ”¹ç‚ºï¼šDaemonSet 1 2 3 # -- Use a `DaemonSet` or `Deployment` kind: DaemonSet # -- Annotations to be added to the controller Deployment or DaemonSet service template çš„ type æ”¹ç‚º NodePort 1 2 3 4 5 6 7 8 9 10 11 12 13 service: # -- Enable controller services or not. This does not influence the creation of either the admission webhook or the metrics service. enabled: true external: # -- Enable the external controller service or not. Useful for internal-only deployments. enabled: true # -- Annotations to be added to the external controller service. See `controller.service.internal.annotations` for annotations to be added to the internal controller service. annotations: {} # -- Labels to be added to both controller services. labels: {} # -- Type of the external controller service. # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types type: NodePort å®‰è£ 1 helm install ingress-nginx --namespace ingress-nginx --create-namespace Cert Manager æº–å‚™ helm chart åŠå®‰è£ 1 2 3 4 5 6 helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true å–å¾— Cloudflare API Token å°‡ API Token çš„ Permission è¨­ç‚º Zone.Zone, Zone.DNS ï¼›Resources è¨­ç‚º All zonesã€‚\nå»ºç«‹ Cluster Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: Secret metadata: name: cloudflare-api-token namespace: cert-manager type: Opaque stringData: api-token: uNlI5Slnd-NqqyUX9iNoVlJ2jh57kFAzT5DxKE2E --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prd namespace: cert-manager spec: acme: email: asus.sdsp@gmail.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-prd solvers: - selector: dnsZones: - \u0026#34;sdsp-dev.com\u0026#34; - \u0026#34;*.sdsp-dev.com\u0026#34; dns01: cloudflare: email: asus.sdsp@gmail.com apiTokenSecretRef: name: cloudflare-api-token key: api-token å»ºç«‹ wildcard æ†‘è­‰ 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: wildcard-sdsp-dev-prd namespace: cert-manager spec: dnsNames: - \u0026#34;sdsp-dev.com\u0026#34; - \u0026#34;*.sdsp-dev.com\u0026#34; issuerRef: kind: ClusterIssuer name: letsencrypt-prd secretName: wildcard-sdsp-dev-prd æ›´æ”¹ ingress-nginx-controller çš„ default ssl cert 1 kubectl edit daemonset.apps/ingress-nginx-controller -n ingress-nginx åœ¨ containers ä¸‹çš„ args åŠ ä¸Šä¸Šé¢ cert-manager ä¸­å‰µå»ºçš„ ca\n1 - --default-ssl-certificate=cert-manager/wildcard-sdsp-dev-prd æ¸¬è©¦ å»ºç«‹æœå‹™çš„ ingress è³‡æº å‡è¨­ç’°å¢ƒä¸‹å·²æœ‰ä¸€å€‹ deployment è³‡æº vitepress\nç‚º vitepress service å»ºç«‹ ingress è³‡æºï¼Œä»¥ä¾›å°å¤–å­˜å–ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: labels: app.kubernetes.io/name: vitepress name: vitepress-sdsp-dev namespace: dev annotations: nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; spec: ingressClassName: nginx tls: - hosts: - doc.sdsp-dev.com rules: - host: doc.sdsp-dev.com http: paths: - backend: service: name: vitepress port: number: 80 path: /.* pathType: Prefix ä¸¦åœ¨ Cloudflare è¨­å®š DNS A Record å³å¯ï¼Œå› ç‚ºåœ¨ Kubernetes é›†ç¾¤å‰é¢æ”¾äº†ä¸€å€‹ Nginx åšè² è¼‰å‡è¡¡ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥å°‡ Cloudflare DNS Record çš„ IP Address è¨­ç‚º 172.20.37.33ã€‚\nè£œå…… Kubernetes + Nginx HA æ¶æ§‹åœ– æç¤º\nå…¶ä»–å¸¸è¦‹çš„ HA æ¶æ§‹ç‚ºä½¿ç”¨ HA Proxy + keepalive\n","date":"2024-05-19T22:20:00+08:00","permalink":"http://localhost:1313/p/k8s-ingress-nginx-cert-manager-setup/","title":"[Kubernetes] Ingress Nginx and Cert Manager Setup"},{"content":"å•é¡Œ rook-ceph é›†ç¾¤é¡¯ç¤º HEALTH WARNï¼Œå…¶åŸå› ç‚º clock skew detected on mon.c, mon.dã€‚\n1 kubectl -n rook-ceph describe cephcluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Status: Ceph: Capacity: Bytes Available: 1174133465088 Bytes Total: 1181116006400 Bytes Used: 6982541312 Last Updated: 2024-04-16T01:21:15Z Details: MON_CLOCK_SKEW: Message: clock skew detected on mon.c, mon.d Severity: HEALTH_WARN Fsid: caec8fab-28a0-464d-9079-463ddbb7c4e3 Health: HEALTH_WARN Last Changed: 2024-04-15T11:31:39Z Last Checked: 2024-04-16T01:21:15Z è§£æ±º åŸæœ¬ä½¿ç”¨ ntpdate å¼·åˆ¶æ¯å°éƒ½èˆ‡æŒ‡å®šçš„ ntp server æ›´æ–°ï¼Œä½†é‚„æœ‰å¦ä¸€å€‹æ–¹æ³•æ˜¯ä¿®æ”¹ mon clock drift allowed é è¨­åƒæ•¸\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 kubectl edit configmap rook-config-override -n rook-ceph -o yaml apiVersion: v1 data: config: | [global] mon clock drift allowed = 1 kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: rook-ceph-cluster meta.helm.sh/release-namespace: rook-ceph creationTimestamp: \u0026#34;2024-04-12T05:38:23Z\u0026#34; labels: app.kubernetes.io/managed-by: Helm name: rook-config-override namespace: rook-ceph ownerReferences: - apiVersion: ceph.rook.io/v1 blockOwnerDeletion: true controller: true kind: CephCluster name: rook-ceph uid: a23004c1-fe54-4905-b9bf-af21a7b2a8ea resourceVersion: \u0026#34;994524\u0026#34; uid: 57ad721c-390a-4999-aa00-9f64e2497310 root@node1:~# kubectl get configmap rook-config-override -n rook-ceph -o yaml apiVersion: v1 data: config: | [global] mon clock drift allowed = 1 kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: rook-ceph-cluster meta.helm.sh/release-namespace: rook-ceph creationTimestamp: \u0026#34;2024-04-12T05:38:23Z\u0026#34; labels: app.kubernetes.io/managed-by: Helm name: rook-config-override namespace: rook-ceph ownerReferences: - apiVersion: ceph.rook.io/v1 blockOwnerDeletion: true controller: true kind: CephCluster name: rook-ceph uid: a23004c1-fe54-4905-b9bf-af21a7b2a8ea resourceVersion: \u0026#34;994524\u0026#34; uid: 57ad721c-390a-4999-aa00-9f64e2497310 æ–°å¢äº†ä»¥ä¸‹å…§å®¹ï¼š\n1 2 3 config: | [global] mon clock drift allowed = 1 æ­¤è™•çš„æ™‚é–“å¯éš¨è‡ªèº«çš„æ™‚é–“å·®è¨­ç½®ï¼Œåœ¨0.5åˆ°1sä¹‹é–“ï¼Œä¸å»ºè­°è¨­å®šéå¤§çš„å€¼\næ¥è‘—åˆªé™¤ mon Pod ä½¿å…¶è¼‰å…¥æ–°çš„è¨­å®šæ–‡ä»¶\n1 2 3 4 kubectl -n rook-ceph delete pod $(kubectl -n rook-ceph get pods -o custom-columns=NAME:.metadata.name --no-headers| grep mon) pod \u0026#34;rook-ceph-mon-a-559d8b5866-dcmcz\u0026#34; deleted pod \u0026#34;rook-ceph-mon-c-bfdbb5598-96kv8\u0026#34; deleted pod \u0026#34;rook-ceph-mon-d-c9ff49c58-ml65k\u0026#34; deleted æŸ¥çœ‹ç‹€æ…‹å·²æ¢å¾©å¥åº·å€¼\n1 2 3 kubectl -n rook-ceph get cephcluster NAME DATADIRHOSTPATH MONCOUNT AGE PHASE MESSAGE HEALTH EXTERNAL FSID rook-ceph /var/lib/rook 3 3d19h Ready Cluster created successfully HEALTH_OK caec8fab-28a0-464d-9079-463ddbb7c4e3 Reference https://blog.csdn.net/m0_59615922/article/details/131459393 ","date":"2024-05-01T22:20:00+08:00","permalink":"http://localhost:1313/p/k8s-rook-ceph-clock-skew-detected/","title":"[Kubernetes] Rook Health Warn with Clock Skew"},{"content":"TL; DR åŸæœ¬ä½¿ç”¨ Kubernetes çš„ä¸‰å€‹ worker node ç¯€é»å»ºç«‹äº†æ“æœ‰ä¸‰å€‹ node çš„ rookï¼Œä½†å› ç‚ºè³‡æºä¸è¶³çš„ç·£æ•…ï¼Œå°‡å…¶ä¸­ä¸€å€‹ç¯€é»ç”±åŸæœ¬çš„ VM æ”¹æˆä»¥å¯¦æ©Ÿçš„æ–¹å¼åŠ å…¥é›†ç¾¤ï¼Œå°è‡´ç¡¬ç¢ŸåŸæœ¬æ˜¯ä»¥ /dev/sdb çš„æ–¹å¼åŠ å…¥ OSDï¼Œä½†ç¾åœ¨éœ€è¦æ”¹æˆ /dev/sdaã€‚\nå…ˆå°‡ rook-ceph-operator åœç”¨ 1 kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0 ä¿®æ”¹ rook cluster é…ç½®ï¼ŒæŒ‡å®šç¯€é»ä½¿ç”¨ sda ç¡¬ç¢Ÿã€‚ 1 2 3 4 5 6 7 8 9 10 # kubectl edit cephclusters.ceph.rook.io -n rook-ceph rook-ceph storage: flappingRestartIntervalHours: 0 nodes: - devices: - name: sda name: node5 store: {} useAllDevices: true useAllNodes: true éƒ¨ç½² ceph toolbox 1 kubectl apply -f toolbox.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 # toolbox.yaml apiVersion: apps/v1 kind: Deployment metadata: name: rook-ceph-tools namespace: rook-ceph # namespace:cluster labels: app: rook-ceph-tools spec: replicas: 1 selector: matchLabels: app: rook-ceph-tools template: metadata: labels: app: rook-ceph-tools spec: dnsPolicy: ClusterFirstWithHostNet serviceAccountName: rook-ceph-default containers: - name: rook-ceph-tools image: quay.io/ceph/ceph:v18.2.2 command: - /bin/bash - -c - | # Replicate the script from toolbox.sh inline so the ceph image # can be run directly, instead of requiring the rook toolbox CEPH_CONFIG=\u0026#34;/etc/ceph/ceph.conf\u0026#34; MON_CONFIG=\u0026#34;/etc/rook/mon-endpoints\u0026#34; KEYRING_FILE=\u0026#34;/etc/ceph/keyring\u0026#34; # create a ceph config file in its default location so ceph/rados tools can be used # without specifying any arguments write_endpoints() { endpoints=$(cat ${MON_CONFIG}) # filter out the mon names # external cluster can have numbers or hyphens in mon names, handling them in regex # shellcheck disable=SC2001 mon_endpoints=$(echo \u0026#34;${endpoints}\u0026#34;| sed \u0026#39;s/[a-z0-9_-]\\+=//g\u0026#39;) DATE=$(date) echo \u0026#34;$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}\u0026#34; cat \u0026lt;\u0026lt;EOF \u0026gt; ${CEPH_CONFIG} [global] mon_host = ${mon_endpoints} [client.admin] keyring = ${KEYRING_FILE} EOF } # watch the endpoints config file and update if the mon endpoints ever change watch_endpoints() { # get the timestamp for the target of the soft link real_path=$(realpath ${MON_CONFIG}) initial_time=$(stat -c %Z \u0026#34;${real_path}\u0026#34;) while true; do real_path=$(realpath ${MON_CONFIG}) latest_time=$(stat -c %Z \u0026#34;${real_path}\u0026#34;) if [[ \u0026#34;${latest_time}\u0026#34; != \u0026#34;${initial_time}\u0026#34; ]]; then write_endpoints initial_time=${latest_time} fi sleep 10 done } # read the secret from an env var (for backward compatibility), or from the secret file ceph_secret=${ROOK_CEPH_SECRET} if [[ \u0026#34;$ceph_secret\u0026#34; == \u0026#34;\u0026#34; ]]; then ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring) fi # create the keyring file cat \u0026lt;\u0026lt;EOF \u0026gt; ${KEYRING_FILE} [${ROOK_CEPH_USERNAME}] key = ${ceph_secret} EOF # write the initial config file write_endpoints # continuously update the mon endpoints if they fail over watch_endpoints imagePullPolicy: IfNotPresent tty: true securityContext: runAsNonRoot: true runAsUser: 2016 runAsGroup: 2016 capabilities: drop: [\u0026#34;ALL\u0026#34;] env: - name: ROOK_CEPH_USERNAME valueFrom: secretKeyRef: name: rook-ceph-mon key: ceph-username volumeMounts: - mountPath: /etc/ceph name: ceph-config - name: mon-endpoint-volume mountPath: /etc/rook - name: ceph-admin-secret mountPath: /var/lib/rook-ceph-mon readOnly: true volumes: - name: ceph-admin-secret secret: secretName: rook-ceph-mon optional: false items: - key: ceph-secret path: secret.keyring - name: mon-endpoint-volume configMap: name: rook-ceph-mon-endpoints items: - key: data path: mon-endpoints - name: ceph-config emptyDir: {} tolerations: - key: \u0026#34;node.kubernetes.io/unreachable\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 5 é€²å…¥ toolbox åŸ·è¡Œç§»é™¤ sdb osd æ“ä½œ 1 kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash æ‰‹å‹•ç§»é™¤å°æ‡‰osd\n1 2 3 4 5 6 7 8 9 ceph osd set noup ceph osd down 0 ceph osd out 0 # æŸ¥çœ‹æ•°æ®å‡è¡¡è¿›åº¦ï¼Œ ç­‰å¾…æ•°æ®å‡è¡¡å®Œæˆ ceph -w # å‡è¡¡è³‡æ–™å®Œæˆå¾Œç§»é™¤å°æ‡‰çš„osd ceph osd purge 0 --yes-i-really-mean-it ceph auth del osd.0 ceph osd crush remove node5 æª¢æŸ¥cephç‹€æ…‹ä»¥åŠosdç‹€æ…‹\n1 2 ceph -s ceph osd tree ç§»é™¤ podï¼Œä¸¦åˆ¤æ–·åˆªé™¤å°æ‡‰çš„ job 1 kubectl delete deploy -n rook-ceph rook-ceph-osd-0 é€²å…¥ node5 ç¯€é»ï¼Œä¸¦æ¸…é™¤ç£ç›¤è³‡æ–™ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/env bash DISK=\u0026#34;/dev/sdb\u0026#34; # Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean) # You will have to run this step for all disks. sgdisk --zap-all $DISK # These steps only have to be run once on each node # If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks. ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % # ceph-volume setup can leave ceph-\u0026lt;UUID\u0026gt; directories in /dev (unnecessary clutter) rm -rf /dev/ceph-* lsblk -f rm -rf /var/lib/rook/* é‡æ–°å°‡ rook-ceph-operator å•Ÿç”¨ï¼Œnode5 sda osd ä¾¿æœƒè‡ªå‹•æ–°å¢ 1 kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0 Reference https://blog.csdn.net/fancy106/article/details/121706998 ","date":"2024-04-30T14:45:00+08:00","permalink":"http://localhost:1313/p/k8s-rook-delete-osd-and-add-on-same-node/","title":"[Kubernetes] Rook åˆªé™¤åŸå…ˆ /dev/sdb çš„ OSD ä¸¦é‡æ–°æ–°å¢åŒå€‹ç¯€é»çš„ /dev/sda çš„ OSD"},{"content":"TL; DR ä½¿ç”¨ Rook Helm Chart åœ¨ Kubernetes ä¸Šå®‰è£ Rook Operator ä»¥åŠ Ceph é›†ç¾¤ï¼Œä¸¦é€é CSI ä½¿ç”¨ PVCã€‚\nPrerequisites Kubernetesï¼šæ”¯æŒ v1.25 è‡³ v1.29 ç‰ˆæœ¬ Ceph Node Resource Requiementï¼šä¸åŒçµ„ä»¶éœ€è¦ä¸åŒçš„ç¡¬é«”è³‡æºå¤§å°ï¼Œè«‹åƒè€ƒ SUSE æ–‡æª” äº†è§£å…·é«”è¦æ±‚ã€‚ æç¤º\næ¸¬è©¦ç™¼ç¾ï¼Œä¸‰å€‹å·¥ä½œç¯€é»é…ç½®ç‚º 4 core vCPU / 8GB RAM æœƒå°è‡´å®‰è£å¤±æ•—ï¼Œå‡ç´šè‡³ 8 core vCPU / 16GB RAM å¾Œæ‰èƒ½é †åˆ©é‹è¡Œã€‚\nè‡³å°‘éœ€è¦ä»¥ä¸‹ä¸€ç¨®æœ¬åœ°å„²å­˜é¡å‹ï¼š\nåŸå§‹ç¡¬ç¢Ÿï¼ˆç„¡åˆ†å€æˆ–æ ¼å¼åŒ–æ–‡ä»¶ç³»çµ±ï¼‰ åŸå§‹ç¡¬ç¢Ÿåˆ†å€ï¼ˆç„¡æ ¼å¼åŒ–æ–‡ä»¶ç³»çµ±ï¼‰ LVM é‚è¼¯å·ï¼ˆç„¡æ ¼å¼åŒ–æ–‡ä»¶ç³»çµ±ï¼‰ åœ¨å·²å­˜åœ¨çš„ Storage Class ä¸­ï¼Œä»¥ block level å‹æ…‹æä¾›çš„ Persistent Volumes RBDï¼š\nCeph éœ€è¦ä½¿ç”¨å…·æœ‰ RBD æ¨¡çµ„çš„ Linux kernelã€‚è¨±å¤š Linux ç™¼è¡Œç‰ˆéƒ½å·²ç¶“åŒ…å«äº† RBD æ¨¡çµ„ï¼Œä½†ä¸æ˜¯æ‰€æœ‰çš„ç™¼è¡Œç‰ˆéƒ½æœ‰ï¼Œä½¿ç”¨ä¸‹é¢ command ç¢ºèªåŠè¼‰å…¥\n1 2 3 4 5 6 lsmod | grep rbd # è‹¥ç„¡è¿”å›ï¼Œå‰‡åŸ·è¡Œä¸‹é¢è¼‰å…¥ sudo modprobe rbd sudo vim /etc/modules-load.d/rbd.conf\t# æ–‡ä»¶åä»»æ„ï¼Œä»¥.conf çµå°¾å³å¯ rbd\t#å†…å®¹å¯« rbd å³å¯ Installation Step æ•´é«”æµç¨‹å¦‚ä¸‹\nå®‰è£ Rook Operator éƒ¨ç½² Ceph å¢é›† é©—è­‰ Ceph é›†ç¾¤ ä½¿ç”¨ Ceph å­˜å„² Install Rook Operator 1 2 helm repo add rook-release https://charts.rook.io/release helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph è³‡è¨Š\nå¦‚æœå¢é›†æ˜¯ä¸€å€‹ä¸‰ç¯€é»çš„å¢é›†ï¼ˆ1 å€‹ Master + 2 å€‹ Workerï¼‰ï¼Œé‚£éº¼ Master ç¯€é»ä¹Ÿéœ€è¦ä½œç‚ºå·¥ä½œè² è¼‰ç¯€é»ä½¿ç”¨ï¼Œå¯ä»¥å»æ‰æ¬²å®‰è£çš„ master ç¯€é»çš„æ±¡é»ï¼š\n1 kubectl taint node node1 node-role.kubernetes.io/master:PreferNoSchedule- Install Rook Cluster æº–å‚™ values.yaml\n1 2 3 4 5 operatorNamespace: rook-ceph cephClusterSpec: dashboard: enabled: true ssl: false å®‰è£ cluster\n1 helm install --namespace rook-ceph rook-ceph-cluster rook-release/rook-ceph-cluster -f values.yaml Verify the Ceph Cluster Installation 1 kubectl -n rook-ceph get cephcluster ç•¶å¥åº·ç‹€æ…‹è¿”å› HEALTH_OK\n1 kubectl -n rook-ceph get all ä¸”æ‰€æœ‰éƒ¨å±¬å…ƒä»¶çš†æ­£å¸¸ Running å¾Œ\nä¾¿å¯ç›´æ¥é€²å…¥ä¸‹ä¸€æ­¥çš„ä½ˆç½² pvc è³‡æºæ¸¬è©¦\nDeploy a Validation PVC and Pod æŸ¥çœ‹ StorageClass\n1 kubectl get sc ä½¿ç”¨ block level storage å®£å‘Š PVC\n1 2 3 4 5 6 7 8 9 10 11 12 13 # pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc namespace: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: ceph-block 1 kubectl apply -f pvc.yaml ç•¶ STATUS è¿”å› Bound å°±æˆåŠŸäº†ã€‚\nCompletely Clean Rook ç•¶å®‰è£å¤±æ•—è¦é‡æ–°å®‰è£æ™‚ï¼Œéœ€è¦åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼Œæ‰èƒ½é‡æ–°å®‰è£ã€‚\nUninstall all rook resource and delete the namespace\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 helm uninstall rook-ceph-cluster -n rook-ceph # æ¸¬è©¦å¾Œç™¼ç¾åˆªé™¤ chart ä¹‹å¾Œä¸æœƒåˆªé™¤è‡ªå‹•åˆªé™¤ç›¸æ‡‰è³‡æºï¼Œéœ€é¡å¤–åˆªé™¤ kubectl -n rook-ceph delete deployment.apps/rook-ceph-crashcollector-node4 kubectl -n rook-ceph delete deployment.apps/rook-ceph-crashcollector-node5 kubectl -n rook-ceph delete deployment.apps/rook-ceph-crashcollector-node6 kubectl -n rook-ceph delete deployment.apps/rook-ceph-mgr-a kubectl -n rook-ceph delete deployment.apps/rook-ceph-mgr-b kubectl -n rook-ceph delete deployment.apps/rook-ceph-mon-b kubectl -n rook-ceph delete deployment.apps/rook-ceph-mon-a kubectl -n rook-ceph delete deployment.apps/rook-ceph-mon-c kubectl -n rook-ceph delete deployment.apps/rook-ceph-mds-ceph-filesystem-a kubectl -n rook-ceph delete deployment.apps/rook-ceph-mds-ceph-filesystem-b kubectl -n rook-ceph delete job.batch/rook-ceph-csi-detect-version kubectl -n rook-ceph delete job.batch/rook-ceph-osd-prepare-node4 kubectl -n rook-ceph delete job.batch/rook-ceph-osd-prepare-node5 kubectl -n rook-ceph delete job.batch/rook-ceph-osd-prepare-node6 kubectl -n rook-ceph delete service/rook-ceph-exporter kubectl -n rook-ceph delete service/rook-ceph-mgr kubectl -n rook-ceph delete service/rook-ceph-mgr-dashboard kubectl -n rook-ceph delete service/rook-ceph-mon-a kubectl -n rook-ceph delete service/rook-ceph-mon-b kubectl -n rook-ceph delete service/rook-ceph-mon-c kubectl -n rook-ceph delete service/rook-ceph-rgw-ceph-objectstore # é™¤äº† k8s é è¨­çš„è³‡æºå¤–ï¼Œé‚„æœ‰ä»¥ä¸‹çš„ CRD è³‡æºé ˆåˆªé™¤ kubectl -n rook-ceph delete cephobjectstore ceph-objectstore \u0026amp; kubectl -n rook-ceph delete cephfilesystem ceph-filesystem \u0026amp; kubectl -n rook-ceph delete cephblockpool ceph-blockpool \u0026amp; kubectl -n rook-ceph delete cephcluster rook-ceph \u0026amp; kubectl -n rook-ceph get cephobjectstores.ceph.rook.io ceph-objectstore -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - kubectl -n rook-ceph get cephblockpools.ceph.rook.io ceph-blockpool -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - kubectl -n rook-ceph get cephfilesystems.ceph.rook.io ceph-filesystem -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - kubectl -n rook-ceph get cephcluster.ceph.rook.io rook-ceph -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - # ç§»é™¤ configmap kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;: []}}\u0026#39; kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;: []}}\u0026#39; kubectl delete configmap -n rook-ceph rook-ceph-csi-config kubectl delete configmap -n rook-ceph rook-ceph-csi-mapping-config kubectl delete configmap -n rook-ceph rook-ceph-operator-config kubectl delete configmap -n rook-ceph rook-ceph-pdbstatemap # ç§»é™¤ Operator helm uninstall rook-ceph -n rook-ceph # ç§»é™¤ CRD for CRD in $(kubectl get crd -n rook-ceph | awk \u0026#39;/ceph.rook.io/ {print $1}\u0026#39;); do kubectl get -n rook-ceph \u0026#34;$CRD\u0026#34; -o name | xargs -I {} kubectl patch -n rook-ceph {} --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;: []}}\u0026#39;; done kubectl get crd | grep rook.io | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete crd # æœ€çµ‚åˆªé™¤ namespace kubectl delete ns rook-ceph Clear Rook data from Ceph node and zap the storage devices\nå‡è¨­å­˜å„²è¨­å‚™æ˜¯ /dev/sdb ç£ç›¤ï¼Œéœ€è¦åˆªé™¤è©²è¨­å‚™æ‰€æœ‰æ•¸æ“šã€‚\n1 2 3 4 5 6 7 8 9 10 # remove data sudo rm -rvf /var/lib/rook # zapping devices DISK=\u0026#34;/dev/sdb\u0026#34; sudo sgdisk --zap-all $DISK sudo dd if=/dev/zero of=\u0026#34;$DISK\u0026#34; bs=1M count=100 oflag=direct,dsync # SSDs may be better cleaned with blkdiscard instead of dd sudo blkdiscard $DISK sudo partprobe $DISK ","date":"2024-03-30T10:45:00+08:00","permalink":"http://localhost:1313/p/k8s-rook-installation/","title":"[Kubernetes] Rook 1.13 (Ceph on Kubernetes) å®‰è£ç´€éŒ„"},{"content":"TL; DR ç´€éŒ„ä½¿ç”¨ Kubespray é€é Ansible å¿«é€Ÿéƒ¨ç½² Kubernetes é›†ç¾¤ã€‚\nç’°å¢ƒ Ansible æ§åˆ¶ç¯€é» Ubuntu 22.04 LTS 2 CPU 4GB RAM 20GB Disk 3 å€‹ master ç¯€é» Ubuntu 22.04 LTS 2 CPU 4GB RAM 20GB Disk 3 å€‹ worker ç¯€é» Ubuntu 22.04 LTS 4 CPU 8GB RAM 80GB Disk Kubespray æœ€ä½è¦æ±‚ Ansible ç¯€é»ï¼š1024 MBã€1 å€‹ CPU èˆ‡ 20 GB ç£ç¢Ÿç©ºé–“ Masterï¼š1500 MB RAMã€2 å€‹ CPU å’Œ 20 GB å¯ç”¨ç£ç¢Ÿç©ºé–“ Workerï¼š1024 MBã€2 å€‹ CPUã€20 GB å¯ç”¨ç£ç¢Ÿç©ºé–“ æ¯å€‹ç¯€é»ä¸Šçš„äº’è¯ç¶²é€£æ¥ æ“æœ‰ sudo ç®¡ç†å“¡æ¬Šé™ é…ç½® Ansible æ§åˆ¶ç¯€é» å®‰è£æ‰€éœ€å¥—ä»¶ 1 2 3 4 5 sudo apt update sudo apt install git python3-pip -y git clone https://github.com/kubernetes-incubator/kubespray.git cd kubespray sudo pip install -r requirements.txt è¤‡è£½åŸ·è¡Œ ssh é‡‘é‘° 1 2 3 4 5 6 7 ssh-keygen # å¦‚æœåœ¨ ~/.ssh/ ä¸‹æ²’æœ‰é‡‘é‘°å‰µå»ºçš„è©± ssh-copy-id ula@192.168.0.48 ssh-copy-id ula@192.168.0.152 ssh-copy-id ula@192.168.0.225 ssh-copy-id ula@192.168.0.233 ssh-copy-id ula@192.168.0.131 ssh-copy-id ula@192.168.0.241 æº–å‚™ Ansible Host æ¸…å–® 1 2 cp -rfp inventory/sample inventory/mycluster declare -a IPS=(192.168.0.48 192.168.0.152 192.168.0.225 192.168.0.233 192.168.0.131 192.168.0.241)CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]} ä¿®æ”¹æ¸…å–®ï¼Œè¨­ç½® master node åŠ worker node å„ 3 å€‹\n1 vi inventory/mycluster/hosts.yaml ä¿®æ”¹ K8s éƒ¨ç½²è®Šæ•¸ 1 vi inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml ä¸»è¦ä¿®æ”¹ä»¥ä¸‹é¸é …ï¼š\n1 2 3 4 kube_version: v1.29.2 kube_network_plugin: calico # default kube_pods_subnet: 10.233.64.0/18 # default kube_service_addresses: 10.233.0.0/18 # default ä¿®æ”¹å®‰è£å¥—ä»¶ 1 vi inventory/mycluster/group_vars/k8s_cluster/addons.yml ä¸»è¦é–‹å•Ÿä»¥ä¸‹åŠŸèƒ½\n1 2 3 4 helm_enabled: true dashboard_enabled: true ingress_nginx_enabled: true ingress_nginx_host_network: true é…ç½® Ansible é ç«¯ç¯€é» è¨­å®šè¦ ssh é ç«¯åŸ·è¡Œçš„å¸³è™Ÿå…å¯†ç¢¼ä½¿ç”¨ sudo\n1 echo \u0026#34;ula ALL=(ALL) NOPASSWD:ALL\u0026#34; | sudo tee /etc/sudoers.d/ula k8s äº‹å‰ç’°å¢ƒè¨­å®š å›åˆ° Ansible æ§åˆ¶ç¯€é»ï¼Œé€éé ç«¯åŸ·è¡Œä¸€æ¬¡è™•ç† Kubernetes å®‰è£å‰çš„ç’°å¢ƒåŸºæœ¬è¨­å®šã€‚\n1 2 3 4 5 6 7 8 9 10 cd kubespray # ç¦ç”¨é˜²ç«ç‰† ansible all -i inventory/mycluster/hosts.yaml -m shell -a \u0026#34;sudo systemctl stop firewalld \u0026amp;\u0026amp; sudo systemctl disable firewalld\u0026#34; # ç¦ç”¨ IPv4 è½‰ç™¼ ansible all -i inventory/mycluster/hosts.yaml -m shell -a \u0026#34;echo \u0026#39;net.ipv4.ip_forward=1\u0026#39; | sudo tee -a /etc/sysctl.conf\u0026#34; # ç¦ç”¨ swap ansible all -i inventory/mycluster/hosts.yaml -m shell -a \u0026#34;sudo sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab \u0026amp;\u0026amp; sudo swapoff -a\u0026#34; é–‹å§‹éƒ¨ç½² 1 ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml å®Œæˆå¾ŒæŸ¥çœ‹åŸ·è¡Œçµæœæ˜¯å¦æˆåŠŸã€‚ æ“ä½œé›†ç¾¤ ç™»å…¥ç¬¬ä¸€å€‹ master nodeï¼Œåˆ‡æ›åˆ° rootï¼Œå°±å¯ä»¥é€é kubectl å‘½ä»¤æ“ä½œé›†ç¾¤äº†!\n1 2 sudo su - kubectl get nodes Reference https://linux.cn/article-15675-1.html ","date":"2024-03-23T16:45:00+08:00","permalink":"http://localhost:1313/p/k8s-kubespray-installation/","title":"[Kubernetes] ä½¿ç”¨ Kubespray å¿«é€Ÿéƒ¨ç½² K8s é›†ç¾¤"},{"content":"TL; DR Kong æœ‰ä¸åŒéƒ¨ç½²æ¶æ§‹ï¼Œæœ¬æ–‡åƒ…ç¤ºç¯„å–®ä¸€çš„ kong gateway ç¯€é»åœ¨ Kubernetes ä¸Šçš„å®‰è£æ–¹å¼ã€‚é€éå®˜æ–¹çš„ helm chart éƒ¨ç½²ï¼ŒåŒ…å« kong adminã€ kong proxy ä»¥åŠåŸç”Ÿçš„ Kong Admin UI - Kong Managerã€‚\nå®‰è£æ­¥é©Ÿ å»ºç«‹ namesapce 1 kubectl create ns kong å»ºç«‹ postgresql database 1 2 3 4 5 6 7 8 9 10 11 # postgre-values.yaml image: tag: 10.23.0 # 11 ç‰ˆå¾Œ kong ä¸ç›¸å®¹ global: storageClass: \u0026#34;rook-ceph-block\u0026#34; postgresql: auth: postgresPassword: \u0026#34;kong\u0026#34; username: \u0026#34;kong\u0026#34; password: \u0026#34;kong\u0026#34; database: \u0026#34;kong\u0026#34; 1 helm install kong-pg -f postgre-values.yaml bitnami/postgresql -n kong å»ºç«‹ kong (å« kong gateway \u0026amp; kong manager) ä¸»è¦ä¿®æ”¹ kong-values.yaml çš„é …ç›®å¦‚ä¸‹ï¼š\nè¨­å®šå¤–éƒ¨ PostgreSQL Database é—œé–‰ Ingress Controllerï¼Œå¦‚æœè¦å»ºç«‹ ingress è³‡æºï¼Œçµ±ä¸€ä½¿ç”¨ Ingress Nginx Controller é—œé–‰æ‰€æœ‰é–‹å•Ÿçš„æœå‹™çš„ https portï¼Œçµ±ä¸€ä½¿ç”¨ httpï¼Œä¸¦é€é Ingress åŠ ä¸ŠåŸå…ˆå·²å­˜åœ¨çš„ Cert-Manager Certificate æš´éœ²æœå‹™ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 # kong-values.yaml env: database: \u0026#34;postgres\u0026#34; pg_host: \u0026#34;kong-pg-postgresql.kong.svc.cluster.local\u0026#34; pg_port: 5432 pg_user: kong pg_password: kong pg_database: kong # ä¸­é–“ç•¥ admin: enabled: true type: ClusterIP loadBalancerClass: annotations: {} labels: {} http: enabled: true servicePort: 8001 containerPort: 8001 parameters: [] tls: enabled: false servicePort: 8444 containerPort: 8444 parameters: - http2 client: caBundle: \u0026#34;\u0026#34; secretName: \u0026#34;\u0026#34; ingress: enabled: true ingressClassName: nginx # TLS secret name. tls: domain-cert-sdsp-stg.com-prod # Ingress hostname hostname: kong-admin.sdsp-stg.com annotations: {} # Ingress path. path: / pathType: ImplementationSpecific # ä¸­é–“ç•¥ proxy: enabled: true type: ClusterIP loadBalancerClass: nameOverride: \u0026#34;\u0026#34; annotations: {} labels: enable-metrics: \u0026#34;true\u0026#34; http: enabled: true servicePort: 80 containerPort: 8000 parameters: [] tls: enabled: false servicePort: 443 containerPort: 8443 parameters: - http2 appProtocol: \u0026#34;\u0026#34; stream: [] ingress: enabled: true ingressClassName: nginx annotations: {} labels: {} hostname: kong.sdsp-stg.com path: / pathType: ImplementationSpecific hosts: [] # TLS secret(s) tls: domain-cert-sdsp-stg.com-prod # ä¸­é–“ç•¥ ingressController: enabled: false # ä¸­é–“ç•¥ manager: enabled: true type: ClusterIP loadBalancerClass: annotations: {} labels: {} http: enabled: true servicePort: 8002 containerPort: 8002 parameters: [] tls: enabled: false servicePort: 8445 containerPort: 8445 parameters: - http2 ingress: enabled: true ingressClassName: nginx tls: domain-cert-sdsp-stg.com-prod hostname: kong-manager.sdsp-stg.com annotations: {} path: / pathType: ImplementationSpecific 1 2 helm repo add kong https://charts.konghq.com helm install kong-dev -f kong-values.yaml kong/kong -n kong ","date":"2024-03-16T16:44:00Z","permalink":"http://localhost:1313/p/single-kong-gw-deploy-on-kubernetes/","title":"[kong] Single Gateway Deployment on Kubernetes"},{"content":"TL; DR åœ¨å…¬å¸éƒ¨å±¬ Ingress è³‡æºå¾Œï¼Œç™¼ç¾ä¸€ç›´æ²’æ³•æ³•åƒåˆ°æŒ‡å®šçš„æ†‘è­‰ï¼Œçµæœæ‰ç™¼ç¾æ˜¯å› ç‚º wildcard çš„å•é¡Œã€‚\nProblem å…¬å¸ Kubernetes ç’°å¢ƒçš„ ingress gateway æœ‰é è¨­çš„ tls æ†‘è­‰ (*.southeastasia.azure.wistron.com)ï¼Œæˆ‘è‡ªå·±ç”³è«‹çš„æ†‘è­‰ç‚º *.wistron.comï¼Œæƒ³è¦æŒ‡å®šçµ¦ tls è·¯å¾‘ç‚º abc.southeastasia.azure.wistron.com çš„ç¶²ç«™ï¼Œä½†éƒ¨ç½²å¾Œé€£ç¶²é ç™¼ç¾åƒçš„æ†‘è­‰æœƒæ˜¯é è¨­çš„ï¼Œè€Œä¸æ˜¯æŒ‡å®šçš„ã€‚\nReason ä½¿ç”¨ wildcard çš„æ†‘è­‰ï¼Œåªèƒ½åƒåˆ°ç¬¬ä¸€éšçš„åŸŸåï¼Œæ¯”å¦‚èªªå¦‚æœæœ‰ *.example.com çš„ SSL è­‰æ›¸ï¼Œé‚£éº¼åƒ…é©ç”¨æ–¼ www.example.com æˆ– XXXX.example.com ç­‰ä¸»æ©Ÿï¼Œä¸é©ç”¨æ–¼ demo.app1.example.com ç­‰ä¸»æ©Ÿã€‚\nReference https://stackoverflow.com/questions/26744696/ssl-multilevel-subdomain-wildcard ","date":"2023-07-23T17:45:00+08:00","permalink":"http://localhost:1313/p/ingress-%E6%8C%87%E5%AE%9A%E4%BA%86-tls-%E6%86%91%E8%AD%89%E5%8D%BB%E5%90%83%E5%88%B0%E4%B8%8D%E6%AD%A3%E7%A2%BA%E7%9A%84/","title":"[Ingress] æŒ‡å®šäº† TLS æ†‘è­‰ï¼Œå»åƒåˆ°ä¸æ­£ç¢ºçš„"},{"content":"TL; DR å› ç‚ºç–«æƒ…ï¼Œè·é›¢ä¸Šä¸€æ¬¡å»æ²–ç¹©å±…ç„¶å·²éäº†å°‡è¿‘å››å¹´ã€‚Covid-19 å¾ä¸€é–‹å§‹çš„åŠä¿¡åŠç–‘åˆ°å¤§çˆ†ç™¼çš„äººäººææ…Œï¼Œå…©å¹´å¤šéå»ï¼Œäººå€‘é–‹å§‹èˆ‡ç—…æ¯’å…±è™•ã€‚ä»Šå¹´å…¨çƒæ—…éŠçµ‚æ–¼éƒ½é–‹æ”¾å•¦! è€Œä»Šå¹´é€£å‡åˆç‰¹åˆ¥å¤šï¼Œå¹´åˆä¸€ç›´æœ‰æ‰“ç®—å‡ºåœ‹ï¼Œç”šè‡³é€£æ—¥å¹£éƒ½è¶è²¶å€¼çš„æ™‚å€™æ›èµ·ä¾†æ”¾äº†ï¼Œæ‹–åˆ°ä¸‰æœˆä¸­æ‰çµ‚æ–¼æ±ºå®šè¦åœ¨ç«¯åˆé€£å‡å®‰æ’å‡ºåœ‹ã€‚ä½†æ®Šä¸çŸ¥å…­æœˆçš„æ—¥æœ¬æ ¹æœ¬ä¸æ˜¯é©åˆå» XDD 3/12 è‡¨æ™‚æ”¹è®Šç›®çš„åœ°ï¼Œä¹Ÿå¾æœ¬ä¾†çš„è‡ªä½è¡Œæ”¹æˆè·Ÿåœ˜æ—…éŠï¼ŒèŠ±äº†ä¸‰å¤©è¨è«–ï¼Œæœ€å¾Œæ‰¾åˆ°ä¸€å€‹å‰›å¥½å¡åœ¨é€£å‡åˆåœ˜è²»ä¸€å€‹äººä¸åŒ…å«å°è²»å…©è¬ä¸€è¶…ä¾¿å®œçš„é¦¬ä¾†è¥¿äºäº”å¤©å››å¤œéŠã€‚åŸæœ¬æ“”å¿ƒçš„è¡Œç¨‹è·Ÿä½å®¿å“è³ªï¼Œåœ¨å»ç©å¾Œæ²’æƒ³åˆ°éƒ½åœ¨æœŸæœ›å€¼ä»¥ä¸Š!\nè¡Œå‰æº–å‚™ é¦¬å¹£åœ¨å°ç£å±¬æ–¼å†·é–€è²¨å¹£ï¼Œæ‰€ä»¥éŠ€è¡Œè³£å‡º 7.å¤š è²·å…¥ 5.å¤š çš„åŒ¯ç‡å®Œå…¨ä¸åˆ’ç®—ï¼Œæ‰ç™¼ç¾å¤§å®¶å¦‚æœè¦æ›é¦¬å¹£éƒ½åœ¨ç•¶åœ°æ›ã€‚å¹¸å¥½åˆ·äº†å…©å¤©èƒŒåŒ…å®¢æ£§ï¼Œç›´æ¥è·Ÿä¸€å€‹å‰›å¾é¦¬ä¾†è¥¿äºå›ä¾†çš„åŒ…å‹ç”¨åŒ¯ç‡ 7 æ›åˆ° $2500 çš„é¦¬å¹£ï¼Œé‚„å¯ä»¥åœ¨å°åŒ—é¢äº¤ï¼Œé¢äº¤å®Œé †ä¾¿å»æ±å€é€›è¡—æ²»è£ Ù©(ËŠá—œË‹*)Ùˆ å¦å¤–ä¹Ÿé †åˆ©åœ¨å‡ºç™¼å‰è²·å¥½ç¶²å¡ï¼Œå…©å¼µ 5 å¤© 10 G é–‹ç†±é»åˆ†äº«ç¶½ç¶½æœ‰é¤˜!\nDAY 1 - é¦¬å…­ç”² èµ·é£› é›–ç„¶æ˜¯è·Ÿåœ˜ï¼Œä½†æ—©ä¸Šå…­é»åŠçš„ç­æ©Ÿä¹Ÿå¹¾ä¹æ˜¯ç´…çœ¼ç­æ©Ÿäº†ï¼Œæ‹”éº»å§ä»–å€‘ä¸€é»åŠå¾å˜‰ç¾©å‡ºç™¼ã€‚æ—©ä¸Šè‡¨æ™‚æ±ºå®šå«è»Šå»ï¼Œæœ¬ä¾†ç”¨å¤§éƒ½æœƒ app åª’åˆäº†ä¸€å€‹ä¸Šåˆæœªæœï¼Œå¾Œä¾†å®¢æœæ‰“ä¾†æœ€å¾Œå¹¸å¥½æœ‰åª’åˆåˆ°è»Š (ã€‚â€¢Ìï¸¿â€¢Ì€ã€‚)ï¼Œå¾æ·¡æ°´ç«¹åœ(é¦¬ä¸Šç”¨äº†æ–°å…¬å¸çš„å…¨è–ªäº‹å‡è«‹ (â‘‰Â¯ê‡´Â¯â‘‰) ) å›åˆ°ä¸­å’Œé‚„èƒ½æ”¶è¡Œææ´—æ¾¡ç¡è¦ºä¸‰é»åŠå‡ºç™¼ï½é›–ç„¶å¤ªèˆˆå¥®äº†ä¹Ÿæ˜¯ç¡ç¡é†’é†’çš„ XD ä¸€åˆ°æ©Ÿå ´å»äº”æ¨“è·Ÿæ‹”éº»å§æœƒåˆåœ¨ 711 åƒæ—©(å®µ?)é¤(å¤œ?)ï¼Œé€£å‡çš„æ©Ÿå ´æœç„¶è¶…ç†±é¬§ï¼Œä¸€å¤§æ¸…æ—©å°±æ“ æ»¿äº†è¦å‡ºåœ‹ç©çš„äººå€‘ï¼Œç©ºæ°£éƒ½ç€°æ¼«è‘—é–‹å¿ƒçš„æ°›åœ! å››é»åŠåˆ°é›†åˆåœ°é»æœƒåˆï¼Œé€™åœ˜ç¸½å…±æœ‰ 19 ä½åœ˜å“¡~æœ‰å››å°åˆ†éšŠ! ç™»æ©Ÿéœ€è¦æ­æ¥é§è»Šåœ¨æ©Ÿå ´ä¸­é–“ä¸Šé£›æ©Ÿï¼Œä¸€ä¸Šé£›æ©Ÿå°±ç«‹é¦¬è£œå€‹çœ ï¼Œä¸éé£›æ©Ÿé¤é¦¬ä¸Šå°±é€ä¾†äº†ï¼Œåƒäº†è•ƒèŒ„æµ·é®®éºµï¼Œåœ¨çœ‹éƒ¨é›»å½±ï¼Œå†å»æˆ´å€‹éš±å½¢çœ¼é¡æ¢³å¦æ‰“æ‰®ä¸€ä¸‹ï¼Œå››å€‹åŠå°æ™‚çš„é£›ç¨‹å¾ˆå¿«å°±åˆ°äº†!\nä¼‘æ¯ç«™ å› ç‚ºå¾å‰éš†å¡ä¸‹é£›æ©Ÿå¾Œåˆ°å…¥å¢ƒå¾Œå¤§ç´„ 12 é»ï¼Œç¬¬ä¸€å¤©çš„è¡Œç¨‹è¦ç›´æ¥é–‹å¾€éº»å…­ç”²ï¼Œå…©å€‹åŠå°æ™‚çš„è»Šç¨‹å¤ªä¹…ï¼Œç•¶åœ°å°éŠ (åœ‹å¤–è·Ÿåœ˜åŸä¾†é™¤äº†é ˜éšŠå¤–ï¼Œé‚„æœƒæœ‰ç•¶åœ°å°éŠ) åœ¨è·¯ä¸Šå…ˆå®‰æ’äº†ä¸€ç«™ä¼‘æ¯ç«™è®“æˆ‘å€‘å…ˆå……é¥‘ï¼Œåœ¨é€™è£¡åƒäº†æ¤°æ¼¿é£¯ã€èœ‚èœœåå¸é…ç”Ÿé›è›‹ã€æ²³ç²‰ã€å’–å“©åŒ…ã€ç‚¸é›ã€ç¾ç¥¿~ éš”å£é‚„æœ‰ç†±æƒ…çš„å°åº¦äººåˆ†äº«ç•¶åœ°æ°´æœé¾å®®æœã€‚\nè·è˜­ç´…å±‹ã€è–ä¿ç¾…æ•™å ‚ ç¬¬ä¸€ç«™åˆ°è·è˜­æ®–æ°‘æ™‚æ‰€å»ºè¨­çš„è·è˜­ç´…å±‹ï¼Œæœ›çœ¼æ”¾å»æ˜¯æ¸…ä¸€è‰²çš„ç´…è‰²å»ºç¯‰ï¼Œæœ‰ç¨®ä¾†åˆ°æ·¡æ°´ç´…æ¯›åŸçš„éŒ¯è¦º XDD æ¥è‘—è«åå…¶å¦™åœ°åè‘—ç¾æ¥çš„è…³è¸èŠ±è»Šï¼Œé‚Šå¤§è²çš„æ’­è‘—è¦è¶´çš„éŸ³æ¨‚ (ä½ æ˜¯æˆ‘çš„èŠ±æœµä¹‹é¡çš„ XDDD) ç¾æ¥çš„åˆ°é”ä¸‹ä¸€ç«™è–ä¿ç¾…æ•™å ‚ã€‚è·Ÿåœ˜å°±æ˜¯æœƒæœ‰é€™å€‹è«åå…¶å¦™ä½†åª½åª½å¯èƒ½å¾ˆå–œæ­¡çš„è¡Œç¨‹å“ˆå“ˆå“ˆ è–ä¿ç¾…æ•™å ‚éºå€è™•ï¼Œå¤åŸå·²ç„¶æ»„æ¡‘è®Šé·ï¼Œå¯ä»¥æ„Ÿå—åˆ°æ­·å²çš„åšé‡ã€‚ å¾€ä¸Šèµ°åˆ°åˆ°åˆ¶é«˜é»ï¼Œå¯ä»¥çœºæœ›é¦¬å…­ç”²çš„é¢¨æ™¯ã€‚\nåœ¨é€™é‚Šå¯ä»¥è¦‹è­‰äº†é¦¬å…­ç”²åœ¨è‘¡è„ç‰™ã€è·è˜­ã€è‹±åœ‹ç­‰æ®–æ°‘å‹¢åŠ›ä¹‹é–“çš„æ›´è¿­ã€‚\næµ·å³½æ¸…çœŸå¯º æ¥è‘—ä¾†åˆ°å»ºåœ¨äººå·¥å³¶ä¸Šçš„æµ·å³½æ¸…çœŸå¯ºï¼Œå¤ªé™½æ˜ ç…§ä¸‹çš„æ™¯è‰²å¾ˆç¾ã€‚ç‚ºäº†å°Šé‡ç•¶åœ°çš„å®—æ•™ç¿’ä¿—ï¼Œå¥³æ€§é€²å…¥æ¸…çœŸå¯ºåƒ…èƒ½éœ²å‡ºè‡‰éƒ¨ï¼Œæ–¼æ˜¯ä¹å°±é«”é©—äº†é¦–æ¬¡çš„ä¼Šæ–¯è˜­æœé£¾ ğŸ¤£ã€‚ æ™šé¤ \u0026amp; é£¯åº— æ™šé¤åƒé¦¬å…­ç”²å¨˜æƒ¹æ‘çš„åˆèœï¼Œé£¯åº—ä½ ECO Treeï¼Œå› ç‚ºæ²’æœ‰ä¸‰äººæˆ¿å‹ï¼Œæ‰€ä»¥æ˜¯é›™äººåºŠå†åŠ ä¸€å¼µè¡Œè»åºŠï¼Œå¹¸å¥½è¡Œè»åºŠå¾ˆå¥½ç¡ (`ï½¥âˆ€ï½¥)b æ™šä¸Šä¸€å®¶äººèµ°åˆ°é™„è¿‘çš„å•†å ´ï¼Œç„¶å¾Œåœ¨å…¨å®¶è²·äº†éš»æ¦´æ§¤å†°åƒ~!\nDAY 2 - é¦¬å…­ç”² é›å ´è¡— ä¸€æ—©ä¾†åˆ°é¦¬å…­ç”²å¸‚ä¸­å¿ƒçš„ä¸€æ¢é›å ´è¡—ï¼Œè²·ä¸€äº›ä¼´æ‰‹ç¦®ï¼Œ è§€å…‰æœ‰æ•…äº‹çš„å’–å•¡é¤¨ï¼Œå¦å¤–ç¸½æ˜¯å°é€™ç¨®æ¶¼èŒ¶æ²’ä»€éº¼æŠ—æ‹’åŠ› ä¸­é€”åœ¨ä¸€é–“é«”é©—å¨˜æƒ¹æœçš„èŒ¶é¤¨åƒäº†äº”é¡å…­è‰²çš„å¨˜æƒ¹ç³• é‚„åƒäº†æ¦´æ§¤æ³¡èŠ™ (å„ç¨®æ¦´æ§¤ XDDD)ï¼Œæ¥è‘—ä¹Ÿåœ¨é€™é‚Šäº«ç”¨åˆé¤é›é£¯ç²’ã€‚ Malaysia Heritage Studios é€™å€‹æ™¯é»æ˜¯ä»¥é¦¬ä¾†è¥¿äºæ–‡åŒ–éºç”¢ç‚ºä¸»é¡Œï¼Œåœ’å€å…§æœ‰13å€‹å·çš„é¦¬ä¾†å‚³çµ±æˆ¿å±‹ï¼Œæ¯å€‹æˆ¿å±‹éƒ½æ„å¤–çš„å¥½æ‹ (â›â—¡â›âœ¿) ä½†å¤©æ°£å¯¦åœ¨æ˜¯å¤ªç†±äº†ï¼Œå¤§å®¶éƒ½é€›å¾ˆå¿«ï¼Œæœ€å¾Œè®Šæˆæ“ åœ¨å…¥å£çš„å•†åº—å°å±‹å¹å†·æ°£ï¼Œæ®Šä¸çŸ¥å…¨ç»ç’ƒçš„ç‰†é¢ä¸€é»æ¶¼æ„Ÿéƒ½æ²’æœ‰ï¼Œæ‰€ä»¥æˆ‘å€‘ä¸€å®¶åœ¨æ™‚é–“é‚„æ²’çµæŸå‰å°±ä¸ŠéŠè¦½è»Šå¹å†·æ°£äº† (à¹‘Â¯âˆ€Â¯à¹‘)\nå¦å¤–åœ¨é›¢é–‹æ™¯é»å¾Œï¼Œç‰¹åœ°è·¯éæ°´æœæ”¤ï¼Œçµ‚æ–¼åƒåˆ°æ¦´æ§¤å±±ç«¹å•¦!!!!!!!! æ™šé¤ \u0026amp; é£¯åº— æ™šé¤æ˜¯åœ¨å»é£¯åº—çš„è·¯ä¸Šåƒçš„æœ‰é»å°å¼çš„åˆèœï¼Œé£¯åº—ä½ Lexis ç´…èŠ±ç³»åˆ—çš„é£¯åº— (éäº”æ˜Ÿç´šé‚£å€‹ ğŸ¤£)ï¼Œä½†ä¸€æ¨£åœ¨å¾ˆé…·çš„æ°´ä¸Šï¼Œç„¶å¾Œå»æ‰€åœ¨é€²é–€è™•çš„è¨­è¨ˆå¯¦åœ¨æ˜¯å¤ªæ–°å¥‡äº†??? DAY 3 - å‰éš†å¡ ç¬¬ä¸‰å¤©çµ‚æ–¼é‡åˆ°ç†±å¸¶å‹æ°£å€™äº†ï¼Œå¹¸å¥½é›¨éƒ½ä¾†å¾—å¿«å»å¾—å¿«ã€‚\nåœ‹å®¶çš‡å®®ï¼ˆIstana Negaraï¼‰ é€™é‚Šæ˜¯é¦¬ä¾†è¥¿äºæœ€é«˜å…ƒé¦–çš„å®®é‚¸ï¼Œåªèƒ½åœ¨å¤–è§€åƒè§€æ‹ç…§ï¼Œä¹Ÿæœ‰é¨è‘—é¦¬çš„æ†²å…µé§å®ˆã€‚\nä¸­é¤çµ‚æ–¼åƒåˆ°é¦¬ä¾†è¥¿äºå¿…åƒçš„è‚‰éª¨èŒ¶ï¼Œåƒå®Œé‚„åœ¨åº—é–€å£å¤–å†è²·æ„å¤–å¥½åƒçš„æ¤°å­é›ªèŠ±å†° ( Ë˜â€¢Ï‰â€¢Ë˜ ) ç¨ç«‹å»£å ´ ä»Šå¤©æ˜¯å¾ˆæ”¿æ²»çš„ä¸€å¤© XDD ç¨ç«‹å»£å ´æ˜¯è‹±åœ‹çµ±æ²»çµæŸå¾Œå®£å¸ƒç¨ç«‹çš„åœ°æ–¹ï¼Œæ—é‚Šç‚ºä¿ç•™ç™¾å¹´çš„å°åˆ·å±€å¤–è§€ï¼Œé¤¨å…§å±•ç¤ºä¸€äº›å‰éš†å¡çš„æ­·å²ï¼Œé‚„æœ‰ç¸®å°ºçš„å¤§å‹åŸå¸‚æ¨¡å‹ã€‚å¦å¤–é‚„æœ‰åœ¨åœ‹æ——æ†å‰çš„ 0 å…¬é‡Œçš„çŸ³ç¢‘ï¼Œä»£è¡¨å‰éš†å¡çš„å¸‚ä¸­å¿ƒæœ¬èº«! (ä¸çŸ¥é“æœ€å¾Œä¸€å¼µå°éŠçš„æ‰‹æ©Ÿç•«è³ªèŠ±ç”Ÿä»€éº¼äº‹ XDDD ???\nåœ‹å®¶éŠ€è¡Œåšç‰©é¤¨ å±•ç¤ºå„å¼å„æ¨£çš„èˆŠéŒ¢å¹£ã€ç´€å¿µå¹£é‚„æœ‰å„åœ‹éŒ¢å¹£ç­‰ï¼Œæ˜¯é¦¬ä¾†è¥¿äºéŒ¢å¹£ç™¼å±•å²çš„ç ”ç©¶ä¸­å¿ƒã€‚\n(ä¸Šåœ–è¡¨ç¤ºæ‰¾åˆ°æ–°å°å¹£ ( â€¢ Ì€Ï‰â€¢Ì )\né›™å­æ˜Ÿ å…ˆåˆ°å››å­£é…’åº—çš„å•†åº—è¡—åƒæ™šé¤ï¼Œå†è‡ªç”±æ™‚é–“å›åˆ° LLC åŸä¸­åŸé™½å…‰å»£å ´è³¼ç‰©ä¸­å¿ƒé€›è¡—ã€‚ ç„¶å¾Œæ˜¯åœ¨é›™å­æ˜Ÿå¤§å»ˆå‰ç•™å½± æ¥è‘—åœ¨å‰éš†å¡å¸‚ä¸­å¿ƒç©¿æ¢­å‰å¾€ Saloma Link å½©è‰²è¡Œäººå¤©æ©‹ï¼Œå°è±¡æœ€æ·±åˆ»çš„å°±æ˜¯æ–‘é¦¬ç·šç¶ ç‡ˆé•·åº¦çŸ­!åˆ°!ä¸!è¡Œ!ï¼Œæ¯æ¬¡éé¦¬è·¯éƒ½å¾—å¥”è·‘ XDDD é£¯åº— æœ€å¾Œå…©æ™šçš„é£¯åº—éƒ½ä½æ¥å¾…å¤§å»³åƒéœæ ¼è¯èŒ²çš„ Bespoke Hotel Puchong è’²ç¨®å®šåˆ¶é…’åº—ï¼Œæœ€ä»¤äººæ»¿æ„çš„å®ƒçš„åœ°ç†ä½ç½®ï¼Œå¤§é–€éé¦¬è·¯å¾Œæœ‰ä¸€å€‹è³£å±±ç«¹æ¦´æ§¤çš„æ°´æœæ”¤ï¼Œå¾Œé–€æœ‰ä¸€é–“å°åº¦æ–™ç†ï¼Œé€£çºŒå…©æ™šéƒ½åƒäº†ç”©é¤… (Â´Ú¡`) æœ€å¾Œä¸€å¤©æ™šä¸Šé‚„å› ç‚ºè¦æŠŠæ›çš„é¦¬ä¾†å¹£å…¨èŠ±å…‰ï¼Œåœ¨é™„è¿‘çš„ä¾¿åˆ©å•†åº—ç‹‚æƒé›¶é£Ÿï¼Œæœ€å¾Œåœ¨åºŠä¸Šæ“ºå‡ºæœ¬æ¬¡çš„æˆ°åˆ©å“ï¼Œæœ‰é»å£¯è§€å¶ ( Â° â–½Â°)ãƒ DAY 4 - å‰éš†å¡ é»‘é¢¨æ´ æ˜¯å°åº¦æ•™è–åœ°ï¼Œè¦é€²å…¥é»‘é¢¨æ´å‰æœ‰è‰²å½©ç¹½ç´›çš„éšæ¢¯ã€‚è£¡é¢æ˜¯é«˜è³çš„æ´ç©´å’Œå£¯è§€çš„å°åº¦æ•™ç¥åƒï¼Œå¤–é¢è¢«çŒ´å­è·Ÿé´¿å­åŒ…åœè‘—ï¼Œæœç„¶æ˜¯è–åœ°! å¦å¤–é‚„å¤§é–‹çœ¼è¦‹çš„çœ‹åˆ°è‹¦è¡ŒéŠè¡Œã€‚ è§€å…‰å®Œé‚„å»ç¾é£Ÿå•†åº—è¡—å–äº†ç‡•çª©è·Ÿæ¤°å­ (Â´Ú¡`) REXKL è¿·å®®æ›¸åº— ä½æ–¼ç•¶åœ°èŒ¨å» è¡—çš„è¤‡åˆå¼å•†å ´ REXKL å…§ï¼Œæ›¸åº—çš„ä½ˆå±€éå¸¸ç¨ç‰¹ï¼Œå°±åƒä¸€å€‹è¿·å®®ä¸€æ¨£ï¼Œç…§æ‹èµ·ä¾†æ›¸é¦™å‘³éƒ½å‡ºä¾†äº† (ã††á´—ã††) èŒ¨å» è¡—ã€é¬¼ä»”å·· æ¥è‘—æ­¥è¡Œåˆ°é€™å…©å€‹åœ°æ–¹ï¼ŒèŒ¨å» è¡—å°±æ˜¯é‚£ç¨®äºæ´²éƒ½ä¸€å®šæœƒæœ‰çš„ä»¿ç²¾å“è¡— Î£(â˜‰â–½â˜‰\u0026ldquo;a é¬¼ä»”å··çš„å»ºç¯‰é¢¨æ ¼èåˆäº†é¦¬ä¾†ã€è¯äººã€å°åº¦å’Œæ­æ´²çš„å…ƒç´ ã€‚å··é“å…©æ—æ—ç«‹è‘—é¤å»³ï¼Œæœ‰è‘—ç²‰å½©å»ºç¯‰ã€‚ The LINC KL ä¾†è³¼ç‰©ä¸­å¿ƒå…§éƒ¨è¸©é»æ‰“å¡æ™¯é»ï¼ŒåŒ…æ‹¬ç™¾å¹´è€æ¨¹ã€4è¬å¤šéš»çš„å½©ç´…ç´™é¶´è·Ÿå½©è™¹éšæ¢¯! åˆé¤æ™šé¤ åˆé¤(å·¦ä¸Š)åˆæ˜¯åƒåˆèœï¼Œæ™šé¤åƒäºç¾…è¡—å¤œå¸‚åƒé»ƒäºè¯ç‡’çƒ¤é¤å»³çš„ç‡’é›ç¿¼åŠé’æª¸è©±æ¢…æ±ï¼Œå†å°é€›ä¸€ä¸‹é€™æ¢å°å¤œå¸‚ï¼Œç„¶å¾Œåˆåˆåˆæ˜¯æ¦´æ§¤ç²‰æ¢ XDDã€‚ DAY 5 - å‰éš†å¡ ç²‰ç´…æ¸…çœŸå¯º é€™é‚Šä¸å¾—ä¸ä¾†å€‹ photo dumpï¼Œå› ç‚ºç¬¬ä¸€æ¬¡çœ‹åˆ°ç²‰å¾—é€™éº¼æ¼‚äº®çš„å»ºç¯‰ï¼Œè¶…ç´šå¤¢å¹» (ï½¡Ã­ _ Ã¬ï½¡) ä»Šå¤©å¤©æ°£åˆè®Šè¶…æ£’ï¼Œå¤ªé™½å¤§åˆ°ä¸è¡Œ XD å¾ˆå¹¸é‹æ²’é‡åˆ°ç¦®æ‹œï¼Œæ‰€ä»¥å¯ä»¥é€²å»åƒè§€ï¼Œå…§è£å¤©èŠ±æ¿ä¹Ÿæ˜¯ç¾çš„é©šäºº! åœ¨æ´—æ‰‹é–“æ—çš„æ²³ç•”é¢¨æ™¯ä¹Ÿå¾ˆæ¼‚äº® è¿”ç¨‹ å›ç¨‹çš„æœ€å¾ŒæŠŠå‰©ä¸‹çš„é›¶éŒ¢å…¨éƒ¨èŠ±åœ¨æ©Ÿå ´æ˜Ÿå·´å…‹ï¼Œç”¨å¾—å¯è¬‚æ˜¯ä¸€æ¯›ä¸å‰©ï¼Œå®Œç¾çš„çµæŸäº”å¤©å››å¤œçš„é¦¬ä¾†è¥¿äºéŠäº† (Ë¶Â´U`Ëµ) å¿ƒå¾— åœ¨æ­¤è¡Œä¹‹å‰ï¼Œå› ç‚ºå½ˆæ€§åº¦ä¸é«˜æ‰€ä»¥åŸæœ¬å°è·Ÿåœ˜æ—…éŠæŒºæ’æ–¥çš„ã€‚ä½†é€™è¶Ÿä¸‹ä¾†ï¼Œçµ‚æ–¼é«”æœƒåˆ°è·Ÿåœ˜çš„å¥½è™•äº†ï¼Œä¸ç”¨ç…©æƒ±äº¤é€šåŠè¡Œç¨‹ï¼Œå°éŠé‚„æœƒä¾ç…§æ™¯é»ä½ç½®å½ˆæ€§å®‰æ’ï¼Œè®“è·¯é€”æ›´åŠ é †æš¢ï¼Œåªè¦ç•¶å¤©é–‹å¿ƒå‡ºéŠã€ç„¶å¾Œå®‰å¿ƒå›å®¶å³å¯~ ä¸‹æ¬¡è‡ªç”±è¡Œä¸æ–¹ä¾¿çš„åœ‹å®¶ï¼Œå°±ç›´æ¥ç„¡è…¦è·Ÿåœ˜å•¦!!\n","date":"2023-06-08T15:00:00+08:00","permalink":"http://localhost:1313/p/malaysia-2023/","title":"äº”å¤©å››å¤œé¦¬ä¾†è¥¿äºéŠè¨˜ğŸï¸"},{"content":"TL;DR æœ¬ç¯‡æ–‡ç« è¨˜éŒ„å¦‚ä½•åœ¨ aplpine base çš„ container ä¸­æ–¼æ¯æœˆçš„æœ€å¾Œä¸€å€‹ç¦®æ‹œæ—¥åŸ·è¡ŒæŒ‡å®šä»»å‹™ã€‚\nSoluiton 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: batch/v1 kind: CronJob metadata: name: hello namespace: test spec: schedule: \u0026#34;0 8 * * 0\u0026#34; jobTemplate: spec: template: spec: containers: - name: hello image: alpine:3.16 resources: limits: cpu: \u0026#34;2\u0026#34; memory: 4000Mi imagePullPolicy: Always command: - /bin/sh - -c - \u0026#39;[ $(date +%m) -ne $(date -d \u0026#34;@$(($(date +%s) + 604800))\u0026#34; +%m) ] \u0026amp;\u0026amp; echo Hello from the Kubernetes cluster\u0026#39; restartPolicy: OnFailure å‘ åŸæœ¬å‰é¢åˆ¤æ–·çš„èªæ³•æ˜¯å¯«åœ¨ ubuntu ä¸‹ run å¾—å¥½å¥½çš„ [ $(date +%m) -ne $(date -d +7days +%m) ]ï¼Œä½†æ˜¯æ®Šä¸çŸ¥æ¬åˆ° alpine container å¾Œæœƒå ±éŒ¯ date: invalid date '+7days'ã€‚æ‰€ä»¥æ”¹å¯«æˆ apline ç’°å¢ƒä¸­èªå¾—çš„ -d æ ¼å¼ï¼\nReference https://stackoverflow.com/questions/71651529/schedule-cronjob-for-last-day-of-the-month-using-kubernetes https://unix.stackexchange.com/a/522622 ","date":"2022-10-23T19:08:00Z","permalink":"http://localhost:1313/p/cronjob-for-last-day-of-the-month-using-kubernetes/","title":"ä½¿ç”¨ alpine base image åœ¨æŒ‡å®šæ™‚é–“å®šæœŸåŸ·è¡Œ CronJob"},{"content":"TL;DR åœ¨ windows ç’°å¢ƒçš„ wsl2 Ubuntu ä¸Šè·‘äº† Minikube è¦ç”¨ä¾†æ¸¬è©¦ kubernetes çš„æ‡‰ç”¨ä½ˆå±¬ï¼Œä½†ä¸€ç›´å¡åœ¨æ‹‰å–å…¬å¸å…§éƒ¨ harbor é¡åƒçš„æ™‚å€™å ± x509: certificate signed by unknown authority éŒ¯èª¤ã€‚\nå˜—è©¦åœ¨ docker desktop çš„ docker engine åŠ ä¸Š insecure-registries çš„åƒæ•¸ä½†æ˜¯ minikube ä¸æ›‰å¾—ç‚ºä»€éº¼åƒä¸åˆ°ï¼Œä¹Ÿè©¦éæŠŠæ†‘è­‰æ”¾åˆ° /etc/ssl/certs ä¸‹ä½†ä¸€æ¨£ä¸èªå¾— =__=? ç¸½ä¹‹æœ€å¾Œæ‰ç™¼ç¾ minikube çš„ docker è·Ÿ docker desktop çš„ docker ç’°å¢ƒæ˜¯åˆ†é–‹çš„ã€‚\nSolution 1 vi ~/.minikube/machines/\u0026lt;PROFILE_NAME\u0026gt;/config.json (in my case ~/.minikube/machines/minikube/config.json) add private repo on InsecureRegistry attribute (json path: HostOptions.EngineOptions.InsecureRegistry) 1 2 minikube stop minikube start Then, change the Docker daemon from Minikube\n1 eval $(minikube docker-env) Reference https://stackoverflow.com/questions/38748717/can-not-pull-docker-image-from-private-repo-when-using-minikube https://tachingchen.com/tw/blog/build-docker-image-in-minikube-vm/ https://stackoverflow.com/questions/52310599/what-does-minikube-docker-env-mean ","date":"2022-10-23T19:07:00Z","permalink":"http://localhost:1313/p/minikube-pull-harbor-image/","title":"Minikube Pull Image from Private Repository in WSL2"},{"content":"TL;DR åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­è©¦äº†åœ¨ alpine docker container ä¸­ä½¿ç”¨ non root user è·‘ crondï¼Œä½†å°‡ build å¥½çš„ docker image æ¬åˆ° kubernetes çµ¦ deployment çš„ pod ä½¿ç”¨æ™‚ï¼Œå»æœƒå‡ºç¾ initgroup operation not permitted çš„éŒ¯èª¤ã€‚\nè¸©å‘è¸©äº†æ•´æ•´ä¸‰å¤©ï¼Œè©²æ”¹çš„æ¬Šé™éƒ½æ”¹äº†ï¼Œæœ€å¾Œçµ‚æ–¼æ‰¾åˆ° supercronic é€™å€‹é…·æ±è¥¿ T_T\nDockerfile é€™é‚Šçš„ dockerfile ç›´æ¥å…ˆä¸‹è¼‰å¥½ supercronic build å¥½çš„ binaryï¼Œå† COPY é€² image ä¸­ï¼Œä¹Ÿå¯ä»¥åƒè€ƒ installation instruction åœ¨ build çš„éšæ®µä¸‹è¼‰ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # We want to populate the module cache based on the go.{mod,sum} files. COPY go.mod . COPY go.sum . RUN go mod download COPY . . # Build the Go app RUN go build -o ./out/ccoe-bot . # Start fresh from a smaller image FROM harbor.wistron.com/base_image/alpine:3.12 USER root RUN apk update \u0026amp;\u0026amp; addgroup --gid 1000 ccoebot \u0026amp;\u0026amp; adduser --disabled-password --ingroup ccoebot --uid \u0026#34;1000\u0026#34; ccoebot COPY --from=build_base --chown=ccoebot:ccoebot /tmp/ccoe-bot/out/ccoe-bot /home/ccoebot/app/ccoe-bot COPY bin/git-sync /usr/bin COPY bin/terragrunt /usr/bin COPY bin/terrascan /usr/bin COPY bin/supercronic /usr/bin COPY --chown=ccoebot:ccoebot init.sh /home/ccoebot/app USER ccoebot RUN git clone https://gitlab.wistron.com/ccoe/terrascan_policy.git /home/ccoebot/terrascan_policy WORKDIR /home/ccoebot/terrascan_policy RUN terrascan init -c terrascan-config.toml WORKDIR /home/ccoebot/.terrascan RUN git config --bool branch.master.sync true \u0026amp;\u0026amp; git branch -D HEAD # set scheduler for git-sync RUN echo \u0026#34;*/10 * * * * cd /home/ccoebot/.terrascan \u0026amp;\u0026amp; date \u0026gt;\u0026gt; /home/ccoebot/app/sync.log \u0026amp;\u0026amp; /usr/bin/git-sync \u0026gt;\u0026gt; /home/ccoebot/app/sync.log\u0026#34; \u0026gt;\u0026gt; /home/ccoebot/app/mycron # This container exposes port 8080 to the outside world EXPOSE 8080 # Run the binary program produced by `go install` #CMD [\u0026#34;/app/ccoe-bot\u0026#34;] # repack the ccoe-bot and crond to init.sh CMD [\u0026#34;/home/ccoebot/app/init.sh\u0026#34;] init.sh é€™è£¡åˆæ˜¯å¦ä¸€å€‹è¦æ³¨æ„çš„åœ°æ–¹ï¼Œå› ç‚º supercronic ä¹Ÿæ˜¯ä¸€å€‹è¦è·‘çš„ä½¿ç”¨è€…ç¨‹åºï¼Œæ•…é€™å€‹æ¡ˆä¾‹åŒæ™‚æœƒæœ‰å…©å€‹ç¨‹åºéœ€è¦åœ¨ CMD è£¡é¢ä¸€åŒè·‘èµ·ï¼Œä½¿ç”¨ä¸‹é¢çš„å¯«æ³•å®Œæˆåœ¨åŒå€‹ container ä¸­è·‘å…©å€‹ç¨‹åºã€‚\n1 2 3 4 5 6 7 #!/bin/bash /usr/bin/supercronic /home/ccoebot/app/mycron \u0026amp; P1=$! /home/ccoebot/app/ccoe-bot \u0026amp; P2=$! wait $P1 $P2 æˆ–æ˜¯\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # Start the first process /usr/bin/supercronic /home/ccoebot/app/mycron \u0026amp; # Start the second process /home/ccoebot/app/ccoe-bot \u0026amp; # Wait for any process to exit wait -n # Exit with status of process that exited first exit $? kubernetes deployment yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion: apps/v1 kind: Deployment metadata: name: dev-ccoebot namespace: atlantis spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: dev-ccoebot strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: spec: containers: - image: harbor.wistron.com/k8sprdwhqccoe/ccoe-bot:nonroot imagePullPolicy: IfNotPresent name: dev-ccoebot resources: requests: memory: \u0026#39;256Mi\u0026#39; cpu: \u0026#39;512m\u0026#39; limits: memory: \u0026#39;1024Mi\u0026#39; cpu: \u0026#39;1024m\u0026#39; securityContext: runAsUser: 1000 runAsGroup: 1000 allowPrivilegeEscalation: false privileged: false readOnlyRootFilesystem: false runAsNonRoot: true stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsPolicy: ClusterFirst imagePullSecrets: - name: hbsrt restartPolicy: Always schedulerName: default-scheduler securityContext: runAsUser: 1000 runAsGroup: 1000 fsGroup: 1000 runAsNonRoot: true terminationGracePeriodSeconds: 30 Result Reference æ‹¯æ•‘æˆ‘ç”¨ supercronic çš„æ–‡ç« , æ„Ÿè¬ alesk å¤§\nhttps://gist.github.com/alesk/33b716f04cdce0751473b8232405dc32 Run Mutiple Process in Container: https://docs.docker.com/config/containers/multi-service_container/ https://stackoverflow.com/a/56663151 ","date":"2022-10-23T19:04:00Z","permalink":"http://localhost:1313/p/run-crond-as-non-root-in-alpine-container-by-pod-or-deployment/","title":"Run Crond as Non Root in Alpine Container by Pod/Deployment"},{"content":"ç°¡ä»‹ å®˜æ–¹å®šç¾©:\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\nâ€• offical site Kubernetes åˆç¨±ç‚º k8sï¼Œæœ€åˆç”± google ç”¨ golang é–‹ç™¼è€Œå¾Œé‡‹å‡ºçš„å°ˆæ¡ˆã€‚ç”¨æ–¼æ“ä½œè‡ªå‹•åŒ–å®¹å™¨ï¼ŒåŒ…æ‹¬éƒ¨ç½²ï¼Œèª¿åº¦å’Œç¯€é»é›†ç¾¤é–“æ“´å±•ã€‚\næ¶æ§‹ ä¸Šåœ–ç‚ºä¸€å€‹ç°¡æ˜“çš„ Kubernetes Clusterï¼Œé€šå¸¸ä¸€å€‹ Cluster ä¸­æœƒæœ‰å¤šå€‹ Master ä½œç‚ºå‚™æ´ï¼Œä½†ç‚ºäº†ç°¡åŒ–æˆ‘å€‘åªé¡¯ç¤ºä¸€å€‹ã€‚\né‹ä½œæ–¹å¼ ä½¿ç”¨è€…é€é User Commandï¼ˆkubectlï¼‰å»ºç«‹ Pod æ™‚ç¶“éä½¿ç”¨è€…èº«ä»½çš„èªè­‰å¾Œï¼Œå†å°‡æŒ‡ä»¤å‚³éåˆ° Master Node ä¸­çš„ API Serverï¼ŒAPI Server æœƒæŠŠæŒ‡ä»¤å‚™ä»½åˆ° etcd ã€‚æ¥ä¸‹ä¾† controller-manager æœƒå¾ API Server æ”¶åˆ°éœ€è¦å‰µå»ºä¸€å€‹æ–°çš„ Pod çš„è¨Šæ¯ï¼Œä¸¦æª¢æŸ¥å¦‚æœè³‡æºè¨±å¯ï¼Œå°±æœƒå»ºç«‹ä¸€å€‹æ–°çš„ Podã€‚æœ€å¾Œ Scheduler åœ¨å®šæœŸè¨ªå• API Server æ™‚ï¼Œæœƒè©¢å• controller-manager æ˜¯å¦æœ‰å»ºç½®æ–°çš„ Podï¼Œå¦‚æœç™¼ç¾æ–°å»ºç«‹çš„ Pod æ™‚ï¼ŒScheduler å°±æœƒè² è²¬æŠŠ Pod é…é€åˆ°æœ€é©åˆçš„ä¸€å€‹ Node ä¸Šé¢ã€‚\nç¯€é»èˆ‡çµ„ä»¶ Master Node ç‚º Kubernetes å¢é›†çš„æ§åˆ¶å°ï¼Œè² è²¬ç®¡ç†é›†ç¾¤ã€å”èª¿æ‰€æœ‰æ´»å‹•ï¼ŒåŒ…å«çš„å…ƒä»¶å¦‚ä¸‹ï¼š\nAPI Server API Server ç®¡ç† Kubernetes çš„æ‰€æœ‰ api interfaceï¼Œç”¨ä¾†å’Œé›†ç¾¤ä¸­çš„å„ç¯€é»é€šè¨Šä¸¦é€²è¡Œæ“ä½œã€‚\nScheduler Scheduler æ˜¯ Pods èª¿åº¦å“¡ï¼Œç›£è¦–æ–°å»ºç«‹ä½†é‚„æ²’æœ‰è¢«æŒ‡å®šè¦è·‘åœ¨å“ªå€‹ Worker Node ä¸Šçš„ Podï¼Œä¸¦æ ¹æ“šæ¯å€‹ Node ä¸Šé¢è³‡æºå»å”èª¿å‡ºä¸€å€‹æœ€é©åˆæ”¾ç½®çš„å°è±¡çµ¦è©² Podã€‚\nController Manager è² è²¬ç®¡ç†ä¸¦é‹è¡Œ Kubernetes controller çš„çµ„ä»¶ï¼Œcontroller æ˜¯è¨±å¤šè² è²¬ç›£è¦– Cluster ç‹€æ…‹çš„ Processï¼Œåˆå¯åˆ†ç‚ºä¸‹åˆ—ä¸åŒçš„ç¨®é¡\nNode controller - è² è²¬é€šçŸ¥èˆ‡å›æ‡‰ç¯€é»çš„ç‹€æ…‹ Replication controller - è² è²¬æ¯å€‹è¤‡å¯«ç³»çµ±å…§ç¶­æŒè¨­å®šçš„ Pod æ•¸é‡ End-Point controller - è² è²¬ç«¯é»çš„æœå‹™ç™¼å¸ƒ Service Account \u0026amp; Token controller - è² è²¬å‰µå»ºæœå‹™å¸³æˆ¶èˆ‡æ–°ç”Ÿæˆçš„ Namespace çš„ API å­˜å– Token etcd ç”¨ä¾†å­˜æ”¾ Kubernetes Cluster çš„è³‡æ–™ä½œç‚ºå‚™ä»½ï¼Œç•¶ Master å› ç‚ºæŸäº›åŸå› è€Œæ•…éšœæ™‚ï¼Œæˆ‘å€‘å¯ä»¥é€é etcd å¹«æˆ‘å€‘é‚„åŸ Kubernetes çš„ç‹€æ…‹ã€‚\nWorker Node ç‚º Kubernetes çš„ runtime åŸ·è¡Œç’°å¢ƒï¼ŒåŒ…å«çš„å…ƒä»¶å¦‚ä¸‹ï¼š\nPod Kubernetes pod æ˜¯ Kubernetes ç®¡ç†çš„æœ€å°å–®å…ƒï¼Œè£¡é¢åŒ…å«ä¸€å€‹æˆ–å¤šå€‹ containerï¼Œå¯è¦–ç‚ºä¸€å€‹æ‡‰ç”¨ç¨‹å¼çš„é‚è¼¯ä¸»æ©Ÿã€‚ åŒä¸€å€‹ Pod ä¸­çš„ Containers å…±äº«ç›¸åŒè³‡æºåŠç¶²è·¯ï¼Œå½¼æ­¤é€é local port number æºé€šã€‚pod é‹è¡Œåœ¨ç§æœ‰éš”é›¢çš„ç¶²çµ¡ä¸Šï¼Œé»˜èªæƒ…æ³ä¸‹åœ¨åŒä¸€é›†ç¾¤çš„å…¶ä»– pod å’Œ service ä¸­å¯è¦‹ï¼Œä½†æ˜¯å¤–éƒ¨ä¸å¯è¦‹ï¼Œéœ€è¦è—‰åŠ© service æš´éœ²çµ¦å¤–éƒ¨ã€‚\nKubelet Kubelet æ¥å— API server çš„å‘½ä»¤ï¼Œç”¨ä¾†å•Ÿå‹• pod ä¸¦ç›£æ¸¬ç‹€æ…‹ï¼Œç¢ºä¿æ‰€æœ‰ container éƒ½åœ¨é‹è¡Œã€‚å®ƒæ¯éš”å¹¾ç§’é˜å‘ master node æä¾›ä¸€æ¬¡ heartbeatã€‚å¦‚æœ replication controller æœªæ”¶åˆ°è©²æ¶ˆæ¯ï¼Œå‰‡å°‡è©²ç¯€é»æ¨™è¨˜ç‚ºä¸æ­£å¸¸ã€‚\nKube Proxy é€²è¡Œç¶²è·¯é€£ç·šçš„ forwardingï¼Œè² è²¬å°‡ request è½‰ç™¼åˆ°æ­£ç¢ºçš„ containerã€‚\nResource https://blog.sensu.io/how-kubernetes-works https://medium.com/@C.W.Hu/kubernetes-basic-concept-tutorial-e033e3504ec0 https://ithelp.ithome.com.tw/articles/10202135 ","date":"2022-09-07T21:13:00Z","permalink":"http://localhost:1313/p/kubernets-basic/","title":"Kubernetes Introduction"},{"content":"å•é¡Œ åœ¨ä½¿ç”¨ kustomize é…ç½® Kubernetes è³‡æºæ™‚ï¼ŒKustomization å®šç¾©çš„ ConfigMap ç„¡æ³•æ­£ç¢ºçš„æ¸²æŸ“åˆ° CronJob è³‡æºä¸­ã€‚ åŸ yaml æª”å¦‚ä¸‹:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # å»ºç«‹ templatevar æ–‡ä»¶ cat \u0026lt;\u0026lt;EOF \u0026gt;templatevar FOO=Bar EOF # å»ºç«‹ cronjob æ–‡ä»¶ cat \u0026lt;\u0026lt;EOF \u0026gt;cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: hello namespace: infrase spec: schedule: \u0026#34;25,45,05 * * * *\u0026#34; concurrencyPolicy: Replace jobTemplate: spec: template: spec: containers: - name: hello image: busybox:1.28 resources: limits: cpu: \u0026#34;1\u0026#34; memory: 500Mi imagePullPolicy: IfNotPresent securityContext: runAsNonRoot: true runAsUser: 1000 allowPrivilegeEscalation: false command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster \u0026amp;\u0026amp; cat /config/templatevar volumeMounts: - mountPath: /config/ name: templatevar volumes: - name: templatevar configMap: name: templatevar restartPolicy: OnFailure EOF # å»ºç«‹ kustomization æ–‡ä»¶ cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - cronjob.yaml configMapGenerator: - name: templatevar files: - templatevar EOF ä½¿ç”¨ kustomize æ¸²æŸ“å¾Œï¼Œå¯ä»¥çœ‹åˆ° CronJob ä¸­æŒ‡å®šçš„ ConfigMap æ²’æœ‰æ­£ç¢ºåƒåˆ° configMapGenerator æ‰€ç”¢ç”Ÿçš„æª”æ¡ˆã€‚\n1 kubectl kustomize ./ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 apiVersion: v1 data: templatevar: \u0026#34;FOO=Bar\u0026#34; kind: ConfigMap metadata: name: templatevar-tk9cdghbt6 namespace: infrase --- apiVersion: batch/v1 kind: CronJob metadata: name: hello namespace: infrase spec: jobTemplate: spec: template: spec: containers: - command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster \u0026amp;\u0026amp; cat /config/templatevar image: busybox:1.28 imagePullPolicy: IfNotPresent name: hello securityContext: allowPrivilegeEscalation: false runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /config/ name: templatevar imagePullSecrets: - name: hbsrt restartPolicy: OnFailure volumes: - configMap: name: templatevar name: templatevar schedule: 57 * * * * è§£æ±ºæ–¹å¼ éœ€è¦åœ¨ kustomiztion æª”æ¡ˆä¸­æŒ‡å®š namespace\n1 2 3 4 5 6 7 8 9 10 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: infrase resources: - cronjob.yaml configMapGenerator: - name: templatevar files: - templatevar Reference https://github.com/kubernetes-sigs/kustomize/issues/1301 ","date":"2022-09-06T21:02:00Z","permalink":"http://localhost:1313/p/kustomization-render-failed-in-cronjob/","title":"Kustomize Canâ€™t Render the ConfigMap Hashing Name to CronJob Resource"},{"content":"å‰è¨€ åŸå…ˆé›†ç¾¤ä½¿ç”¨çš„ helm chart ç‚º stable/nginx-ingressï¼Œè€Œæ­¤ helm chart å·²ç¶“è¢«æ£„ç”¨ï¼Œè‹¥ nginx ç¶­æŒåœ¨èˆŠç‰ˆçš„è©±ï¼Œä¹‹å¾Œæ–°çš„æ¼æ´ä¿®è£œéƒ½ç„¡æ³•è¢«å«æ‹¬ã€‚\næ­¤ç¯‡è¨˜éŒ„å¦‚ä½•å°‡é›†ç¾¤ä¸Šé¢è·‘çš„ nginx-ingress-controller æ›æˆæ–°çš„ç‰ˆæœ¬çš„ chart ingress-nginx/ingress-nginxã€‚\nCurrent stable/nginx-ingress åŸå…ˆ nginx-ingress ä½¿ç”¨çš„ç‰ˆæœ¬\n1 2 3 4 5 6 7 8 $ kubectl exec -it -n nginx-ingress nginx-ingress-controller-585bb7f5b4-2nlzz -- /nginx-ingress-controller ------------------------------------------------------------------------------- NGINX Ingress controller Release: v0.34.1 Build: v20200715-ingress-nginx-2.11.0-8-gda5fa45e2 Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.19.1 ------------------------------------------------------------------------------- æŸ¥è©¢è©² helm chart æœ‰æ²’æœ‰æ›´æ–°çš„ç‰ˆæœ¬å¯ç›´æ¥æ›´æ–°ï¼Œä½†çµæœå¦‚ä¸‹ï¼Œç›®å‰é›†ç¾¤å®‰è£çš„å·²ç¶“æ˜¯è©² chart çš„æœ€æ–°ç‰ˆæœ¬äº†ï¼Œä¸”å·²æ¨™ç¤º deprecated ä¸æœƒå†ç¶­è­·ã€‚\n1 2 3 4 5 6 7 8 9 $ helm search repo stable/nginx-ingress --versions NAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 1.41.3 v0.34.1 DEPRECATED! An nginx Ingress controller that us... stable/nginx-ingress 1.41.2 v0.34.1 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.41.1 v0.34.1 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.41.0 v0.34.0 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.40.3 0.32.0 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.40.2 0.32.0 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.40.1 0.32.0 An nginx Ingress Install ingress-nginx/ingress-nginx å®‰è£operator\n1 2 3 4 $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update $ kubectl create ns ingress-nginx $ helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx å®Œæˆå¾Œæ‡‰è©²æœƒçœ‹åˆ°ä»¥ä¸‹è¼¸å‡º\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 [root@master1 ~]# helm install ingress-nginx ingress-nginx/ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx NAME: ingress-nginx LAST DEPLOYED: Wed Aug 18 13:41:42 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running \u0026#39;kubectl --namespace default get services -o wide -w ingress-nginx-controller\u0026#39; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls é©—è­‰å®‰è£ 1 2 3 4 5 6 7 8 9 $ kubectl exec -it -n ingress-nginx ingress-nginx-controller-b65df6fbb-jx4tf -- /nginx-ingress-controller --version ------------------------------------------------------------------------------- NGINX Ingress controller Release: v0.48.1 Build: 30809c066cd027079cbb32dccc8a101d6fbffdcb Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.20.1 ------------------------------------------------------------------------------- Create ingress resource æº–å‚™ ingress resource çš„ yaml æª”ï¼Œè«‹æ³¨æ„é›–ç„¶ä¸Šæ–¹çš„å®‰è£æˆåŠŸçš„è¨Šæ¯æœ‰ç¤ºç¯„ ingress çš„ yamlï¼Œä½†å› ç‚º networking.k8s.io/v1beta1 å·²åœ¨ Kubernetes 1.19+ è¢«æ£„ç”¨ï¼Œå¦‚æœç¶­æŒä½¿ç”¨ï¼Œå‰‡æœƒé‡åˆ° Warning: [networking.k8s.io/v1beta1](http://networking.k8s.io/v1beta1) Ingress is deprecated in v1.19+, unavailable in v1.22+; use [networking.k8s.io/v1](http://networking.k8s.io/v1) Ingressçš„éŒ¯èª¤ï¼Œæ‰€ä»¥åƒè€ƒingress-nginx(https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/) çš„å®˜ç¶²ï¼Œæ”¹æˆä»¥ä¸‹æ ¼å¼ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myservicea annotations: # use the shared ingress-nginx kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: myservicea.foo.org http: paths: - path: / pathType: Prefix backend: service: name: myservicea port: number: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myserviceb annotations: # use the shared ingress-nginx kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: myserviceb.foo.org http: paths: - path: / pathType: Prefix backend: service: name: myserviceb port: number: 80 åˆªé™¤èˆŠçš„ ingress controller ç¢ºèªæ‰€æœ‰æµé‡éƒ½å·²ç¶“å°åˆ°æ–°çš„ controller å¾Œï¼Œå°±å¯ä»¥æŠŠèˆŠçš„ stable/nginx-ingress çš„ controller åˆªæ‰äº†ã€‚\n1 $ helm uninstall nginx-ingress Reference https://jfrog.com/blog/migrate-nginx-from-stable-helm-charts-with-chartcenter/ è£œå……ï¼šå¦‚æœè¦å¯¦ç¾ zero-downtime çš„éƒ¨å±¬ï¼Œå¯ä»¥åƒè€ƒé€™ç¯‡æ–‡ç«  https://medium.com/codecademy-engineering/kubernetes-nginx-and-zero-downtime-in-production-2c910c6a5ed8 ","date":"2022-05-22T21:17:00Z","permalink":"http://localhost:1313/p/helm-migrate-stable-nginx-ingress-to-ingress-nginx/","title":"Helm Migrate stable/nginx-ingress to ingress-nginx"},{"content":"cordonã€drain å’Œ delete ä¸‰å€‹å‘½ä»¤éƒ½æœƒä½¿ kubernetes node åœæ­¢è¢«èª¿åº¦ï¼Œæœ¬ç¯‡è¨˜éŒ„å¦‚ä½•å„ªé›…çš„åˆªé™¤ç¯€é»ã€‚\nDrain the Node ä½¿ç”¨ kubectl drain å°‡ Node ç‹€æ…‹è®Šæ›´ç‚ºç¶­è­·æ¨¡å¼ï¼Œè©² Node ä¸Šé¢çš„ Pod å°±æœƒè½‰ç§»åˆ°å…¶ä»– Node ä¸Šã€‚\n1 kubectl drain worker3 --ignore-daemonsets --delete-local-data kubectl drain æ“ä½œæœƒå°‡æŒ‡å®šç¯€é»ä¸Šçš„ Pod åˆªé™¤ï¼Œä¸¦åœ¨å¯èª¿åº¦ç¯€é»ä¸Šé¢èµ·ä¸€å€‹å°æ‡‰çš„ Podã€‚ç•¶èˆŠ Podæ²’æœ‰è¢«æ­£å¸¸åˆªé™¤çš„æƒ…æ³ä¸‹ï¼Œæ–° Pod ä¸æœƒèµ·ä¾†ã€‚ä¾‹å¦‚èˆŠ Pod ä¸€ç›´è™•æ–¼Terminating ç‹€æ…‹ã€‚æ‰€ä»¥å¯ä»¥å¼·åˆ¶åˆªé™¤è©² Podã€‚\n1 2 3 kubectl get pod all -A -o wide | grep worker3 kubectl delete pods -n my-kafka-project my-cluster-zookeeper-2 --force Delete the Node 1 kubectl delete nodes worker3 delete æ˜¯ä¸€ç¨®æš´åŠ›åˆªé™¤ node çš„æ–¹å¼ï¼Œæœƒå¼·åˆ¶é—œé–‰å®¹å™¨é€²ç¨‹ä»¥é©…é€ podã€‚ åŸºäº node çš„è‡ªè¨»å†ŠåŠŸèƒ½ï¼Œæ¢å¾©èª¿åº¦å‰‡é‡å•Ÿ kubelet æœå‹™å³å¯ã€‚\n1 systemctl restart kubelet Drain v.s Cordon cordon åœæ­¢èª¿åº¦ï¼ˆä¸å¯èª¿åº¦ï¼Œå¾ K8S é›†ç¾¤éš”é›¢ï¼‰ åªæœƒå°‡ node æ¨™è­˜ç‚ºSchedulingDisabled ä¸å¯èª¿åº¦ç‹€æ…‹ã€‚æ–°å‰µå»ºçš„è³‡æºï¼Œä¸æœƒè¢«èª¿åº¦åˆ°è©²ç¯€é»ã€‚è€ŒèˆŠæœ‰çš„ pod ä¸æœƒå—åˆ°å½±éŸ¿ï¼Œä»æ­£å¸¸å°å¤–æä¾›æœå‹™ã€‚\n1 2 3 4 # ç¦æ­¢èª¿åº¦ kubectl cordon \u0026lt;nodeName\u0026gt; # æ¢å¾©èª¿åº¦ kubectl uncordon \u0026lt;nodeName\u0026gt; drain é©…é€ç¯€é»ï¼ˆå…ˆä¸å¯èª¿åº¦ï¼Œç„¶å¾Œæ’ä¹¾ Podï¼‰ æœƒé©…é€ Node ä¸Šçš„ pod è³‡æºåˆ°å…¶ä»–ç¯€é»é‡æ–°å‰µå»ºã€‚æ¥è‘—ï¼Œå°‡ç¯€é»èª¿ç‚º SchedulingDisabled ä¸å¯èª¿åº¦ç‹€æ…‹ã€‚\n1 2 3 4 # ç¦æ­¢èª¿åº¦ kubectl drain \u0026lt;nodeName\u0026gt; --force --ignore-daemonsets --delete-local-data # æ¢å¾©èª¿åº¦ kubectl uncordon \u0026lt;nodeName\u0026gt; --force ç•¶ä¸€äº› pod ä¸æ˜¯ç¶“ReplicationController, ReplicaSet, Job, DaemonSet æˆ–è€… StatefulSet ç®¡ç†çš„æ™‚å€™å°±éœ€è¦ç”¨ \u0026ndash;force ä¾†å¼·åˆ¶åŸ·è¡Œ(ä¾‹å¦‚ kube-proxy) --ignore-daemonsets ç„¡è¦–DaemonSet ç®¡ç†ä¸‹çš„ Podã€‚å› ç‚ºdeamonset æœƒå¿½ç•¥ unschedulable æ¨™ç±¤ï¼Œå› æ­¤ deamonset æ§åˆ¶å™¨æ§åˆ¶çš„ pod è¢«åˆªé™¤å¾Œå¯èƒ½é¦¬ä¸Šåˆåœ¨æ­¤ç¯€é»ä¸Šå•Ÿå‹•èµ·ä¾†ï¼Œé€™æ¨£å°±æœƒæˆç‚ºæ­»å¾ªç’°ï¼Œå› æ­¤é€™è£¡å¿½ç•¥daemonsetã€‚ --delete-local-data å¦‚æœæœ‰ mount local volumn çš„ podï¼Œæœƒå¼·åˆ¶æ®ºæ‰è©² podã€‚ drain é©…é€æµç¨‹ï¼šå…ˆåœ¨ Node ç¯€é»å„ªé›…é—œé–‰ä¸¦åˆªé™¤ podï¼Œç„¶å¾Œå†åœ¨å…¶ä»– Node ç¯€é»å‰µå»ºè©² podã€‚æ‰€ä»¥ç‚ºäº†ç¢ºä¿ drain é©…é€ pod éç¨‹ä¸­ä¸ä¸­æ–·æœå‹™ï¼Œå¿…é ˆä¿è­‰è¦é©…é€çš„ pod å‰¯æœ¬æ•¸å¤§æ–¼ 1ï¼Œä¸¦ä¸”æ¡ç”¨äº† anti-affinity å°‡é€™äº› pod èª¿åº¦åˆ°ä¸åŒçš„ Node ç¯€é»ä¸Šã€‚ Reference https://www.cnblogs.com/kevingrace/p/14412254.html ","date":"2022-05-19T11:36:00Z","permalink":"http://localhost:1313/p/kubernetes-delete-worker-node/","title":"[Kubernetes] åœæ­¢èª¿åº¦ / åˆªé™¤ç¯€é»"},{"content":"æœ¬ç¯‡æ–‡ç« è¨˜éŒ„æ€éº¼ä½¿ç”¨ cert-manager ç‚ºå°å¤–çš„ istio gateway åŠ ä¸Š httpsã€‚\næ†‘è­‰åˆ†é¡ è‡ªç°½æ†‘è­‰ï¼šæŸäº›ä¸éœ€è¦è¢«å…¬é–‹å­˜å–ã€ä½†å¸Œæœ›é”åˆ°è³‡æ–™å‚³è¼¸èƒ½åŠ å¯†çš„å…§éƒ¨æœå‹™ï¼Œå¯ä»¥ä½¿ç”¨è‡ªç°½æ†‘è­‰ï¼ŒClient å»å­˜å–çš„æ™‚å€™è‡ªå·±å¸¶ä¸Š CA æ†‘è­‰å»é©—è­‰å³å¯ï¼Œä¾‹å¦‚ HashiCorp Vault, AWS RDS TLS é€£ç·š\u0026hellip;ç­‰ã€‚\nç¬¬ä¸‰æ–¹ CA æ©Ÿæ§‹ç°½ç™¼æ†‘è­‰ï¼šå¦‚æœæ˜¯å…¬é–‹çš„ç¶²è·¯æœå‹™ï¼Œå°±å¿…é ˆé€éæ­£è¦çš„ CA æ©Ÿæ§‹ä¾†ç°½ç™¼ï¼Œå¦‚éœ€è¦æ”¶è²»çš„ Digicert, SSL.com, Symantec\u0026hellip;ç­‰ï¼Œæˆ–æ˜¯å…è²»çš„ Letâ€™s Encryptã€‚\ncert-manager cert-manager æ˜¯åŸºæ–¼ Kubernetes æ‰€é–‹ç™¼çš„æ†‘è­‰ç®¡ç†å·¥å…·ï¼Œå®ƒå¯ä»¥å¯ä»¥å¹«å¿™ç™¼å‡ºä¾†è‡ªå„å®¶çš„ TLS æ†‘è­‰ï¼Œä¾‹å¦‚ä¸Šé¢æ‰€æåˆ°çš„ ACME (Letâ€™s Encrypt), HashiCorp Vault, Venafi æˆ–æ˜¯è‡ªå·±ç°½ç™¼çš„æ†‘è­‰ï¼Œè€Œä¸”å®ƒé‚„å¯ä»¥ç¢ºä¿ TLS æ†‘è­‰ä¸€ç›´ç¶­æŒåœ¨æœ‰æ•ˆæœŸé™å…§ã€‚\nAbove Reference\nInstall 1 2 3 4 5 6 $ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.4.0/cert-manager.yaml $ kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c6866597-zw7kh 1/1 Running 0 2m cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m Issuer Issuer ç”¨ä¾†é ’ç™¼æ†‘è­‰ï¼Œåˆ†ç‚ºå…©ç¨®è³‡æºé¡å‹ï¼š\nissuerï¼šåªä½œç”¨æ–¼ç‰¹å®š namespace ClusterIssuerï¼šä½œç”¨æ–¼æ•´å€‹ k8s é›†ç¾¤ cert-manager æœ‰æ”¯æ´å¹¾ç¨®çš„ issuer typeï¼š\nCA: ä½¿ç”¨ x509 keypair ç”¢ç”Ÿ certificateï¼Œå­˜åœ¨ kubernetes secret Self Signed: è‡ªç°½ certificate ACME: å¾ ACME (ex. Let\u0026rsquo;s Encrypt) server å–å¾— ceritificate Vault: å¾ Vault PKI backend é ’ç™¼ certificate Venafi: Venafi Cloud Above Refenrence\nCreate Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: # å‰µå»ºçš„ç°½ç™¼æ©Ÿæ§‹çš„åç¨±ï¼Œå¾Œé¢å‰µå»ºè­‰æ›¸çš„æ™‚å€™æœƒå¼•ç”¨ name: letsencrypt-prod spec: acme: # è­‰æ›¸å¿«éæœŸçš„æ™‚å€™æœƒæœ‰éƒµä»¶æé†’ï¼Œä¸é cert-manager æœƒåˆ©ç”¨ acme å”è­°è‡ªå‹•çµ¦æˆ‘å€‘é‡æ–°é ’ç™¼è­‰æ›¸ä¾†çºŒæœŸ email: ulahsieh@nexaiot.com privateKeySecretRef: # Name of a secret used to store the ACME account private key æŒ‡ç¤ºæ­¤ç°½ç™¼æ©Ÿæ§‹çš„ç§é‘°å°‡è¦å­˜å„²åˆ°å“ªå€‹ Secret ä¸­ name: letsencrypt-prod # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory solvers: # æŒ‡ç¤ºç°½ç™¼æ©Ÿæ§‹ä½¿ç”¨ HTTP-01 çš„æ–¹å¼é€²è¡Œ acme å”è­°(é‚„å¯ä»¥ç”¨ DNS æ–¹å¼ï¼Œacme å”è­°çš„ç›®çš„æ˜¯è­‰æ˜é€™å°æ©Ÿå™¨å’ŒåŸŸåéƒ½æ˜¯å±¬æ–¼ä½ çš„ï¼Œç„¶å¾Œæ‰å‡†è¨±çµ¦ä½ é ’ç™¼è­‰æ›¸) - http01: ingress: class: istio å‚™è¨»ï¼š\nletsencrypt å¯ä»¥çœ‹åˆ°ç¶²è·¯ä¸Šæœ‰å…©å€‹ä¸åŒåå­—çš„è¨­å®šï¼Œä¸€å€‹æ˜¯ letsencrypt-staging ç”¨æ–¼æ¸¬è©¦ï¼Œä¸€å€‹æ˜¯ letsencrypt-prod ç”¨æ–¼ production. 1 2 $ kubectl apply -f clusterIssuer.yaml $ kubectl describe clusterissuers.cert-manager.io åšå®Œçš„æ™‚å€™ç™¼ç”Ÿäº† server misbehaving éŒ¯èª¤\n1 2 3 4 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ErrInitIssuer 8m2s (x3 over 8m7s) cert-manager Error initializing issuer: Get \u0026#34;https://acme-v02.api.letsencrypt.org/directory\u0026#34;: dial tcp: lookup acme-v02. api.letsencrypt.org on 10.96.0.10:53: server misbehaving æŸ¥äº†ä¸€ä¸‹ç™¼ç¾æ˜¯ dns è§£æçš„å•é¡Œï¼Œä¾¿å»è‡ªå»ºçš„ dns server æ”¹ /etc/named.conf æª”ï¼Œç™¼ç¾åœ¨ option ä¸­å°‘åŠ äº† forwarders æ¬„ä½ã€‚\nforwarders æ˜¯æŒ‡ç•¶æœ¬ DNS è§£æä¸äº†çš„åŸŸåï¼Œè¦è½‰çµ¦èª°ä¾†è§£æçš„æ„æ€ï¼Œé€šå¸¸è½‰çµ¦å†ä¸Šä¸€å±¤ï¼Œä¹Ÿå°±æ˜¯å¤–ç¶²æœ¬èº«çš„ DNSï¼Œç°¡å–®ä¾†èªªå¯ç›´æ¥ä½¿ç”¨ 8.8.8.8ï¼Œä¸¦æ·»åŠ  allow-query any;ï¼Œè®“é›†ç¾¤å…§çš„ç¶²æ®µéƒ½èƒ½ä¾†ä½¿ç”¨ã€‚\næ”¹å¥½ä¹‹å¾Œå›åˆ° k8s é›†ç¾¤å†æ¬¡æŸ¥çœ‹ clusterissuer æ˜¯å¦å¯ä»¥å»ºæˆ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 [root@k8sm1 cert-manager]# kubectl describe clusterissuers.cert-manager.io Name: letsencrypt-prod Namespace: Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: cert-manager.io/v1 Kind: ClusterIssuer Metadata: Creation Timestamp: 2021-06-21T12:12:47Z Generation: 1 Resource Version: 104200852 Self Link: /apis/cert-manager.io/v1/clusterissuers/letsencrypt-prod UID: f0e9ecc6-9a50-491c-af78-b88670885e18 Spec: Acme: Email: ulahsieh@nexaiot.com Preferred Chain: Private Key Secret Ref: Name: letsencrypt-prod Server: https://acme-v02.api.letsencrypt.org/directory Solvers: http01: Ingress: Class: istio Status: Acme: Last Registered Email: ulahsieh@nexaiot.com Uri: https://acme-v02.api.letsencrypt.org/acme/acct/127768449 Conditions: Last Transition Time: 2021-06-21T12:13:35Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u0026lt;none\u0026gt; Create Certificate é€é Issuer ç”³è«‹ Certificate æ†‘è­‰\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # cat istio-cert.yaml apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: istio-cert namespace: istio-system spec: commonName: convergence.nexmasa.com dnsNames: - convergence.nexmasa.com secretName: istio-cert issuerRef: name: letsencrypt-prod kind: ClusterIssuer èªªæ˜ï¼š\nspec.secretName æŒ‡ç¤ºè­‰æ›¸æœ€çµ‚å­˜åˆ°å“ªå€‹ Secret ä¸­ spec.issuerRef.kind å€¼ç‚º ClusterIssuer èªªæ˜ç°½ç™¼æ©Ÿæ§‹ä¸åœ¨æœ¬namespace ä¸‹ï¼Œè€Œæ˜¯åœ¨å…¨å±€ spec.issuerRef.name æˆ‘å€‘å‰µå»ºçš„ç°½ç™¼æ©Ÿæ§‹çš„ Issuer åç¨± spec.dnsNames æŒ‡ç¤ºè©²è­‰æ›¸çš„å¯ä»¥ç”¨æ–¼å“ªäº›åŸŸå è¸©å‘å›‰ éƒ¨å±¬ certificate å¾Œä¸€ç›´å¡åœ¨è·Ÿ letsencrypt issueing é€™å¡Šï¼ŒéŒ¯èª¤è¨Šæ¯æ˜¯:\n1 2 3 4 5 6 $ kubectl describe certificate -n istio-system istio-cert $ kubectl get event -n istio-system 30s Normal Issuing certificate/istio-cert Issuing certificate as Secret does not exist 29s Normal Generated certificate/istio-cert Stored new private key in temporary Secret resource \u0026#34;istio-cert-hrrgb\u0026#34; 29s Normal Requested certificate/istio-cert Created new CertificateRequest resource \u0026#34;istio-cert-89tj8\u0026#34; 3s Warning Failed certificate/istio-cert The certificate request has failed to complete and will be retried: Failed to wait for order resource \u0026#34;istio- cert-89tj8-2838533447\u0026#34; to become ready: order is in \u0026#34;invalid\u0026#34; state: è©¦äº†å¾ˆä¹…ï¼Œç™¼ç¾å®˜ç¶²æœ‰å¯« debug éç¨‹ï¼Œæ‰ç™¼ç¾è¦å»çœ‹ challenge çš„ log\nhttps://cert-manager.io/docs/faq/acme/\n1 2 3 4 5 6 7 8 $ kubectl describe challenges.acme.cert-manager.io -n istio-system istio-cert-22gl9-2838533447-3373762545 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Started 32m cert-manager Challenge scheduled for processing Normal Presented 32m cert-manager Presented challenge using HTTP-01 challenge mechanism Warning Failed 32m cert-manager Accepting challenge authorization failed: acme: authorization error for convergence.nexmasa.com: 400 urn:ietf:params:acme:error: dns: DNS problem: NXDOMAIN looking up A for convergence.nexmasa.com - check that a DNS record exists for this domain ç„¶å¾Œæ‰¾äº†ä¸€ä¸‹è§£ç­”ç™¼ç¾ï¼Œletsencrypt åªé©åˆç”¨åœ¨ç¶²éš›ç¶²è·¯å­˜å–åˆ°çš„ DNS å•Š \u0026hellip; = =\nhttps://github.com/jetstack/cert-manager/issues/3543\nThis error comes from Let\u0026rsquo;s Encrypt which cannot reach your domain (it tries to as .int is a public known TLD). In order for Let\u0026rsquo;s Encrypt to work they need to have public access to verify the ownership of your domain. For internal only domains you might want to look into using an internal CA.\nhttps://serverfault.com/questions/1048678/check-that-a-dns-record-exists-for-this-domain\ndomain names that are in the global DNS tree\nä¸éé€™é‚Šé‚„æ˜¯å¯ä»¥è¨˜éŒ„å¹¾ç¯‡å¯ä»¥åƒè€ƒä½¿ç”¨ letsencrypt çš„æ–‡ç« \nhttps://www.qikqiak.com/k8strain/istio/cert-manager/ https://medium.com/intelligentmachines/istio-https-traffic-secure-your-service-mesh-using-ssl-certificate-ac20ec2b6cd6 æ”¹å»ºè‡ªç°½æ†‘è­‰ å»ºç«‹ Issuer 1 2 3 4 5 6 7 8 9 kubectl apply -f \u0026lt;(echo \u0026#34; apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: istio-system spec: selfSigned: {} \u0026#34;) å»ºç«‹ Certificate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f \u0026lt;(echo \u0026#39; apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-crt namespace: istio-system spec: secretName: tls-secret duration: 175200h renewBefore: 12h issuerRef: kind: Issuer name: selfsigned commonName: \u0026#34;convergence.nexmasa.com\u0026#34; isCA: true dnsNames: - \u0026#34;convergence.nexmasa.com\u0026#34; \u0026#39;) æŸ¥çœ‹æ†‘è­‰åŠé‡‘é‘°çš„æœ‰æ•ˆæ€§ 1 kubectl get secrets/tls-secret -n istio-system -o \u0026#34;jsonpath={.data[\u0026#39;tls\\.crt\u0026#39;]}\u0026#34; | base64 -d | openssl x509 -text -noout 1 kubectl get secrets/tls-secret -n istio-system -o \u0026#34;jsonpath={.data[\u0026#39;tls\\.key\u0026#39;]}\u0026#34; | base64 -D | openssl rsa -check ç‚ºæœå‹™åŠ ä¸Šæ†‘è­‰ ä¿®æ”¹ istio gatewayï¼ŒåŠ ä¸Š https çš„ protocol ä¸¦æŒ‡å®šä¸Šé¢å»ºç«‹çš„ Secret Nameã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: converg-api-gw namespace: converg-api spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: Port-80-M55Sa number: 80 protocol: HTTP - hosts: - \u0026#39;*\u0026#39; port: name: https number: 443 protocol: HTTPS tls: credentialName: tls-secret mode: SIMPLE ","date":"2022-05-19T09:11:00Z","permalink":"http://localhost:1313/p/add-https-to-istio-gw/","title":"ç‚ºå°å¤–çš„ istio gateway åŠ ä¸Š https"},{"content":"ç’°å¢ƒèªªæ˜ Kubernetes 1.20.10 Calico 3.23 éŒ¯èª¤æè¿° éƒ¨ç½² calico ç¶²çµ¡å¾Œç‹€æ…‹é›–ç‚º Runningï¼Œä½† container éƒ½ç„¡æ³•æˆåŠŸé‹ä½œï¼Œcalico-controller ä»¥åŠæ”¾åœ¨æ¯å€‹ç¯€é»çš„ calico-node çš†ç„¡æ³•åˆå§‹åŒ–ï¼Œå¦‚ä¸‹åœ–\ncalico-kube-controllers æ—¥èªŒ 1 2 3 4 2022-05-17 01:11:09.503 [FATAL][1] main.go 120: Failed to initialize Calico datastore error=Get \u0026#34;https://10.254.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\u0026#34;: context deadline exceeded 2022-05-17 01:11:09.962 [INFO][1] main.go 94: Loaded configuration from environment config=\u0026amp;config.Config{LogLevel:\u0026#34;info\u0026#34;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:\u0026#34;\u0026#34;, DatastoreType:\u0026#34;kubernetes\u0026#34;} W0822 01:11:09.963646 1 client_config.go:615] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. 2022-05-17 01:11:09.964 [INFO][1] main.go 115: Ensuring Calico datastore is initialized calico-node æ—¥èªŒ 1 2 3 4 5 2022-05-17 01:11:14.102 [WARNING][69] felix/health.go 211: Reporter is not ready. name=\u0026#34;async_calc_graph\u0026#34; 2022-05-17 01:11:14.102 [WARNING][69] felix/health.go 211: Reporter is not ready. name=\u0026#34;int_dataplane\u0026#34; 2022-05-17 01:11:14.103 [WARNING][69] felix/health.go 173: Health: not ready 2022-05-17 01:11:14.540 [INFO][69] felix/watchercache.go 180: Full resync is required ListRoot=\u0026#34;/calico/resources/v3/projectcalico.org/kubernetesendpointslices\u0026#34; 2022-05-17 01:11:14.544 [INFO][69] felix/watchercache.go 193: Failed to perform list of current data during resync ListRoot=\u0026#34;/calico/resources/v3/projectcalico.org/kubernetesendpointslices\u0026#34; error=resource does not exist: KubernetesEndpointSlice with error: the server could not find the requested resource åŸå› åŠè§£æ³• åŸå› æ˜¯å› ç‚ºå®‰è£çš„ Calico ç‰ˆæœ¬ä¸ç›¸å®¹æ–¼ Kubernetesï¼Œä¾æ“š Calico å®˜ç¶² çš„æè¿°ï¼Œ3.23 ç‰ˆæœ€ä½æ”¯æ´çš„ k8s ç‰ˆæœ¬ç‚º 1.21ï¼Œæ•…ç™¼ç”Ÿè©²éŒ¯èª¤ã€‚ è§£æ±ºæ–¹æ³•æ˜¯å°‡ Calico é‡æ–°éƒ¨ç½²ç‚º 3.21 ç‰ˆï¼Œä»¥ç›¸å®¹æ–¼ k8s 1.20 çš„ç’°å¢ƒ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 curl -s https://docs.projectcalico.org/manifests/calico.yaml | kubectl delete -f - configmap \u0026#34;calico-config\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;bgpconfigurations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;bgppeers.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;blockaffinities.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;caliconodestatuses.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;clusterinformations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;felixconfigurations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;globalnetworkpolicies.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;globalnetworksets.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;hostendpoints.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamblocks.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamconfigs.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamhandles.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ippools.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipreservations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;kubecontrollersconfigurations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;networkpolicies.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;networksets.crd.projectcalico.org\u0026#34; deleted clusterrole.rbac.authorization.k8s.io \u0026#34;calico-kube-controllers\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;calico-kube-controllers\u0026#34; deleted clusterrole.rbac.authorization.k8s.io \u0026#34;calico-node\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;calico-node\u0026#34; deleted daemonset.apps \u0026#34;calico-node\u0026#34; deleted serviceaccount \u0026#34;calico-node\u0026#34; deleted deployment.apps \u0026#34;calico-kube-controllers\u0026#34; deleted serviceaccount \u0026#34;calico-kube-controllers\u0026#34; deleted error: unable to recognize \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;PodDisruptionBudget\u0026#34; in version \u0026#34;policy/v1\u0026#34; é‡æ–°éƒ¨ç½²\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 curl -s https://docs.projectcalico.org/v3.21/manifests/calico.yaml | kubectl apply -f - configmap/calico-config created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created éƒ¨ç½²ä¾ç…§ç’°å¢ƒé…ç½®å¤§æ¦‚éœ€è¦ç­‰å¹¾åˆ†é˜ä¸ç­‰ï¼Œéç¨‹ä¸­å¯èƒ½æœƒé€ æˆå…¶ä»– Pod è¢«é‡å•Ÿï¼Œè§€å¯Ÿä¸‹ä¾†æ˜¯æ­£å¸¸çš„æƒ…æ³\nå…¨éƒ¨å®Œæˆå¾Œå°±å¯ä»¥çœ‹åˆ° pod çš†æ­£å¸¸é‹ä½œäº†\nReference https://projectcalico.docs.tigera.io/archive/v3.21/getting-started/kubernetes/requirements https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/633 ","date":"2022-05-19T08:53:00Z","permalink":"http://localhost:1313/p/calico-running-but-unready/","title":"Calico Running but Unready (Ready 0/1)"},{"content":"åœ¨ kubernetes ç’°å¢ƒä¸Šæ‹‰å–ç§æœ‰é¡åƒå€‰åº« harbor çš„ image æ™‚ï¼Œä¸€ç›´å¡åœ¨ ImagePullBackOff çš„ç‹€æ…‹ï¼Œdecribe pod ç™¼ç¾æ˜¯æ¬Šé™å•é¡Œå°è‡´æ‹‰å–å¤±æ•—ã€‚\nç‹€æ³èªªæ˜ éŒ¯èª¤è¨Šæ¯å¦‚ä¸‹\n1 Failed to pull image \u0026#34;10.1.5.142:4433/test/findkpsn:9d2e44d2\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: unauthorized: unauthorized to access repository: test/findkpsn, action: pull: unauthorized to access repository: test/findkpsn, action: pull ç„¶è€Œå¯¦éš›ä¸Šåœ¨æœ¬æ©Ÿä¸Šå·²ç¶“ docker login æˆåŠŸéäº†ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ docker pull æ‹‰å–ï¼Œä½†é€é k8s æ‹‰å–ä»æœƒå¤±æ•—ã€‚\ndebug æ€è·¯ ç¢ºèªåœ¨åŒæ¨£ repo çš„ project ä¸‹çš„å…¶ä»– image æ˜¯å¦ä¹Ÿç™¼ç”ŸåŒæ¨£çš„æƒ…æ³ æ˜¯ï¼ŒåŒæ¨£ repo çš„ project çš„å…¶ä»– image ä¹Ÿç›¸åŒã€‚ ç¢ºèªåœ¨ä¸åŒçš„ repo æ˜¯å¦ä¹Ÿç™¼ç”ŸåŒæ¨£çš„æƒ…æ³ å¦ï¼Œå…¶ä»– repo èƒ½æ­£å¸¸å¤ é kubectl æ‹‰å–ï¼Œæ‡‰èƒ½æ¨æ–·éƒ¨å±¬ç’°å¢ƒä¸Šæ²’å•é¡Œã€‚ åŸå›  çµæœæ˜¯å› ç‚ºæ²’æœ‰æŠŠ project å…¬é–‹ =__=\næ„å¤–ç™¼ç¾ ä»ç„¶é‚„æ˜¯å¯ä»¥è®“ project ç¶­æŒåœ¨ç§æœ‰çš„ç‹€æ³ä¸‹ï¼Œé€é kubectl æ‹‰å–ã€‚åªè¦åœ¨å®šç¾©è³‡æºæ™‚ï¼ŒåŠ ä¸Š imagePullSecrets çš„å±¬æ€§ï¼Œå€¼æŒ‡å®šç‚ºæ¬²å‰µå»ºè³‡æºçš„ namespace ä¸‹çš„ kubernetes.io/dockerconfigjson çš„ secretï¼Œå³å¯æ‹‰å–æˆåŠŸã€‚\nå‰µå»º docker-registry secret 1 2 3 4 kubectl create secret docker-registry \u0026lt;secretName\u0026gt; \\ --docker-server=DOCKER_REGISTRY_SERVER \\ --docker-username=DOCKER_USER \\ --docker-password=DOCKER_PASSWORD -n \u0026lt;NAMESPACE\u0026gt; deployment éƒ¨å±¬æª” åœ¨ spec.template.spec ä¸‹æ–°å¢ imagePullSecrets\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: Deployment metadata: name: test namespace: test labels: app: test spec: replicas: 1 selector: matchLabels: app: test template: metadata: labels: app: test spec: containers: - name: test image: 10.1.5.142:4433/test/test/findkpsn:9d2e44d2 imagePullSecrets: - name: harbor é‡æ–°ä½ˆç½² åŠ ä¸Š imagePullSecrets å¾Œï¼Œå°±å¯ä»¥æˆåŠŸæ‹‰å–ç§æœ‰å°ˆæ¡ˆçš„é¡åƒäº†!\nReference https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod ","date":"2022-04-14T19:33:00Z","permalink":"http://localhost:1313/p/image-pull-backoff-after-harbor-login/","title":"å·²ç™»å…¥ harbor ä½† kubelet ä»æœƒ ImagePullBackOff"},{"content":"ç›®å‰æ‰‹ä¸Šæœ‰ä¸€å€‹ç›£è½ Oracle CDC çš„ç¨‹å¼è·‘åœ¨ä»¥ Debian ç‚ºåŸºåº•çš„ kubernetes pod ä¸­ï¼Œæœƒå®šæœŸå› ç‚º Oracle çš„éŒ¯èª¤è¨Šæ¯ ORA-12518: TNS ç›£è½ç¨‹å¼ç„¡æ³•åˆ†ç™¼å®¢æˆ¶æ©Ÿé€£ç·šçš„å•é¡Œè€Œæ–·ç·šã€‚æ­¤æ™‚é›–ç„¶ç¨‹å¼æœ‰ error logï¼Œä½† Pod çš„ç‹€æ…‹ä»ç„¶ç‚º Runningï¼Œåªè¦é‡å•Ÿ Pod å³å¯é‡æ–°æ­£å¸¸é‹ä½œã€‚\nORA-12518 é¦–å…ˆé †ä¾¿è§£é‡‹æ­¤éŒ¯èª¤çš„åŸå›  The process of handing off a client connection to another process failed. åƒè€ƒç¶²è·¯ä¸Šå…¶ä»–åˆ†äº«ï¼š\nstackoverflow ittutorial cnblogs å¾æ ¹æœ¬å¯ä»¥è§£æ±ºçš„æ–¹å¼å¦‚ä¸‹ï¼š\nEdit /etc/systemd/system.conf file and Set DefaultTasksMax to â€˜infinityâ€™. dedicated server: ä¿®æ”¹ oracle processes \u0026amp; sessions parameters shared server: ä¿®æ”¹ oracle dispatcher parameters ç„¶è€Œ å› ç‚º IT server ä¸¦ä¸åœ¨æˆ‘æ§ç®¡çš„ç¯„åœï¼Œæ‰€ä»¥åªèƒ½è‡ªå·±æ‰‹å‹•é‡å•Ÿ Podã€‚åŸæœ¬æ˜¯æƒ³èªªå¯« cronJob å®šæœŸé‡å•Ÿ podï¼Œä½†åœ¨æ‰¾è³‡æ–™çš„éç¨‹ä¸­ï¼Œç™¼ç¾åœ¨ stackoverflow crobjob å•é¡Œ çš„è§£æ³•ä¸­æœ‰äººæå‡ºäº†ç›´æ¥ä½¿ç”¨ livenessprobe è§£æ±ºã€‚\nå¾è¨­å®š livenessProbe è§£æ±º Kubelet ä½¿ç”¨ liveness probeï¼ˆå­˜æ´»æ¢é‡ï¼‰ä¾†ç¢ºå®šä½•æ™‚é‡å•Ÿå®¹å™¨ã€‚ç•¶æ‡‰ç”¨ç¨‹åºè™•æ–¼é‹è¡Œç‹€æ…‹ä½†ç„¡æ³•åšé€²ä¸€æ­¥æ“ä½œï¼Œliveness æ¢é‡å°‡æ•ç²åˆ° deadlockï¼Œé‡å•Ÿè™•æ–¼è©²ç‹€æ…‹ä¸‹çš„å®¹å™¨ï¼Œä½¿æ‡‰ç”¨ç¨‹åºåœ¨å­˜åœ¨ bug çš„æƒ…æ³ä¸‹ä¾ç„¶èƒ½å¤ ç¹¼çºŒé‹è¡Œä¸‹å»ã€‚\nexec.Commandï¼šè¦åœ¨å®¹å™¨å…§åŸ·è¡Œçš„æª¢æ¸¬å‘½ä»¤ï¼Œå¦‚æœå‘½ä»¤åŸ·è¡ŒæˆåŠŸï¼Œå°‡è¿”å› 0ï¼Œkubelet å°±æœƒèªç‚ºè©²å®¹å™¨æ˜¯æ´»è‘—çš„ä¸¦ä¸”å¾ˆå¥åº·ã€‚å¦‚æœè¿”å›é 0 å€¼ï¼Œkubelet å°±æœƒæ®ºæ‰é€™å€‹å®¹å™¨ä¸¦é‡å•Ÿå®ƒã€‚ periodSecondsï¼šliveness probe å¤šä¹…æª¢æŸ¥ä¸€æ¬¡ initialDelaySecondsï¼šé¦–æ¬¡å•Ÿå‹• pod å¾Œï¼Œè¦å»¶é²å¤šä¹…å¾ŒåŸ·è¡Œ liveness probe probe command è¦å¯«å•¥? æ¥ä¸‹ä¾†åˆå¦ä¸€å€‹å•é¡Œä¾†äº†ï¼Œæˆ‘çš„ probe ä¸­çš„æª¢æ¸¬å‘½ä»¤è¦å¯«å•¥? å› ç‚ºåœ¨æ‰‹å‹•é‡å•Ÿæ™‚ï¼Œåªèƒ½å¾ kubectl logs ç‚ºä¾æ“šï¼ŒæŸ¥çœ‹æœ‰ç„¡éŒ¯èª¤è¨Šæ¯ã€‚ç„¶è€Œç¾åœ¨ command è¦åŸ·è¡Œåœ¨å®¹å™¨ä¸­ï¼Œä½†å®¹å™¨è£¡é¢æ²’è¾¦æ³•ç›´æ¥ä½¿ç”¨ kubectl å–å¾—æ‡‰ç”¨çš„ stdout çš„è¨Šæ¯ã€‚åˆå»å †ç–Šæº¢ä½(XD æ‰¾åˆ°äº†å…©ç¨®è§£æ±ºæ–¹æ³•ã€‚\nåœ¨å®¹å™¨è£¡ curl Kubernete API server è¨­å®š pod é€£ kubernetes api server è«‹åƒè€ƒå¦å¤–ä¸€ç¯‡æ–‡ç« è¨˜éŒ„ã€‚ command æ‡‰è©²å°±æœƒé•·æˆä»¥ä¸‹ï¼Œå¦‚æœ curl å›åˆ°çš„ output æœƒ grep åˆ° error è¨Šæ¯ï¼Œå‰‡é‡å•Ÿã€‚\n1 2 3 4 5 6 7 8 livenessProbe: exec: command: - bash - -c - \u0026#34;curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; -X GET https://kubernetes.default.svc/api/v1/namespaces/converg-it/pods/converg-it-adapter-oracle-8575f54dc6-7lnv6/log?sinceSeconds=100 | grep \u0026#39;error\u0026#39;\u0026#34; initialDelaySeconds: 120 periodSeconds: 60 åœ¨å®¹å™¨è£¡æŸ¥çœ‹ç›£è½çš„ port åœ¨ç›®å‰è·‘çš„å®¹å™¨ä¸­ï¼Œä½¿ç”¨ ss æŸ¥çœ‹ç›®å‰ç³»çµ±çš„ socket ç‹€æ…‹ï¼Œå¯ä»¥ç™¼ç¾åˆ°å…¶å¯¦åœ¨æ­£å¸¸é€£çµçš„æƒ…æ³ä¸‹èƒ½åµæ¸¬åˆ°é€£ç·š(establish state) oracle server çš„ç›£è½ã€‚\né‚£éº¼ç•¶ç™¼ç”Ÿé€£ç·šç•°å¸¸æ™‚(ORA-12518)ï¼Œå°±å¯ä»¥ç•¶ä½œæ˜¯é‡å•Ÿçš„æ¢ä»¶ã€‚\n1 2 3 4 5 6 7 8 livenessProbe: exec: command: - bash - -c - \u0026#34;ss -an | grep -q \u0026#39;EST.*:1521 *$\u0026#39;\u0026#34; initialDelaySeconds: 120 periodSeconds: 60 Reference https://jimmysong.io/kubernetes-handbook/guide/configure-pod-service-account.html https://stackoverflow.com/questions/49000280/monitor-and-take-action-based-on-pod-log-event https://stackoverflow.com/questions/57711963/kubernetes-liveness-probe-can-a-pod-monitor-its-own-stdout ","date":"2022-03-31T22:52:15Z","permalink":"http://localhost:1313/p/setup-liveness-probe-to-restart-when-pod-has-error-log/","title":"è¨­å®š liveness probe ç›£è½æ‡‰ç”¨ä»¥é‡å•Ÿ pod"},{"content":"é€£æ¥ k8s çš„ api-server æœ‰ä¸‰ç¨®æ–¹å¼ï¼š\nKubernetes Node é€šé kubectl proxy ä¸­è½‰é€£æ¥ é€šéæˆæ¬Šé©—è­‰ç›´æ¥é€£æ¥ï¼Œä¾‹å¦‚ kubectl å’Œå„ç¨® client å°±æ˜¯é€™ç¨®æƒ…æ³ å®¹å™¨å…§éƒ¨é€šé ServiceAccount é€£æ¥ æœ¬æ–‡ä»¥ç¬¬ä¸‰ç¨®æƒ…æ³ä½œç¯„ä¾‹ã€‚\nKubernetes API Server åœ¨ Kubernetes é›†ç¾¤è¢«å‰µå»ºæ™‚ï¼Œé è¨­æœƒåœ¨ default namespace ä¸­å‰µå»º kubernetes çš„æœå‹™ï¼Œç”¨æ–¼è¨ªå• Kubernetes apiserverã€‚å› æ­¤ï¼ŒPod ä¹‹é–“å¯ä»¥ç›´æ¥ä½¿ç”¨ kubernetes.default.svc ä¸»æ©Ÿåä¾†æŸ¥è©¢ API serverã€‚\nService Account ServiceAccount æ˜¯çµ¦åŸ·è¡Œåœ¨ Pod çš„ç¨‹å¼ä½¿ç”¨çš„èº«ä»½èªè­‰ï¼Œçµ¦ Pod å®¹å™¨çš„ç¨‹å¼è¨ªå• API Server æ™‚ä½¿ç”¨ï¼›ServiceAccount åƒ…ä¾·é™å®ƒæ‰€åœ¨çš„ namespaceï¼Œæ¯å€‹ namespace å»ºç«‹æ™‚éƒ½æœƒè‡ªå‹•å»ºç«‹ä¸€å€‹ default service accountï¼›å»ºç«‹ Pod æ™‚ï¼Œå¦‚æœæ²’æœ‰æŒ‡å®š Service Accountï¼ŒPod å‰‡æœƒä½¿ç”¨ default Service Accountã€‚\nService Account Secret SA å°æ‡‰çš„ Secret æœƒè‡ªå‹•æ›è¼‰åˆ° Pod çš„ /var/run/secrets/kubernetes.io/serviceaccount/ ç›®éŒ„ä¸­(åŒ…å« tokenã€ca.crtã€namespace)ã€‚\nå‰µå»º Role \u0026amp; Role Binding å¦‚æœç›´æ¥ä½¿ç”¨é è¨­çš„ sa è¨ªå• api server æœƒé‡åˆ°æ¬Šé™ä¸è¶³çš„å•é¡Œ\næ­¤æ™‚éœ€è¦å»ºç«‹è§’è‰²é–‹æ”¾å­˜å– api æŒ‡å®šè·¯å¾‘çš„æ¬Šé™ä¸¦ç¶å®šè§’è‰²åˆ° SA ä¸Š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # role\u0026amp;binding.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default-role namespace: converg-it rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - pods - pods/log verbs: - get - list --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-roldbinding namespace: converg-it subjects: - kind: ServiceAccount name: default roleRef: kind: Role name: default-role apiGroup: rbac.authorization.k8s.io 1 2 3 kubectl -n converg-it apply -f role\u0026amp;binding.yaml role.rbac.authorization.k8s.io/default-role created rolebinding.rbac.authorization.k8s.io/default-roldbinding created curl API é€²å…¥å®¹å™¨ç’°å¢ƒ\n1 kubectl exec -it -n converg-it converg-it-adapter-oracle-8575f54dc6-7lnv6 -- bash get target API\n1 curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; -X GET https://kubernetes.default.svc/api/v1/namespaces/converg-it/pods/converg-it-adapter-oracle-8575f54dc6-7lnv6/log?sinceSeconds=300 Reference https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/ ","date":"2022-03-31T21:57:12Z","permalink":"http://localhost:1313/p/curl-kubernetes-api-server-within-pod/","title":"åœ¨å®¹å™¨è£¡ curl Kubernetes API server"},{"content":"æ‰‹ä¸Šæœ‰ä¸€å€‹å¯«å¥½çš„ API è¦å°å¤–é‡‹å‡ºï¼Œåœ¨è¨­ç½®å®Œ istio è³‡æºä¹‹å¾Œï¼Œcurl istio ingress gateway/targetAPI å»ä¸€ç›´å›å‚³ 404 Not Foundï¼Œç…§ç†ä¾†èªªé€™å€‹ API å¦‚æœæ‰¾ä¸åˆ°è³‡æ–™å›å‚³çš„ 404 è¨Šæ¯æ‡‰è©²æ˜¯ {\u0026quot;error\u0026quot;:\u0026quot;Record Not Found, the serial number doesn't exist\u0026quot;}ï¼Œç”¨é€™ç¯‡æ–‡ç« è¨˜éŒ„å•é¡Œè·Ÿè§£æ±ºæ–¹å¼ã€‚\nå•é¡Œæ’æŸ¥ åœ¨ç›¸åŒçš„ istio ingress gateway ä¸Šçš„ API çš†æ­£å¸¸é‹ä½œï¼Œæ’é™¤ istio æœ¬èº«å¯èƒ½æœƒæœ‰å•é¡Œ ç”¨åŒæ¨£çš„é…ç½®æª”ï¼Œéƒ¨å±¬åœ¨å¦å¤–ä¸€å€‹ K8s ç’°å¢ƒä¸Šçš„ istioï¼Œç™¼ç¾é‹ä½œæ­£å¸¸ï¼Œæ’é™¤é…ç½®æª”æœ‰èª¤çš„å•é¡Œ ç”¨å…¶ä»– API éƒ¨å±¬ï¼Œä¹Ÿä¸€æ¨£ç›´æ¥ 404 Not Foundï¼Œæ’é™¤åŸå…ˆ API æœ¬èº«å¯èƒ½æœ‰èª¤çš„å•é¡Œ æŸ¥çœ‹ istio ingress gateway çš„ log 1 2 3 [2022-03-24T06:23:07.653Z] \u0026#34;GET /api/convergence/findRecord/TBCC32008806 HTTP/1.1\u0026#34; 200 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 9491 5 4 \u0026#34;10.1.5.32\u0026#34; \u0026#34;PostmanRuntime/7.29.0\u0026#34; \u0026#34;3655dd39-9c2c-9c14-ac5c-53fc7547a155\u0026#34; \u0026#34;10.1.5.41\u0026#34; \u0026#34;10.244.64.149:8080\u0026#34; outbound|8080||converg-api.converg-api.svc.cluster.local 10.244.128.48:42104 10.244.128.48:8080 10.1.5.32:39159 - http-Sbups --- [2022-03-24T06:26:32.759Z] \u0026#34;GET /api/convergence/grabreflow/TBCBB2039913 HTTP/1.1\u0026#34; 404 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 18 1 1 \u0026#34;10.1.5.32\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\u0026#34; \u0026#34;61998a8c-0c79-94a0-9c84-cb03a2f10390\u0026#34; \u0026#34;10.1.5.41\u0026#34; \u0026#34;10.244.128.19:8080\u0026#34; outbound|8080||reverseapi.converg-apipost.svc.cluster.local 10.244.128.48:36740 10.244.128.48:8080 10.1.5.32:15717 - http-XbhqV ä¸Šé¢ 200 çš„æ˜¯æ­£å¸¸é‹ä½œçš„ APIï¼Œä¸‹é¢ 404 æ˜¯æ–°è¨­ç½®çš„ APIï¼Œæ‰€ä»¥å…¶å¯¦ 404 é€™å€‹ istio gw \u0026amp; virtual service å…¶å¯¦æ˜¯æœ‰é‹ä½œçš„ï¼Œå¾—å‡ºä¾†çš„çµè«–å°±æ˜¯ï¼Œistio æ‰¾ä¸åˆ°æ–°è¨­ç½®çš„ API å»è·¯ç”±ï¼Œæ¥µå¤§å¯èƒ½æ˜¯è·Ÿå…¶ä»– URL è¦å‰‡è¡çªã€‚\nè§£æ±º çµ‚æ–¼ç™¼ç¾å‰é¢æœ€å¾Œä¸€å€‹è¨­ç½®çš„ API æ²’æœ‰è¨­ prefixï¼Œæ‰€ä»¥ istio å°±ç›´æ¥ç›£è½ /ï¼Œå°è‡´å¾Œé¢æ€éº¼è¨­æ–°çš„ APIï¼Œéƒ½èªä¸åˆ°!!!!!!!!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 kubectl get virtualservices.networking.istio.io -n converg-apipost converg-apipost-vs -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2021-08-04T10:04:00Z\u0026#34; generation: 9 labels: protocol: http name: converg-apipost-vs namespace: converg-apipost resourceVersion: \u0026#34;241532937\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/converg-apipost/virtualservices/converg-apipost-vs uid: 15049a96-f97f-4b00-bee9-001fafcdd2ff spec: gateways: - converg-apipost-gw hosts: - \u0026#39;*\u0026#39; http: - match: - uri: prefix: \u0026#34;\u0026#34; name: http-OuFqP route: - destination: host: reverseapi port: number: 8080 æŠŠ prefix åŠ ä¸Šå»å¾Œï¼ŒåŸæœ¬å¾ŒåŠ çš„ API å°±æˆåŠŸé‹ä½œäº† \u0026#x270c;\u0026#xfe0f;\n1 2 [root@k8sm1 ~]# curl 10.1.5.41/api/convergence/grabreflow/test {\u0026#34;error\u0026#34;:\u0026#34;Record Not Found, the serial number doesn\u0026#39;t exist\u0026#34;} ","date":"2022-03-24T21:52:00Z","permalink":"http://localhost:1313/p/istio-404-not-found/","title":"Istio æ²’æ›ï¼Œä½†æ­£ç¢ºçš„è¨­ç½® gateway è·Ÿ virtual service å¾Œï¼Œå»ä¸€ç›´ 404 not found"},{"content":"æœ€è¿‘åœ¨åˆªé™¤ namespace çš„æ™‚å€™ç¸½æ˜¯æœƒå¡åœ¨ Terminating çš„ç‹€æ…‹ï¼Œä¸€ç›´ä¸ç–‘æœ‰ä»–çš„ç›´æ¥ä½¿ç”¨ç¶²è·¯ä¸Šå¸¸çœ‹çš„è§£æ±ºæ–¹æ³•å°‡ spec.finalizers æ¸…ç©ºã€‚ä½†å› ç‚ºæ¯æ¬¡åˆªã€æ¯æ¬¡å¡ï¼Œå°±é€£å®Œå…¨ç„¡ä»»ä½•è³‡æºçš„å‘½åç©ºé–“ä¹Ÿæ˜¯å¡ï¼ä»”ç´°çœ‹å¾Œæ‰ç™¼ç¾åŸä¾†æ˜¯æœ‰å…¶ä»–å…ƒä»¶éŒ¯èª¤ï¼Œé€²è€Œé€ æˆå½±éŸ¿ã€‚\nåŸå› æ’æŸ¥ å…ˆä½¿ç”¨æ¸…ç©º finalizer çš„æ–¹æ³•å¼·åˆ¶åˆªé™¤ namespaceï¼Œ\n1 2 kubectl proxy Starting to serve on 127.0.0.1:8001 å¦é–‹ä¸€å€‹ terminal\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026lt;\u0026lt;EOF | curl -X PUT \\ localhost:8001/api/v1/namespaces/test/finalize \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data-binary @- { \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;finalizers\u0026#34;: null } } EOF æœƒå›å‚³ä¸‹é¢çµæœ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 { \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;353766ea-be97-4ccf-9275-0b39bd651afe\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1359585\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2022-03-21T04:18:38Z\u0026#34;, \u0026#34;deletionTimestamp\u0026#34;: \u0026#34;2022-03-21T04:18:51Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;kubernetes.io/metadata.name\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;manager\u0026#34;: \u0026#34;kubectl-create\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-03-21T04:18:38Z\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:metadata\u0026#34;: { \u0026#34;f:labels\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:kubernetes.io/metadata.name\u0026#34;: {} } } } }, { \u0026#34;manager\u0026#34;: \u0026#34;kube-controller-manager\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:conditions\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceContentRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionContentFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionDiscoveryFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionGroupVersionParsingFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceFinalizersRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} } } } }, \u0026#34;subresource\u0026#34;: \u0026#34;status\u0026#34; } ] }, \u0026#34;spec\u0026#34;: {}, \u0026#34;status\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;Terminating\u0026#34;, \u0026#34;conditions\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionDiscoveryFailure\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:56Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;DiscoveryFailed\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Discovery failed for some groups, 1 failing: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently un able to handle the request\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionGroupVersionParsingFailure\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ParsedGroupVersions\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All legacy kube types successfully parsed\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionContentFailure\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentDeleted\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content successfully deleted, may be waiting on finalization \u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceContentRemaining\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentRemoved\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content successfully removed\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceFinalizersRemaining\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentHasNoFinalizers\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content-preserving finalizers finished\u0026#34; } ] } } å¯ä»¥çœ‹åˆ°æœ€å¾Œçš„ Terminating çš„å€å¡Šæœ‰è§£é‡‹åŸå› ã€‚\nè§£æ±ºæ–¹æ³• æŒ‰ç…§åŸå› è§£æ±ºå³å¯æˆåŠŸåˆªé™¤ namespaceï¼Œæœ¬æ–‡é‡åˆ°çš„å•é¡Œæ˜¯å› ç‚ºå®‰è£ metric server æ™‚å¤±æ•—ï¼Œå¯ä»¥çœ‹åˆ° apiservice å…¶ä¸­æœ‰å‡ºç¾ false ç‹€æ…‹ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 kubectl get apiservice NAME SERVICE AVAILABLE AGE v1. Local True 9d v1.admissionregistration.k8s.io Local True 9d v1.apiextensions.k8s.io Local True 9d v1.apps Local True 9d v1.authentication.k8s.io Local True 9d v1.authorization.k8s.io Local True 9d v1.autoscaling Local True 9d v1.batch Local True 9d v1.certificates.k8s.io Local True 9d v1.coordination.k8s.io Local True 9d v1.crd.projectcalico.org Local True 9d v1.discovery.k8s.io Local True 9d v1.events.k8s.io Local True 9d v1.monitoring.coreos.com Local True 3d23h v1.networking.k8s.io Local True 9d v1.node.k8s.io Local True 9d v1.policy Local True 9d v1.rbac.authorization.k8s.io Local True 9d v1.scheduling.k8s.io Local True 9d v1.storage.k8s.io Local True 9d v1alpha1.kafka.strimzi.io Local True 3h3m v1alpha1.monitoring.coreos.com Local True 3d23h v1beta1.batch Local True 9d v1beta1.discovery.k8s.io Local True 9d v1beta1.events.k8s.io Local True 9d v1beta1.flowcontrol.apiserver.k8s.io Local True 9d v1beta1.kafka.strimzi.io Local True 3h3m v1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d1h v1beta1.node.k8s.io Local True 9d v1beta1.policy Local True 9d v1beta1.storage.k8s.io Local True 9d v1beta2.core.strimzi.io Local True 3h3m v1beta2.flowcontrol.apiserver.k8s.io Local True 9d v1beta2.kafka.strimzi.io Local True 3h3m v2.autoscaling Local True 9d v2beta1.autoscaling Local True 9d v2beta2.autoscaling Local True 9d å…ˆæš«æ™‚ç§»é™¤è‡¨æ™‚è£çš„ metric server\n1 helm uninstall metrics-server --namespace kube-system å°±å¯ä»¥æˆåŠŸ delete äº†\n1 2 3 4 5 6 [root@node ~]# kubectl create ns test namespace/test created [root@node ~]# [root@node ~]# kubectl delete ns test namespace \u0026#34;test\u0026#34; deleted [root@node ~# è­¦å‘Š\nå¦å¤–é—œæ–¼ metric-server çš„ debugï¼Œç´€éŒ„åœ¨å¦å¤–ä¸€ç¯‡ æ–‡ç« ã€‚\n","date":"2022-03-22T14:49:00Z","permalink":"http://localhost:1313/p/kubernetes-namespace-delete-terminating/","title":"Kubernetes namespace ä¸€ç›´ delete ä¸æˆåŠŸçš„åŸå›  (å¡åœ¨ terminating status)"},{"content":"Metrics Server é€šé kubeletï¼ˆcAdvisorï¼‰ç²å–ç›£æ§æ•¸æ“šï¼Œä¸»è¦ä½œç”¨æ˜¯ç‚º kube-schedulerã€HPA(Horizontal Pod Autoscaler)ç­‰ k8s æ ¸å¿ƒçµ„ä»¶ï¼Œä»¥åŠ kubectl top å‘½ä»¤å’Œ Dashboard ç­‰ UI çµ„ä»¶æä¾›æ•¸æ“šä¾†æºï¼Œå¯ä»¥ç”¨ä¾†çœ‹ node æˆ– pod çš„è³‡æº (CPU \u0026amp; Memory) æ¶ˆè€—ã€‚é ˆæ³¨æ„çš„æ˜¯ï¼ŒMetric Server æ˜¯ in memory çš„ monitorï¼Œåªå¯ä»¥æŸ¥è©¢ç•¶å‰çš„åº¦é‡æ•¸æ“šï¼Œä¸¦ä¸ä¿å­˜æ­·å²æ•¸æ“šã€‚\nå®‰è£ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm upgrade --install metrics-server metrics-server/metrics-server --namespace kube-system Release \u0026#34;metrics-server\u0026#34; does not exist. Installing it now. NAME: metrics-server LAST DEPLOYED: Mon Mar 21 16:32:14 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: *********************************************************************** * Metrics Server * *********************************************************************** Chart version: 3.8.2 App version: 0.6.1 Image tag: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 *********************************************************************** é€£ç·šå¤±æ•—å•é¡Œ å¯ä»¥ç™¼ç¾ä½¿ç”¨é è¨­å€¼å®‰è£å®Œå¾Œ deployment ä¸€ç›´ç„¡æ³• readyï¼ŒæŸ¥çœ‹ deployment è³‡è¨Š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # kubectl describe deployments.apps -n kube-system metrics-server Name: metrics-server Namespace: kube-system CreationTimestamp: Mon, 21 Mar 2022 16:32:17 +0800 Labels: app.kubernetes.io/instance=metrics-server app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=metrics-server app.kubernetes.io/version=0.6.1 helm.sh/chart=metrics-server-3.8.2 Annotations: deployment.kubernetes.io/revision: 1 meta.helm.sh/release-name: metrics-server meta.helm.sh/release-namespace: kube-system Selector: app.kubernetes.io/instance=metrics-server,app.kubernetes.io/name=metrics-server Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app.kubernetes.io/instance=metrics-server app.kubernetes.io/name=metrics-server Service Account: metrics-server Containers: metrics-server: Image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 Port: 4443/TCP Host Port: 0/TCP Args: --secure-port=4443 --cert-dir=/tmp --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s Liveness: http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3 Environment: \u0026lt;none\u0026gt; Mounts: /tmp from tmp (rw) Volumes: tmp: Type: EmptyDir (a temporary directory that shares a pod\u0026#39;s lifetime) Medium: SizeLimit: \u0026lt;unset\u0026gt; Priority Class Name: system-cluster-critical Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: metrics-server-7d76b744cd (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 3m50s deployment-controller Scaled up replica set metrics-server-7d76b744cd to 1 æŸ¥çœ‹ Pod log\n1 kubectl logs -f -n kube-system metrics-server-7d76b744cd-fv9ns é€£ç·šå¤±æ•—å•é¡Œä¿®æ­£ åŠ ä¸Š --kubelet-insecure-tls å•Ÿå‹•åƒæ•¸\n1 kubectl patch -n kube-system deployment metrics-server --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/template/spec/containers/0/args/-\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;--kubelet-insecure-tls\u0026#34;}]\u0026#39; æ¸¬è©¦ æŸ¥çœ‹ç¯€é»è³‡æºæ¶ˆè€—\n1 2 3 4 5 [root@node ~]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node 312m 7% 2769Mi 36% rockyw 234m 2% 3335Mi 21% rockyw2 199m 2% 2727Mi 17% Reference åœ¨æ‰¾è³‡æ–™çš„æ™‚å€™ç™¼ç¾ä»¥ä¸‹å…©ç¯‡é‡å° metric server çš„ä»‹ç´¹ï¼Œå€¼å¾—æ‹œè®€ã€‚\nKubernetes Monitoring 101 â€” Core pipeline \u0026amp; Services Pipeline Kubernetes è‡ªå‹•å½ˆæ€§ä¼¸ç¸® ","date":"2022-03-21T21:28:00Z","permalink":"http://localhost:1313/p/kubernets-install-metrics-server-by-helm/","title":"ä½¿ç”¨ helm å®‰è£ Metrics Server"},{"content":"kubernetes 1.22 ç‰ˆä¹‹å¾Œï¼Œå°±ä¸å†æ”¯æŒ Docker ä½œç‚º container runtime ä»¥åŠç®¡ç†å®¹å™¨åŠé¡åƒçš„å·¥å…·äº†ã€‚å¯ä»¥ä½¿ç”¨ containerd å–ä»£ docker çš„ container runtimeï¼›ä»¥åŠ crictl ä½œç‚º CRI(Container Runtime Interface)ï¼Œå¦å¤– podman ä¹Ÿå¯ä»¥ç”¨ä¾†ç®¡ç†å®¹å™¨å’Œé¡åƒã€‚æœ¬ç¯‡è¨˜éŒ„åŸºæ–¼ containerd \u0026amp; crictl ä½¿ç”¨ kubeadm éƒ¨å±¬ Kubernetes é›†ç¾¤çš„éç¨‹ã€‚\nç³»çµ±ç’°å¢ƒé…ç½® (æ‰€æœ‰ç¯€é») æœ€å°ç³»çµ±è³‡æºéœ€æ±‚ æ¯å°æ©Ÿå™¨ 4 GiB ä»¥ä¸Š RAM master control plane ç¯€é»è‡³å°‘éœ€è¦æœ‰å…©å€‹ä»¥ä¸Šçš„ vCPU é›†ç¾¤ä¸­æ‰€æœ‰æ©Ÿå™¨ä¹‹é–“çš„å®Œæ•´ç¶²çµ¡é€£æ¥ (can be private or public) Server Type Hostname Spec master node.ulatest.com 4 vCPU, 8G RAM worker rockyw.ulatest.com 8 vCPU, 16G RAM worker rockyw2.ulatest.com 8 vCPU, 16G RAM é…ç½® /etc/hosts 1 2 3 4 5 6 7 8 9 10 cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.1.5.130 nexdata nexdata.ulatest.com 10.1.5.146 node node.ulatest.com 10.1.5.147 rockyw rockyw.ulatest.com 10.1.5.148 rockyw2 rockyw2.ulatest.com 10.1.5.130 nfs nfs.ulatest.com æ›´æ–°è»Ÿé«”å¥—ä»¶ 1 yum update -y ç³»çµ±é…ç½® åœç”¨é˜²ç«ç‰† 1 2 systemctl stop firewalld systemctl disable firewalld é—œé–‰ SELINUX 1 2 3 4 5 6 7 8 9 10 11 12 13 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted é—œé–‰ swap 1 2 3 4 # Turn off swap swapoff -a # comment out the line of swap\u0026#39;s mount point sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab é…ç½® kernel module è‡ªå‹•åŠ è¼‰ 1 2 3 4 5 6 7 8 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä½¿é…ç½®ç”Ÿæ•ˆ modprobe overlay modprobe br_netfilter èª¿æ•´ kernel åƒæ•¸ Kubernetes çš„æ ¸å¿ƒæ˜¯ä¾é  netfilter kernel module ä¾†è¨­å®šä½ç´šåˆ¥çš„é›†ç¾¤ IP è² è¼‰å‡è¡¡ï¼Œéœ€è¦å…©å€‹é—œéµçš„ moduleï¼šIPè½‰ç™¼å’Œæ©‹æ¥ã€‚\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysctl.d/kubernetes.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 EOF # åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä½¿é…ç½®ç”Ÿæ•ˆ sysctl -p /etc/sysctl.d/kubernetes.conf ä»¥ä¸Šæ“ä½œå«æ„å¦‚ä¸‹ï¼š\né–‹å•Ÿ iptables å° bridge çš„æ•¸æ“šé€²è¡Œè™•ç† é–‹å•Ÿæ•¸æ“šåŒ…è½‰ç™¼åŠŸèƒ½ï¼ˆå¯¦ç¾ vxlanï¼‰ ç¦æ­¢ä½¿ç”¨ swap ç©ºé–“ï¼Œåªæœ‰ç•¶ç³»çµ± OOM æ™‚æ‰å…è¨±ä½¿ç”¨å®ƒ é–‹å•Ÿ ipvs module Kube-Proxy æ˜¯ Kubernetes ç”¨ä¾†æ§åˆ¶ Service è½‰ç™¼éç¨‹çš„ä¸€å€‹å…ƒä»¶ï¼Œé è¨­æœƒä½¿ç”¨ iptables ä½œç‚º Kubernetes Service çš„åº•å±¤å¯¦ç¾æ–¹å¼ï¼Œè€Œæ­¤æ¨¡å¼æœ€ä¸»è¦çš„å•é¡Œæ˜¯åœ¨æœå‹™å¤šçš„æ™‚å€™ç”¢ç”Ÿå¤ªå¤šçš„ iptables è¦å‰‡ï¼Œå¤§è¦æ¨¡æƒ…æ³ä¸‹æœ‰æ˜é¡¯çš„æ€§èƒ½å•é¡Œã€‚å¯ä»¥é€éåƒæ•¸è®ŠåŒ–çš„æ–¹å¼è¦æ±‚ Kube-Proxy ä½¿ç”¨ ipvsã€‚é–‹å•Ÿ ipvs çš„å‰ææ¢ä»¶æ˜¯åŠ è¼‰ä»¥ä¸‹çš„ kernal moduleï¼š\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack ä¸Šé¢è…³æœ¬å‰µå»ºäº†çš„ /etc/sysconfig/modules/ipvs.modules æ–‡ä»¶ï¼Œä¿è­‰åœ¨ç¯€é»é‡å•Ÿå¾Œèƒ½è‡ªå‹•åŠ è¼‰æ‰€éœ€æ¨¡å¡Šã€‚ä½¿ç”¨ lsmod | grep -e ip_vs -e nf_conntrack å‘½ä»¤æŸ¥çœ‹æ˜¯å¦å·²ç¶“æ­£ç¢ºåŠ è¼‰æ‰€éœ€çš„å…§æ ¸æ¨¡å¡Šã€‚\næ¥ä¸‹ä¾†é‚„éœ€è¦ç¢ºä¿å„å€‹ç¯€é»ä¸Šå·²ç¶“å®‰è£äº† ipset è»Ÿä»¶åŒ…ï¼Œä»¥åŠç®¡ç†å·¥å…· ipvsadm ä¾¿æ–¼æŸ¥çœ‹ ipvs çš„ä»£ç†è¦å‰‡ã€‚\n1 yum install -y ipset ipvsadm å¦‚æœä»¥ä¸Šå‰ææ¢ä»¶å¦‚æœä¸æ»¿è¶³ï¼Œå‰‡å³ä½¿ kube-proxy çš„é…ç½®é–‹å•Ÿäº† ipvs æ¨¡å¼ï¼Œä¹Ÿæœƒé€€å›åˆ° iptables æ¨¡å¼ã€‚\nå®‰è£ containerd \u0026amp; crictl (æ‰€æœ‰ç¯€é») å®‰è£ containerd 1 2 3 4 5 yum install -y yum-utils # ä½¿ç”¨ docker.ce ä½œç‚º containerd çš„ repo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y containerd.io systemctl enable containerd ç”Ÿæˆ containerd çš„é…ç½®æ–‡ä»¶:\n1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml æ ¹æ“š Kubernetes æ–‡æª” Container runtimes ä¸­çš„å…§å®¹ï¼Œå°æ–¼ä½¿ç”¨ systemd ä½œç‚º init system çš„ Linux ç™¼è¡Œç‰ˆï¼Œä½¿ç”¨ systemd ä½œç‚ºå®¹å™¨çš„ cgroup driver å¯ä»¥ç¢ºä¿æœå‹™å™¨ç¯€é»åœ¨è³‡æºç·Šå¼µçš„æƒ…æ³æ›´åŠ ç©©å®šï¼Œå› æ­¤é€™è£¡é…ç½®å„å€‹ç¯€é»ä¸Š containerd çš„ cgroup driver ç‚º systemdã€‚\nå¦‚æœæª”æ¡ˆä¸­[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] å€å¡Šä¸‹ï¼Œæ²’æœ‰ SystemdCgroup çš„é¸é …ï¼Œä¸‹ï¼š 1 sed -i \u0026#39;s|\\(\\s\\+\\)\\[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options\\]|\\1\\[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options\\]\\n\\1 SystemdCgroup = true|g\u0026#39; /etc/containerd/config.toml å¦‚æœæª”æ¡ˆä¸­[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] å€å¡Šä¸‹ï¼Œæœ‰ SystemdCgroup = false çš„é¸é …ï¼Œä¸‹ï¼š 1 sed -i \u0026#39;s/ SystemdCgroup = false/ SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml ä¿®æ”¹å®Œç•¢å¾Œ config å…§å®¹æœƒå¦‚ä¸‹ï¼š\n1 2 3 4 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true é‡å•Ÿ containerd å·²æ‡‰ç”¨ config\n1 systemctl restart containerd å®‰è£ crictl 1 2 3 yum install -y wget tar wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.23.0/crictl-v1.23.0-linux-amd64.tar.gz tar zxvf crictl-v1.23.0-linux-amd64.tar.gz -C /usr/local/bin è¨­å®š container runtime interface ç‚º containerd\n1 2 3 4 5 6 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF æ¸¬è©¦\n1 2 crictl images IMAGE TAG IMAGE ID SIZE å¦‚æœä¸Šæ–¹ CRI æ²’æœ‰æŒ‡å®šçš„è©±ï¼Œæœƒå‡ºç¾ä»¥ä¸‹éŒ¯èª¤\nå®‰è£ kubernetes å¥—ä»¶ (æ‰€æœ‰ç¯€é») æ–°å¢ kubernetes repo 1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF åœ¨æ›´æ–° yum æºå¾Œï¼Œä½¿ç”¨ yum makecache ç”Ÿæˆç·©å­˜ï¼Œå°‡å¥—ä»¶åŒ…è¨Šæ¯æå‰åœ¨æœ¬åœ° cache ä¸€ä»½ï¼Œç”¨ä¾†æé«˜æœç´¢å®‰è£å¥—ä»¶çš„é€Ÿåº¦ã€‚\n1 yum makecache -y é€šé yum list å‘½ä»¤å¯ä»¥æŸ¥çœ‹ç•¶å‰æºçš„ç©©å®šç‰ˆæœ¬ï¼Œç›®å‰çš„ç©©å®šç‰ˆæœ¬æ˜¯ 1.23.4-0ã€‚å®‰è£ kubeadm ä¾¿æœƒå°‡ kubeletã€kubectl ç­‰ä¾è³´ä¸€ä½µå®‰è£ã€‚\n1 yum list kubeadm 1 yum install -y kubeadm-1.23.4-0 é…ç½®å‘½ä»¤åƒæ•¸è‡ªå‹•è£œå…¨åŠŸèƒ½ 1 2 3 4 yum install -y bash-completion echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc echo \u0026#39;source \u0026lt;(kubeadm completion bash)\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc å•Ÿå‹•kubelet æœå‹™ 1 2 systemctl enable kubelet systemctl restart kubelet é…ç½®ç¯€é» kubeadm éƒ¨ç½² master ç¯€é» æº–å‚™é…ç½®æ–‡ä»¶ 1 2 kubeadm config print init-defaults \u0026gt; kubeadm-init.yaml vim kubeadm.yaml æ›´æ”¹ä»¥ä¸‹é…ç½®\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.1.5.146 # æ”¹ç‚º master node IP bindPort: 6443 nodeRegistration: criSocket: unix:///run/containerd/containerd.sock # æ”¹ç‚º containerd Unix socket åœ°å€ imagePullPolicy: IfNotPresent name: rockym # æŒ‡å®šç¯€é»åç¨± taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: 1.23.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # æŒ‡å®š pod å­ç¶² cidrï¼Œåœ¨è¨­å®š calico æ™‚æœƒç”¨åˆ° scheduler: {} å¢é›†åˆå§‹åŒ– 1 kubeadm init --config=kubeadm-init.yaml å®Œæˆå¾ŒæŒ‰ç…§æç¤ºå°‡ /etc/kubernetes/admin.conf è¤‡è£½åˆ° $HOME/.kube/config\n1 2 3 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config ä¸¦è¤‡è£½ä¸‹é¢é‚£ä¸€ä¸²åŠ å…¥æŒ‡ä»¤ä»¥ä¾¿å…¶ä»– node åŠ å…¥ (å…©å€‹å°æ™‚éæœŸ)\n1 2 kubeadm join 10.1.5.146:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fc56685aedecf323023637a6e02cc1584cfc88bfeb0690dc0e2a1feca278f008 æˆ–æ˜¯ä¹‹å¾Œä½¿ç”¨ kubeadm token create \u0026ndash;print-join-command å»ºç«‹æ–°çš„ã€‚\nä»¥ä¸Šå°±å®Œæˆ master ç¯€é»çš„éƒ¨å±¬ï¼Œå¯ä»¥ä½¿ç”¨ kubectl command ç¢ºèªã€‚\nå› ç›®å‰ç¶²è·¯å°šæœªè¨­ç½®ï¼Œæ‰€ä»¥ coredns ç‹€æ…‹ç‚º Pending æ˜¯æ­£å¸¸çš„ã€‚\nå®‰è£ calico 1 curl -s https://docs.projectcalico.org/manifests/calico.yaml | kubectl apply -f - å®‰è£å®Œç•¢å¾Œå°±å¯ä»¥ç™¼ç¾ç¯€é»å·²ç¶“éƒ¨å±¬å®Œæˆäº†ã€‚\nåŠ å…¥å·¥ä½œç¯€é» åœ¨å„å·¥ä½œç¯€é»ä¸Šç›´æ¥è¼¸å…¥ä¸Šæ–¹çš„ join command\n1 2 kubeadm join 10.1.5.146:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fc56685aedecf323023637a6e02cc1584cfc88bfeb0690dc0e2a1feca278f008 å°±å¯å¤§åŠŸå‘Šæˆäº†~\nè¨»é‡‹\nå¾Œè¨˜\næœ‰äº›æˆªåœ–è£¡é¢å¯ä»¥ç™¼ç¾åŸæœ¬çš„ master node çš„ hostname æœ¬ä¾†å« rockym çš„ï¼Œå¯æ˜¯åœ¨åŠ å…¥ master node ç¯€é»çš„æ™‚å€™çš„åå­—å¿˜è¨˜æ”¹ (å†) å°è‡´ master node å¼·è¿«æ”¹åç‚º node \u0026hellip; æ±‚åŠ©è°·æ­Œå¤§ç¥ï¼Œç™¼ç¾æ”¹ç¯€é»åç¨±æœ€ä¹¾æ·¨ä¸”ç°¡å–®çš„æ–¹å¼å°±æ˜¯åˆªæ‰ç¯€é»å¾Œé‡æ–°åŠ å…¥ï¼Œä½†ä¸å·§åœ°æ˜¯æˆ‘è¦æ”¹çš„ç¯€é»å°±æ˜¯å”¯ä¸€ä¸€å€‹çš„ master node =__= åªå¥½æŠ˜è¡·å°‡éŒ¯å°±éŒ¯æ”¹ hostnameï¼Œä¸çŸ¥é“å¾ŒçºŒæœƒä¸æœƒç™¼ç”Ÿå•é¡Œï¼Œå…ˆè¨˜éŒ„ä¸€ä¸‹ã€‚\nReference kube-proxy https://kubernetes.io/zh/docs/concepts/services-networking/service/#proxy-mode-ipvs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd ","date":"2022-03-13T14:56:00Z","permalink":"http://localhost:1313/p/install-kubernetes-123-on-rocky-linux/","title":"åœ¨ Rocky Linux 8 å®‰è£ Kubernetes 1.23 (containerd as cri)"},{"content":"Kubernetes Dashboard æ˜¯ç”±å®˜æ–¹ç¶­è­·çš„ Kubernetes é›†ç¾¤ WEB UI ç®¡ç†å·¥å…·ï¼Œèƒ½æŸ¥çœ‹ Kubernetes Cluster ä¸Šè³‡æºåˆ†ä½ˆèˆ‡ä½¿ç”¨ç‹€æ³ï¼Œä¹Ÿå¯ä»¥å‰µå»ºæˆ–è€…ä¿®æ”¹ Kubernetes è³‡æºï¼Œè®“ä½¿ç”¨è€…é€é Web UI ä»‹é¢å–ä»£æŒ‡ä»¤çš„ç®¡ç† Kubernetesã€‚\nå®‰è£ å®‰è£éå¸¸ç°¡å–®ï¼Œåªè¦é€éä¸‹é¢ command å³å¯éƒ¨å±¬ã€‚\n1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml ç¢ºèªå®‰è£çµæœ\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@ula ~]# kubectl get all -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-799d786dbf-zhvlc 1/1 Running 0 24s pod/kubernetes-dashboard-fb8648fd9-wd4t5 1/1 Running 0 24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 10.111.121.48 \u0026lt;none\u0026gt; 8000/TCP 24s service/kubernetes-dashboard ClusterIP 10.107.248.188 \u0026lt;none\u0026gt; 443/TCP 25s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dashboard-metrics-scraper 1/1 1 1 24s deployment.apps/kubernetes-dashboard 1/1 1 1 25s NAME DESIRED CURRENT READY AGE replicaset.apps/dashboard-metrics-scraper-799d786dbf 1 1 1 24s replicaset.apps/kubernetes-dashboard-fb8648fd9 1 1 1 24s è¨ªå• å°‡ kubernetes-dashboard æœå‹™æš´éœ² NodePort\n1 kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard å°‡åŸæœ¬ type: ClusterIP æ”¹æˆ type: NodePortã€‚å®Œæˆå¾Œå°±å¯ä»¥ä½¿ç”¨ https://NodeIP:nodePort åœ°å€è¨ªå• dashboardã€‚\n1 2 3 [root@ula ~]# kubectl get svc -n kubernetes-dashboard kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.107.248.188 \u0026lt;none\u0026gt; 443:31302/TCP 44m ç”±æ–¼ Dashboard é»˜èªä½¿ç”¨ httpsï¼Œå…¶è­‰æ›¸ä¸å—ç€è¦½å™¨ä¿¡ä»»ï¼Œæ‰€ä»¥è¨ªå•æ™‚åŠ ä¸Š https å¼·åˆ¶è·³è½‰å°±å¯ä»¥äº†ã€‚\nç™»å…¥ ç™»éŒ„ Dashboard æ”¯æŒ Kubeconfig å’Œ Token å…©ç¨®èªè­‰æ–¹å¼ï¼ŒKubeconfig ä¸­ä¹Ÿä¾è³´ token å­—æ®µï¼Œæ‰€ä»¥ç”Ÿæˆ token é€™ä¸€æ­¥æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ä¸‹é¢ç´€éŒ„ä½¿ç”¨ token çš„æ–¹å¼ç™»éŒ„ã€‚\nå»ºç«‹ service account \u0026amp; role binding æº–å‚™ yaml æª” sc-ula.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ula annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef: kind: ClusterRole name: cluster-admin # k8s é è¨­å»ºç«‹çš„è§’è‰² apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: ula namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: ula namespace: kube-system labels: kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile ä½¿ç”¨ kubectl apply å»ºç«‹\n1 kubectl apply -f sc-ula.yaml å–å¾— Server Account Token æŸ¥çœ‹ service account secret\n1 2 3 4 5 6 7 8 9 10 11 kubectl get sa ula -n kube-system -o=yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2022-03-14T03:12:41Z\u0026#34; name: ula namespace: kube-system resourceVersion: \u0026#34;332586\u0026#34; uid: f048e242-5947-4945-8310-432628e635b7 secrets: - name: ula-token-qwddr å–å¾— token å­—æ®µï¼Œä¸¦ä½¿ç”¨ base64 decode\n1 2 kubectl get secret ula-token-qwddr -o jsonpath={.data.token} -n kube-system | base64 -d eyJhbGciOiJSUzI1NiIsImtpZCI6IjVadWpvSGEtR2tMR2ZFMGVHaGloWjJNOFdRcn............... ç„¶å¾Œåœ¨ dashboard ç™»éŒ„é é¢ä¸Šç›´æ¥ä½¿ç”¨ä¸Šé¢å¾—åˆ°çš„ token å­—ç¬¦ä¸²å³å¯ç™»éŒ„ï¼Œé€™æ¨£å°±å¯ä»¥æ“æœ‰ç®¡ç†å“¡æ¬Šé™æ“ä½œæ•´å€‹ kubernetes é›†ç¾¤çš„å°è±¡ï¼Œä¹Ÿå¯ä»¥æ–°å»ºä¸€å€‹æŒ‡å®šæ“ä½œæ¬Šé™çš„ç”¨æˆ¶ã€‚\nReference https://github.com/kubernetes/dashboard https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/ ","date":"2022-03-13T14:46:00Z","permalink":"http://localhost:1313/p/kubernets-dashboard-installation/","title":"å®‰è£ Kubernetes Dashboard - å–®é›†ç¾¤å¯è¦–åŒ–ç®¡ç†"},{"content":"å»å¹´ç–«æƒ…æ‚¶äº†åŠå¹´ï¼Œåä¸€æœˆåˆå°±æ—©æ—©è¨‚äº† 228 çš„å¸é¦¬åº«æ–¯çš„æ«»èŠ±ç¥­æ—…éŠã€‚å› ç‚ºä¸æƒ³èˆŸè»Šå‹é “çš„é–‹ä¸‰å››å€‹å°æ™‚çš„å±±è·¯ï¼Œå°±æ±ºå®šé‚€è«‹å¤§å§Šç”·å‹å®¶ä¸€èµ·ç›´æ¥åŒ…å…«äººå°åœ˜å‡ºéŠï¼æ¯æ¬¡å‡ºéŠå‰ä¸€å€‹ç¦®æ‹œå°±æœƒå¿ä¸ä½æ¯å¤©éƒ½çœ‹å¥½å¹¾æ¬¡çš„æ°£è±¡é å ±ï¼Œä½†çµæœæ˜¯ï¼Œä¸æ˜¯å‰ä¸€å¤©çœ‹çš„é å ±éƒ½ä¸æº– \u0026#x1f923;\nç„¶è¶…ç´šå¹¸é‹çš„æ˜¯ï¼ŒåŸå…ˆé€£çºŒå…©å€‹ç¦®æ‹œæ¯å¤©éƒ½åœ¨ä¸‹é›¨çš„åŒ—éƒ¨ï¼Œç«Ÿç„¶å°±é€™éº¼å‰›å‰›å¥½çš„åœ¨é€£å‡å‰ä¸€å¤©æ­£å¼æ”¾æ™´ \u0026#x2600;\u0026#xfe0f; \u0026#x2600;\u0026#xfe0f; è¡Œå‰æ¯å¤©éƒ½åœ¨ç¥ˆç¦±å¤§æ™´å¤©ï¼Œç”šè‡³é‚„åœ¨å›å˜‰çš„å‰ä¸€å¤©æŒ‰ç…§ç¶²è·¯ä¸Šçš„ä¸ç§‘å­¸ç¿’ä¿—ç•«äº†çƒé¾œç‡’ XD ä½†å„˜ç®¡å®Œå…¨æ²’æ ¹æ“šï¼Œé‚„æ˜¯å¾ˆç¥å¥‡çš„å…©å¤©éƒ½æ˜¯ä¸€é»é›²éƒ½æ²’æœ‰çš„è¶…ç´šå¥½å¤©æ°£!!(æ•…æ„æ²’ç•«é›²) amazing~~ \u0026#x1f973; \u0026#x1f60e; ç¬¬ä¸€æ™šå¤œå®¿æ–°ç«¹å¸‚å€ å› ç‚ºæƒ³å¤šç•™ä¸€é»æ™‚é–“åœ¨å¸é¦¬åº«æ–¯ï¼Œæ‰€ä»¥ç¬¬ä¸€å€‹æ™šä¸Šæ±ºå®šå…ˆä¸Šæ–°ç«¹ä½ä¸€æ™šã€‚æº–æ™‚çš„ç´„äº†æ—…è¡Œç¤¾å®‰æ’çš„æ™šä¸Šä¸ƒé»ï¼Œä½†é€£å‡åœ‹é“å¡å¥½å¡æ»¿ï¼Œæ‰€ä»¥æœ€å¾Œåé»æ‰åˆ°é£¯åº—ã€‚åŸéšå»Ÿå¤œå¸‚å¹¾ä¹éƒ½é—œäº†ï¼Œæ‰€å¹¸é‚„èƒ½é‡åˆ°é–‹å¾—å¾ˆæ™šçš„ç¢³çƒ¤ï¼Œä¾¿è²·äº†å¹¾é“èœé…é…’ï¼Œä¸äº¦æ¨‚ä¹ï¼ ç„¶å¾Œç¬¬ä¸€å¤©æ™šä¸Šçš„æ—…é¤¨çˆ›åˆ°ä¸çŸ¥é“å¦‚ä½•è©•è«– \u0026#x1f643; ä¸‹æ¬¡éè»Šç¨‹å¾ˆé çš„æ—…è¡Œé‚„æ˜¯è‡ªåŠ©éŠæœ€å¥½äº†ï¼Œå¦‚ä»Šå·²ç¶“ä¸åƒå¤§å­¸æ™‚çš„çª®éŠå¯ä»¥äº«å—çœéŒ¢çš„å¿«æ¨‚å•¦ \u0026#x1f92a;\nåˆèˆˆè»Šç«™ éš”å¤©ä¸€æ—©å…«é»åŠä¾¿å¾é£¯åº—å‡ºç™¼ï¼Œæ—©ä¸Šé£¯åº—é™„é¤æ˜¯å¾ˆç¥å¥‡çš„ä¾¿ç•¶ =_=? åƒå®Œä¾¿ç•¶é…èœå°±è·Ÿå§Šå€‘å»é™„è¿‘çš„å…¨å®¶è§£æ±ºã€‚è·¯ä¸Šè²·äº†å¸æ©Ÿæ¨è–¦çš„æ–°ç«¹æº«åª½åª½å®¢å®¶èœåŒ…è·Ÿè‰ä»”ç²¿ï¼Œç±³é¦™å¾ˆé¦™ï¼Œå…§é¤¡ä¹Ÿå¥½åƒï¼ å°æ¯”ä¹‹ä¸‹ï¼Œæˆ‘çš„æ§‹åœ–é‚„æ˜¯æœ‰å¾…åŠ å¼· TVT è¾›è‹¦æˆ‘å§Šäº† åˆ°ç¬¬ä¸€å€‹æ™¯é»æ™‚ï¼Œé›²å°±éƒ½æ•£äº†ï¼Œå¤©æ°£è¶…å¥½! è»è‰¦å²© å‰å¾€è»è‰¦å²©å‰ç¶“éäº†ä¸€å€‹è§€æ™¯å°ï¼Œåœ¨é¦¬é‡Œå…‰éƒ¨è½é™„è¿‘ã€‚ åœ¨è»è‰¦å²©æœ‰å…©åº§åŠæ©‹ï¼Œæ˜¥å¤©å¤§éƒ¨åˆ†çš„æ¨¹è‘‰éƒ½è®Šç¶ äº†ï¼Œä½†é‚„èƒ½çœ‹åˆ°äº›è¨±ç´…é»ƒè¤è‰²çš„è‘‰å­ç©¿æ’ï¼Œå‘ˆç¾å±±å·’çš„å±¤æ¬¡ç¾ã€‚æ²³æ°´é‡å……æ²›ï¼Œä¸”éå¸¸ä¹¾æ·¨ï¼Œå¥½ç¾å“‡~~ å¸é¦¬åº«æ–¯ ä¸€å€‹å°æ™‚å¾Œï¼Œå¤§æ¦‚ä¸€é»å°±åˆ°å¸é¦¬åº«æ–¯äº†ï¼å…ˆåƒå€‹å¾åˆèˆˆè»Šç«™å¸¶ä¸Šä¾†çš„å±±è±¬è‚‰ä¾¿ç•¶(æ„å¤–å¥½åƒ)ã€‚åœ¨ä¸‰é»å…¥ä½å‰åˆ°è™•åœ¨éƒ¨è½é€›é€›ï¼Œè¶è‘—å‰ä¸€æ‰¹éŠå®¢é›¢é–‹ã€ä¸‹ä¸€æ‰¹éŠå®¢åˆ°ä¾†å‰ï¼Œåœ¨å¤§é“è·Ÿéƒ¨è½å…¥å£æ‹ç…§ï¼Œææ—©ä¾†çœŸçš„æ˜¯å¤ªå°äº† \u0026#x1f97a; \u0026#x1f607; å¤§æ¦‚ååˆ†é˜åˆ°ä¸Šé¢çš„è§€æ™¯å°ï¼Œè·¯é€”ä¸Šæœ‰ä¸€éš»å¯æ„›çš„é»ƒç‰› \u0026#x1f42e;ï¼Œè§€æ™¯å°å¯ä»¥çœ‹åˆ°æ•´å€‹å¸é¦¬åº«æ–¯éƒ¨è½ã€‚ ç„¶å¾Œä¸‰é»æº–æ™‚è·Ÿå¸æ©Ÿ check-in å°æœ¨å±‹ã€‚é€™æ¬¡å› ç‚ºæ²’æœ‰é¦¬ä¸Šå°±è¨‚å¥½ï¼Œæ‰€ä»¥åªèƒ½ä½é è¿‘å¤§é–€çš„ä¼¯ç‰¹åˆ©å±‹ é›…æˆ¿ï¼Œä½†æ…¶å¹¸çš„æ˜¯ä½åœ¨äºŒæ¨“ï¼Œè€Œä¸”è¡›æµ´å°±æˆ¿é–“åœ¨æ—é‚Šï¼å¤§å®¶åœ¨æˆ¿é–“å…§å„è‡ªä¼‘æ¯ï¼Œå—‘äº†ä¸€äº›å¸¶ä¾†çš„é›¶é£Ÿè·Ÿæ—©ä¸Šæ²’åƒå®Œçš„èœåŒ…å……é£¢ï¼Œé‚„ç¡äº†ä¸€å°è¦ºè£œçœ ï¼Œå¤•é™½ç‘é€²ä¾†ï¼Œå‰›å‰›å¥½çš„æº«åº¦ï¼Œå¤ªèˆ’æœå•¦ \u0026#x1f606; å¾äºŒæ¨“æ‹å‡ºå»çš„é¢¨æ™¯ï¼Œç´€éŒ„å¸é¦¬åº«æ–¯å¤ªé™½ä¸‹å±±çš„å…‰å½±è®ŠåŒ–ã€‚ ä¸ƒé»æº–æ™‚åˆ°ä¹Ÿæ˜¯æœ¨é ­æ­å»ºçš„é¤å»³åƒæå‰ä¸€å€‹ç¦®æ‹œè·Ÿæ—…è¡Œç¤¾é»å¥½çš„æ™šé¤ï¼Œæ‹”é‚„è²·äº†æ°´èœœæ¡ƒé…’è·Ÿç±³é…’é…ã€‚ åƒé£½å‡ºä¾†å¤©è‰²å°±å®Œå…¨æš—äº†ï¼Œä¸€æŠ¬é ­æ˜¯æ»¿å¤©çš„æ˜Ÿè¾°å¤§æµ·ï¼Œç¾å¾—èª‡å¼µ \u0026#x2b50;\u0026#x2b50;\u0026#x2b50; ç”¨äºŒå§Šçš„ 11 pro å¤œé–“æ¨¡å¼æ‹å‡ºä¾†çš„æ˜Ÿæ˜Ÿé‚„èƒ½é€™éº¼æ˜é¡¯ \u0026#x1f97a; è®šå˜† iPhoneï¼ æ´—äº†å€‹æ°´å£“è¶…å¼·çš„èˆ’æœç†±æ°´æ¾¡ä¹‹å¾Œå°±è¿…é€Ÿçš„ç¡è¦ºäº† \u0026#x1f923; éš”å£é‚„èƒ½è½åˆ°æ‹”çš„æ‰“å‘¼è²å“ˆå“ˆå“ˆ \u0026#x1f923;\néš”å¤©èµ·åºŠï¼Œå‡Œæ™¨å¤ªé™½é‚„æ²’å‡ºä¾†çš„æ™‚å€™ï¼Œè¶…Â·ç´šÂ·å†·ï¼ç›´æ¥æŠŠæ‰€æœ‰è¡£æœå¥—åœ¨èº«ä¸Š \u0026#x1f62c; åƒå®Œæ—©é¤å¾Œå¤ªé™½æ•´å€‹å‡ºä¾†åˆæŠŠæ—©ä¸Šç©¿çš„è¡£æœåˆè„«å›å» XDD æº«å·®è¶…å¤§ã€‚ æ—©é¤æ˜¯ä¸ƒé»ï¼Œåƒæ¸…ç²¥å°èœè·Ÿåå¸é‚„æœ‰è˜‹æœ(æœ¬ä¾†å¾Œä¾†è¦å¸¶å»çˆ¬å±±åƒï¼Œæ®Šä¸çŸ¥æ²’åƒåˆ°é‚„ç›´æ¥å¿˜è¨˜ï¼Œå®Œæ•´çš„å¸¶å››é¡†å›å°åŒ— \u0026#x1f602; \u0026#x1f643; ))\nå·¨æœ¨ç¾¤ç™»å±±æ­¥é“ å¤§æ¦‚å¿«å…«é»é–‹å§‹ç™»å±±ï¼Œæ•´æ¢æ­¥é“ä¾†å›å¤§æ¦‚åä¸€å…¬é‡Œï¼Œè·¯é€”ä¸Šå¾ˆå¹³ç·©ï¼Œå› ç‚ºä¸Šå±±å·²ç¶“ç”¨è»Šçˆ¬ä¸‰å››å€‹å°æ™‚äº†å˜› XDDD è·¯ä¸Šé¢¨æ™¯ä¹Ÿå¾ˆæ¼‚äº®ï¼Œèƒ½ç¶“éä¸€æ•´æ’çš„æ«»èŠ±æ—è·Ÿç«¹å­æ—ï¼Œæœ€å¾Œåˆ°é”å·¨æœ¨ç¾¤ã€‚å¹¸è™§é€™å…©å¤©çš„å¥½å¤©æ°£ï¼Œè®“æ³¥åœŸå¤§éƒ¨åˆ†éƒ½ä¹¾äº†ï¼Œä¿ä½äº†æˆ‘è·Ÿå§Šç‚ºäº†é€™å…©å¤©è²·çš„æ–°é‹ \u0026#x1f923; ä¸€å¤§æ—©å‡ºç™¼çš„å¥½è™•å°±æ˜¯ç™»å±±å£éƒ½æ²’äººï¼Œæ‹äº†è¶…ç´šæ¼‚äº®çš„èªè­‰ç…§ \u0026#x1f607; è·¯ä¸Šåªæœ‰æˆ‘å€‘çš„æ«»èŠ±æ—ï¼Œå¯¦åœ¨å¤ªç¾äº†ã€‚ å›é¡§ç…§ç‰‡ä»èˆŠæœƒè®šå˜†çš„ç¾æ™¯ã€‚ æŠµé”å·¨æœ¨ç¾¤çµ‚é» (é¡Œå¤–è©±ï¼šåˆ°çµ‚é»çš„æ™‚å€™å‰›å¥½é‡åˆ°æ¯”æˆ‘å€‘æ—©å…ˆå‡ºç™¼çš„åœ˜ï¼Œæ‹ç…§å°±ç„¡æ³•é‚£éº¼æ‚ é–’çš„æ‹äº†ï¼Œæ±ºå®šå¿«èµ°åˆ°ä¸‹ä¸€åº§å·¨æœ¨ï¼Œä¸ç„¶æ¡‘å±å°±è¿½ä¸Šä¾†å•¦ -v-) è¬å®¶ç…§ç‰‡åˆé›† æ‹”éº»çš„æ„å¢ƒç…§ï¼Œå¤ªå–œæ­¡ç¬¬äºŒå¼µæ«»èŠ±æ­¥é“çš„èƒŒå½±ç…§äº†ï¼Œå›ä¾†ç›´æ¥è¨­ç‚ºæ‰‹æ©Ÿå°é¢!!! å¤§å§Š äºŒå§Š æœ¬å¦¹ ","date":"2022-02-28T21:00:00+08:00","image":"http://localhost:1313/p/smangus-2022/cover_hu8184729626239843418.jpg","permalink":"http://localhost:1313/p/smangus-2022/","title":"å¸é¦¬åº«æ–¯å…©æ—¥éŠğŸ¤"},{"content":"å•é¡Œæè¿° nginx ingress å¾åŸæœ¬ deprecated çš„ stable/nginx-ingress helm chart æ”¹ç‚º ingress-nginx/ingress-nginx chart å¾Œï¼Œç™¼ç¾ ingress resource çš„ nginx ç¶²é  404 not foundï¼ŒæŸ¥çœ‹ ingress nginx controller log ç™¼ç¾æœ‰ ingress does not contain a valid IngressClass çš„éŒ¯èª¤ã€‚\nå®Œæ•´ log å¦‚ä¸‹ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [root@testm terraform]# kubectl logs -f -n ingress-nginx ingress-nginx-controller-5c5bf8c854-7pcf7 ------------------------------------------------------------------------------- NGINX Ingress controller Release: v1.1.1 Build: a17181e43ec85534a6fea968d95d019c5a4bc8cf Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.19.9 ------------------------------------------------------------------------------- W0209 05:43:07.359176 7 client_config.go:615] Neither --kubeconfig nor --master was specified. Using thesterConfig. This might not work. I0209 05:43:07.359883 7 main.go:223] \u0026#34;Creating API client\u0026#34; host=\u0026#34;https://10.96.0.1:443\u0026#34; I0209 05:43:07.379993 7 main.go:267] \u0026#34;Running in Kubernetes cluster\u0026#34; major=\u0026#34;1\u0026#34; minor=\u0026#34;20\u0026#34; git=\u0026#34;v1.20.15\u0026#34; \u0026#34;clean\u0026#34; commit=\u0026#34;8f1e5bf0b9729a899b8df86249b56e2c74aebc55\u0026#34; platform=\u0026#34;linux/amd64\u0026#34; I0209 05:43:07.644539 7 main.go:104] \u0026#34;SSL fake certificate created\u0026#34; file=\u0026#34;/etc/ingress-controller/ssl/defake-certificate.pem\u0026#34; I0209 05:43:07.675915 7 ssl.go:531] \u0026#34;loading tls certificate\u0026#34; path=\u0026#34;/usr/local/certificates/cert\u0026#34; key=\u0026#34;/ual/certificates/key\u0026#34; I0209 05:43:07.727278 7 nginx.go:255] \u0026#34;Starting NGINX Ingress controller\u0026#34; I0209 05:43:07.828276 7 event.go:282] Event(v1.ObjectReference{Kind:\u0026#34;ConfigMap\u0026#34;, Namespace:\u0026#34;ingress-nginxe:\u0026#34;ingress-nginx-controller\u0026#34;, UID:\u0026#34;c6943575-18fe-471a-aa44-5f251af7a277\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;104FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;CREATE\u0026#39; ConfigMap ingress-nginx/ingress-nginx-controller I0209 05:43:08.929877 7 nginx.go:297] \u0026#34;Starting NGINX process\u0026#34; I0209 05:43:08.930002 7 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-cler-leader... I0209 05:43:08.930504 7 nginx.go:317] \u0026#34;Starting validation webhook\u0026#34; address=\u0026#34;:8443\u0026#34; certPath=\u0026#34;/usr/local/icates/cert\u0026#34; keyPath=\u0026#34;/usr/local/certificates/key\u0026#34; I0209 05:43:08.930926 7 controller.go:155] \u0026#34;Configuration changes detected, backend reload required\u0026#34; I0209 05:43:08.945063 7 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controllder I0209 05:43:08.945130 7 status.go:84] \u0026#34;New leader elected\u0026#34; identity=\u0026#34;ingress-nginx-controller-5c5bf8c854- I0209 05:43:09.015595 7 controller.go:172] \u0026#34;Backend successfully reloaded\u0026#34; I0209 05:43:09.015708 7 controller.go:183] \u0026#34;Initial sync, sleeping for 1 second\u0026#34; I0209 05:43:09.015766 7 event.go:282] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;ingress-nginx\u0026#34;, Namress-nginx-controller-5c5bf8c854-7pcf7\u0026#34;, UID:\u0026#34;f84b09e4-7d7d-40bf-ae66-f2eb72ab7a59\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceV:\u0026#34;104846\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;RELOAD\u0026#39; NGINX reload triggered due to a change in configurat W0209 05:43:26.721706 7 controller.go:988] Error obtaining Endpoints for Service \u0026#34;monitoring/prometheus-or-prometheus\u0026#34;: no object matching key \u0026#34;monitoring/prometheus-operator-prometheus\u0026#34; in local store W0209 05:43:26.721831 7 controller.go:1083] Service \u0026#34;monitoring/prometheus-operator-grafana\u0026#34; does not havactive Endpoint. W0209 05:43:26.722981 7 controller.go:988] Error obtaining Endpoints for Service \u0026#34;monitoring/prometheus-or-alertmanager\u0026#34;: no object matching key \u0026#34;monitoring/prometheus-operator-alertmanager\u0026#34; in local store I0209 05:43:26.781310 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.06s renderingIngressLength:1 renderingIngressTime:0.001s admissionTime:18.0kBs testedConfiguraze:0.061} I0209 05:43:26.781370 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorinetheus-ingress\u0026#34; I0209 05:43:26.784116 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.063s renderingIngressLength:1 renderingIngressTime:0s admissionTime:18.0kBs testedConfiguratio0.063} I0209 05:43:26.784155 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorinana-ingress\u0026#34; I0209 05:43:26.785125 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.063s renderingIngressLength:1 renderingIngressTime:0.002s admissionTime:18.1kBs testedConfigurize:0.065} I0209 05:43:26.785171 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorintmanager-ingress\u0026#34; I0209 05:43:26.785238 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.064s renderingIngressLength:1 renderingIngressTime:0s admissionTime:18.0kBs testedConfiguratio0.064} I0209 05:43:26.785259 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;istio-sysali-ingress\u0026#34; I0209 05:43:26.931088 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/grafana-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; I0209 05:43:26.931132 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/prometheus-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; I0209 05:43:26.931149 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/alertmanager-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; I0209 05:43:26.945691 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;istio-system/kiali-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; åŸ Ingress é…ç½® ä»¥ grafana-ing.tf ç‚ºç¯„ä¾‹ (ä½¿ç”¨ terraform è‡ªå‹•éƒ¨ç½²å·¥å…·å®‰è£)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;grafana_ingress\u0026#34; { count = var.package.prometheus == true ? 1 : 0 metadata { name = \u0026#34;grafana-ingress\u0026#34; namespace = \u0026#34;monitoring\u0026#34; } spec { rule { host = \u0026#34;${var.prometheus-options.host_grafana}.${var.domain}\u0026#34; http { path { backend { service_name = \u0026#34;prometheus-operator-grafana\u0026#34; service_port = 80 } path = \u0026#34;/\u0026#34; } } } } depends_on = [ helm_release.ingress-nginx, ] } ç™¼ç”ŸåŸå›  åŸå› æ˜¯åœ¨ ingress v1.0.0 ç‰ˆä¹‹å¾Œï¼Œingress éœ€è¦åŠ ä¸Š ingress classï¼Œè«‹åƒè€ƒ github çš„ #7341 pull requestï¼Œå¦‚æœæ²’æœ‰ï¼Œcontroller æœƒä¸Ÿ Ignoring ingress because of error while validating ingress class\u0026quot; ingress=\u0026quot;k8sNamespace/ingressResourceName\u0026quot; error=\u0026quot;ingress does not contain a valid IngressClass\u0026quot; çš„éŒ¯èª¤ã€‚\nè³‡è¨Š\nAn Ingress Class is basically a category which specify who needs to serve and manage the Ingress, this is necessary since in a cluster you can have more than one Ingress controller, each one with its rules and configurations.\nè§£æ±ºæ–¹æ³• åœ¨ ingress resource ä¸­çš„ metadata æ¬„ä½åŠ ä¸Š annotations: kubernetes.io/ingress.class: \u0026quot;nginx\u0026quot; å³å¯ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;grafana_ingress\u0026#34; { count = var.package.prometheus == true ? 1 : 0 metadata { name = \u0026#34;grafana-ingress\u0026#34; namespace = \u0026#34;monitoring\u0026#34; annotations = { \u0026#34;kubernetes.io/ingress.class\u0026#34; = \u0026#34;nginx\u0026#34; } } spec { rule { host = \u0026#34;${var.prometheus-options.host_grafana}.${var.domain}\u0026#34; http { path { backend { service_name = \u0026#34;prometheus-operator-grafana\u0026#34; service_port = 80 } path = \u0026#34;/\u0026#34; } } } } depends_on = [ helm_release.ingress-nginx, ] } æ³¨æ„ ä¸Šè¿°æ–¹æ³•åƒ…é©ç”¨æ–¼ kubernetes 1.22 ç‰ˆä»¥å‰ã€‚k8s 1.22 ç‰ˆä»¥å¾Œè«‹ä½¿ç”¨ ingressClassName æ–¼ spec å€å¡Šä¸‹ã€‚è«‹åƒè€ƒå®˜ç¶²èªªæ˜ã€‚\nReference https://forum.linuxfoundation.org/discussion/859965/exercise-7-nginx-update-requires-change-to-yaml https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/ingress https://stackoverflow.com/questions/65979766/ingress-with-nginx-controller-not-working-address-missing ","date":"2022-02-11T06:20:00Z","permalink":"http://localhost:1313/p/kubernets-ingress-invalid-ingressclass/","title":"Ingress does not contain a valid IngressClass"},{"content":"åœ¨ä¸‹ kubectl æ™‚å‡ºç¾ Unable to connect to the server: x509: certificate has expired or is not yet valid çš„éŒ¯èª¤ï¼ŒåŸå› æ˜¯ kubernetes apiserver è­‰æ›¸å·²éæœŸï¼Œkubernetes çš„ apiServer èˆ‡ kubelet çš„è¨ªå•æˆæ¬Šè­‰æ›¸æ˜¯ä¸€å¹´ï¼Œå®˜æ–¹è¡¨ç¤ºé€šéé€™ç¨®æ–¹å¼ï¼Œè®“ç”¨æˆ¶ä¸æ–·çš„å‡ç´šç‰ˆæœ¬ã€‚\nç›®å‰æœ‰å¹¾ç¨®è§£æ±ºæ–¹å¼ï¼š\né‡æ–°ç”Ÿæˆè­‰æ›¸å–ä»£éæœŸçš„è­‰æ›¸ (æœ¬æ¬¡ä½œæ³•) å‡ç´šé›†ç¾¤ä»¥è‡ªå‹•æ›´æ–°è­‰æ›¸ éƒ¨å±¬ä¸€å¥—æ–°çš„ç’°å¢ƒï¼Œå°‡æ¥­å‹™é·ç§»éå» å»æ‰è­‰æ›¸é©—è­‰åŠŸèƒ½ (ä¸å®‰å…¨ä¸”ä¸ç§‘å­¸ï¼Œéœ€è¦è‡ªå·±æ”¹ source code) æŸ¥çœ‹è­‰æ›¸çš„æœ‰æ•ˆæ—¥æœŸ é€é openssl ç›´æ¥æŸ¥è­‰æ›¸å…§å®¹\n1 2 3 $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | grep Not Not Before: Nov 17 04:48:20 2020 GMT Not After : Nov 17 04:48:20 2021 GMT æˆ–æ˜¯é€é kubeadm æª¢æŸ¥ Kubernetes ç’°å¢ƒè­‰æ›¸\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W1118 09:51:35.880390 7092 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no apiserver Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; ca no apiserver-etcd-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no apiserver-kubelet-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; ca no controller-manager.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no etcd-healthcheck-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-peer Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-server Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no front-proxy-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; front-proxy-ca no scheduler.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Nov 15, 2030 04:48 UTC 8y no etcd-ca Nov 15, 2030 04:48 UTC 8y no front-proxy-ca Nov 15, 2030 04:48 UTC 8y no ç¶“æŸ¥çœ‹ k8s master çµ„ä»¶è­‰æ›¸éƒ½éæœŸäº†ã€‚\næ›´æ–°è­‰æ›¸ å‚™ä»½èˆŠæœ‰çš„é…ç½®æ–‡ä»¶èˆ‡è­‰æ›¸ 1 $ cp -rf /etc/kubernetes /etc/kubernets.bak æ›´æ–°è­‰æ›¸ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ kubeadm alpha certs renew all [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [renew] Error reading configuration from the Cluster. Falling back to default configuration W1118 11:11:52.322016 26585 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed é‡æ–°ç”Ÿæˆé…ç½®æ–‡ä»¶\né€™äº›é…ç½®æ–‡ä»¶ä¸­åŒ…å«è­‰æ›¸ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°ç”Ÿæˆ\n1 2 $ rm -rf /etc/kubernetes/*.conf $ kubeadm init phase kubeconfig all --apiserver-advertise-address 10.1.5.21 æ›´æ–°é…ç½®èº«ä»½èªè­‰çš„ $HOME/.kube/config æª”æ¡ˆ\nå°‡é‡æ–°ç”Ÿæˆæ–¼ /etc/kubernetes ä¸‹çš„ admin.conf æª”æ¡ˆè¦†è“‹åŸå…ˆçš„ ~/.kube/config 1 2 $ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ chown $(id -u):$(id -g) $HOME/.kube/config é‡æ–°å•Ÿå‹• kubelet \u0026amp; docker service 1 2 $ systemctl restart kubelet $ systemctl restart docker é‡æ–°ä½¿ç”¨ kubectl è¨ªå•é›†ç¾¤ 1 2 3 4 5 6 7 8 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8sm1 Ready master 365d v1.17.13 k8sm2 Ready master 365d v1.17.13 k8sm3 Ready master 365d v1.17.13 k8sw1 Ready \u0026lt;none\u0026gt; 365d v1.17.13 k8sw2 Ready \u0026lt;none\u0026gt; 365d v1.17.13 k8sw3 Ready \u0026lt;none\u0026gt; 365d v1.17.13 å¦‚æœæ˜¯å¤š masterï¼Œä¸Šé¢çš„æ­¥é©Ÿåœ¨æ¯å€‹ master éƒ½è¦åš Reference https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired https://cloud.tencent.com/developer/article/1832411 ","date":"2021-11-21T17:36:00Z","permalink":"http://localhost:1313/p/kubernets-ca-expired/","title":"Unable to connect to the server: x509: certificate has expired or is not yet valid"},{"content":"ä¸‹è¼‰ Istio ä¸‹è¼‰è³‡æº ç”¨è‡ªå‹•åŒ–å·¥å…·ä¸‹è¼‰ä¸¦æå–æœ€æ–°ç‰ˆæœ¬ï¼ˆLinux æˆ– macOSï¼‰ï¼š\n1 $ curl -L https://istio.io/downloadIstio | sh - æˆ–æ˜¯ç”¨æŒ‡å®šåƒæ•¸ä¸‹è¼‰æŒ‡å®šçš„ã€ä¸åŒè™•ç†å™¨é«”ç³»çš„ç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼Œä¸‹è¼‰ x86_64 æ¶æ§‹çš„ã€1.6.8 ç‰ˆæœ¬çš„ Istio ï¼Œé‹è¡Œï¼š\n1 $ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.8 TARGET_ARCH=x86_64 sh - é€²å…¥ Istio åŒ…ç›®éŒ„ 1 $ cd istio-1.11.4 å®‰è£ç›®éŒ„åŒ…å«ï¼š\nsamples/Â ç›®éŒ„ä¸‹çš„ç¤ºä¾‹æ‡‰ç”¨ç¨‹åº bin/Â ç›®éŒ„ä¸‹çš„Â [istioctl](https://www.bookstack.cn/read/istio-1.11-zh/af90c7c768b11bf8.md)Â å®¢æˆ·ç«¯äºŒé€²åˆ¶æ–‡ä»¶ è¨­å®š istioctl å°‡Â istioctlÂ å®¢æˆ·ç«¯åŠ å…¥åŸ·è¡Œè·¯å¾„ï¼ˆLinux or macOSï¼‰:\n1 $ export PATH=$PWD/bin:$PATH éƒ¨ç½² Istio Operator 1 $ istioctl operator init æ­¤å‘½ä»¤é‹è¡Œ Operator åœ¨ istio-operator å‘½åç©ºé–“ä¸­å‰µå»ºä»¥ä¸‹è³‡æºï¼š\nOperator è‡ªå®šç¾©è³‡æºå®šç¾©ï¼ˆCRDï¼‰ Operator æ§åˆ¶å™¨çš„ deployment å°è±¡ ä¸€å€‹ç”¨ä¾†è¨ªå• Operator æŒ‡æ¨™çš„æœå‹™ Istio Operator é‹è¡Œå¿…é ˆçš„ RBAC è¦å‰‡ æŸ¥çœ‹å‰µå»ºçš„è³‡æº\n1 kubectl get all -n istio-operator å®‰è£ Istio å¯ä»¥ä¾æ“š profile å®‰è£æŒ‡å®šçš„ istio å¥—ä»¶\n1 2 3 4 5 6 7 8 9 10 $ kubectl create ns istio-system $ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istiocontrolplane spec: profile: demo EOF å„ç¨® profile çš„ yaml æª”æ”¾ç½®åœ¨ ./manifests/profiles ä¸‹ï¼Œå¯ä¾ç…§éœ€æ±‚æ”¹å…§å®¹åƒæ•¸ã€‚ä¸¦ä½¿ç”¨ $ kubectl apply -f xxx.yaml éƒ¨å±¬ã€‚\nå®‰è£ Addons Component ä¸åŒæ–¼ä»¥å¾€ 1.6 ä»¥å‰çš„ç‰ˆæœ¬çš„ istio yaml æª”å¯ç›´æ¥æŒ‡å®š addonComponentsï¼Œåœ¨ 1.11 ç‰ˆå¦‚æœè¦å®‰è£ Kialiã€Jaeger ç­‰ addon componentï¼Œå‰‡éœ€è¦å¦å¤–éƒ¨å±¬ã€‚ 1 2 3 4 5 # ä¸€æ¬¡éƒ¨å±¬æ‰€æœ‰ addons $ kubectl apply -f samples/addons # å–®ç¨æŒ‡å®šå¥—ä»¶éƒ¨å±¬ $ kubectl apply -f samples/addons/kiali.yaml Resource https://preliminary.istio.io/latest/zh/docs/setup/install/operator/ https://istio.io/latest/docs/setup/getting-started/ ","date":"2021-10-26T20:27:00Z","permalink":"http://localhost:1313/p/kubernets-install-istio/","title":"ä½¿ç”¨ istio operator å®‰è£ Istio v1.11"},{"content":"åŸå…ˆä½¿ç”¨ k8s-at-home çš„ helm chart éƒ¨å±¬ï¼Œä½†å®Œæˆå¾Œç™¼ç¾ node å®‰è£å¾Œæœƒ deploy ç•°å¸¸ï¼Œæ‡·ç–‘æ˜¯ persistence è¨­å®šå•é¡Œï¼Œä½†åˆä¸æƒ³èŠ±æ™‚é–“æ·±ç©¶ï¼Œæ‰€ä»¥å°±ç›´æ¥è‡ªå·±å¯« yaml éƒ¨å±¬æ¯”è¼ƒå¿«ã€‚\næº–å‚™ node-red.yaml æª” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nodered-pvc namespace: node-red spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: nfs --- apiVersion: apps/v1 kind: Deployment metadata: name: node-red namespace: node-red spec: replicas: 1 selector: matchLabels: app: node-red template: metadata: labels: app: node-red spec: containers: - name: node-red image: nodered/node-red:2.1.2 ports: - containerPort: 1880 name: node-red protocol: TCP imagePullPolicy: IfNotPresent volumeMounts: - name: nodered-data mountPath: /data volumes: - name: nodered-data persistentVolumeClaim: claimName: nodered-pvc --- apiVersion: v1 kind: Service metadata: name: node-red namespace: node-red spec: type: NodePort selector: app: node-red ports: - protocol: TCP port: 1880 nodePort: 31880 éƒ¨å±¬ 1 2 kubectl create ns node-red kubectl -n node-red apply -f node-red.yaml å®Œæˆ 1 kubectl get all -n node-red è¨ªå• service ä½¿ç”¨ NodePortï¼Œå‰‡ç›´æ¥ä½¿ç”¨ control plane çš„ IP ä»¥åŠ NodePort æŒ‡å®šçš„ port 31880 è¨ªå• Node-RED å³å¯ã€‚ ","date":"2021-10-26T09:44:00Z","permalink":"http://localhost:1313/p/install-nodered-on-kubernetes/","title":"[Node-RED] Deploy on Kubernetes"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # note the old IPs allocated to the services kubectl get svc # edit config kubectl edit cm -n metallb-system config # delete the metallb pods kubectl -n metallb-system delete pod --all # watch the pods come back up kubectl -n metallb-system get pods -w # inspect new IPs of services kubectl get svc or\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026lt;\u0026lt; EOF \u0026gt; new_config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 10.0.0.10-10.0.0.19 EOF # delete the old configmap kubectl -n metallb-system delete cm config # apply the new configmap kubectl apply -f new_config.yaml why we need to delete all metallb pod MetalLB rejects new configurations if the new configuration would break existing services. Your new configuration does not allow existing services to continue existing, so MetalLB ignored it. There are log entries in the controller and speaker pods about this.\nTo force MetalLB to accept an unsafe configuration, delete all the controller and speaker pods. When they restart, they\u0026rsquo;ll accept the new configuration and change all your services.Â kubectl delete po -n metallb-system --all\nReference https://github.com/metallb/metallb/issues/308 https://github.com/metallb/metallb/issues/348 ","date":"2021-09-16T10:30:00Z","permalink":"http://localhost:1313/p/kubernets-metallb/","title":"Change MetalLB IP Range"},{"content":"minikube æ˜¯ä¸€å€‹ç”± Google ç™¼å¸ƒçš„éƒ¨ç½²å–®ç¯€é»çš„ Kubernetes Cluster çš„å·¥å…·ï¼Œå¯ä»¥å®‰è£åœ¨æœ¬æ©Ÿä¸Šï¼Œæ”¯æ´ Windows èˆ‡ Mac Minikube åªæœ‰ä¸€å€‹ Node (ç¯€é»)ã€‚å°æ–¼æœ¬åœ°å¯¦é©—å¯ä»¥é¿å…ç¯€é»ä¸è¶³çš„å›°æ“¾ï¼›è®“é–‹ç™¼è€…å¯ä»¥åœ¨æœ¬æ©Ÿä¸Šè¼•æ˜“æ¶è¨­ä¸€å€‹ Kubernetes Clusterï¼Œå¿«é€Ÿä¸Šæ‰‹ Kubernetes çš„æŒ‡ä»¤èˆ‡ç’°å¢ƒã€‚\né‹ä½œåŸç†å°±æ˜¯æœƒåœ¨æœ¬æ©Ÿä¸Šå»ºç«‹ä¸€å€‹ virtual machineï¼Œä¸¦ä¸”åœ¨é€™ VM å»ºç«‹ä¸€å€‹ signle-node Kubernetes Clusterã€‚\nminikube é©åˆç”¨æ–¼é–‹ç™¼ç’°å¢ƒæ¸¬è©¦ï¼Œä¸æœƒæŠŠå®ƒç”¨åœ¨å¯¦éš›ç”Ÿç”¢ç’°å¢ƒä¸­ã€‚\nä¸‹è¼‰èˆ‡éƒ¨å±¬ Minikube æ”¯æ´ Windowsã€MacOSã€Linuxï¼Œåœ¨é€™ä¸‰ç¨®å¹³å°çš„æœ¬æ©Ÿç«¯éƒ½å¯ä»¥å®‰è£ä¸¦åŸ·è¡Œ Minikube ã€‚å®‰è£åŠåŸ·è¡Œæ­¥é©Ÿï¼Œè«‹åƒè€ƒå®˜ç¶²ã€‚\næ•´é«”æ­¥é©Ÿå¦‚ä¸‹ï¼š\nå®‰è£Virtualization Softwareï¼Œå¦‚Â VirtualBox å®‰è£Â kubectlÂ å¥—ä»¶ï¼Œç”¨ä»¥å’Œ K8S é›†ç¾¤äº¤äº’æºé€š å¾Â GithubÂ ä¸‹è¼‰ Minikube å¥—ä»¶ å•Ÿå‹• minikube åŠ K8s é›†ç¾¤ ä½¿ç”¨ kubectl æ“ä½œé›†ç¾¤åŠæ‡‰ç”¨ å®˜ç¶²è·Ÿå…¶ä»–æ•™å­¸æ–‡å¯«å¾—å¾ˆè©³ç´°ï¼Œåœ¨é€™è£¡å°±ä¸ä¸€ä¸€åˆ—ç¤ºäº†ã€‚\nReference https://ithelp.ithome.com.tw/articles/10192490 ","date":"2021-09-05T22:17:00Z","permalink":"http://localhost:1313/p/kubernets-minikube/","title":"Minikube"},{"content":"Problem åœ¨å˜—è©¦æ›´æ–° Kubernetes æ™‚ï¼Œä¸‹äº†ä¸‹é¢çš„ command å–å¾—ç›®å‰é›†ç¾¤çš„çµ„ä»¶ç‹€æ…‹ï¼š\n1 $ kubectl get cs ç™¼ç¾ controller-mamager å’Œ scheduler æœ‰ unhealthy çš„ç‹€æ…‹ï¼š\n1 2 3 4 NAME STATUS MESSAGE ERROR controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused scheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34; Reason é€™å…©å€‹ pod çš„éå®‰å…¨ç«¯å£æ²’æœ‰é–‹å•Ÿï¼Œå¥åº·æª¢æŸ¥æ™‚å ±éŒ¯ï¼Œä½†æ˜¯ç”±æ–¼æœ¬èº«æœå‹™æ˜¯æ­£å¸¸çš„ï¼Œåªæ˜¯å¥åº·æª¢æŸ¥çš„ç«¯å£æ²’å•Ÿï¼Œæ‰€ä»¥ä¸å½±éŸ¿æ­£å¸¸ä½¿ç”¨ã€‚\nSolution åœ¨æ‰€æœ‰ Master nodes ä¸Šä¿®æ”¹ä¸‹é¢æª”æ¡ˆ:\n1 2 $ vim /etc/kubernetes/manifests/kube-scheduler.yaml $ vim /etc/kubernetes/manifests/kube-controller-manager.yaml åˆªæ‰æˆ–è¨»é‡‹æ‰ - --port=0 (spec-\u0026gt;containers-\u0026gt;command) é€™è¡Œ\n1 $ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml é‡å•Ÿ kubelet æœå‹™\n1 $ sudo systemctl restart kubelet.service é€™æ™‚10251ï¼Œ10252ç«¯å£å°±é–‹å•Ÿäº†ï¼Œå¥åº·æª¢æŸ¥ç‹€æ…‹ä¹Ÿæ­£å¸¸äº†ã€‚\n1 2 3 [root@master1 ~]# netstat -tulpn | grep \u0026#39;10251\\|10252\u0026#39; tcp6 0 0 :::10251 :::* LISTEN 11863/kube-schedule tcp6 0 0 :::10252 :::* LISTEN 11902/kube-controll Reference https://www.cnblogs.com/wuliping/p/13780147.html ","date":"2021-08-31T21:15:00Z","permalink":"http://localhost:1313/p/kubernets-scheduler-controller-manager-unhealthy/","title":"è§£æ±º scheduler and controller-manager unhealthy state"},{"content":"ä¸Šæ¬¡å°‡ K8s é›†ç¾¤å¾ 1.7 å‡ç´šåˆ° 1.20 ä¹‹å¾Œï¼Œåœ¨å‰µå»º pvc æ™‚ï¼Œç™¼ç¾ç‹€æ…‹æœƒä¸€ç›´åœç•™åœ¨ Pendingï¼Œè©³ç´°è³‡è¨Šå¦‚ä¸‹ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@master1 telegraf]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Pending nfs 5s [root@master1 telegraf]# kubectl describe pvc test Name: test Namespace: default StorageClass: nfs Status: Pending Volume: Labels: \u0026lt;none\u0026gt; Annotations: volume.beta.kubernetes.io/storage-provisioner: cluster.local/nfs-sc-0-nfs-client-provisioner Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 10s (x5 over 50s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \u0026#34;cluster.local/nfs-sc-0-nfs-client-provisioner\u0026#34; or manually created by system administrator æŸ¥äº†åŸå› å¯èƒ½æ˜¯å› ç‚º selfLink ç‚ºç©ºï¼Œç„¡æ³•å»ºç«‹åƒè€ƒã€‚ Kubernetes v1.20 é–‹å§‹ï¼Œæ ¹æ“š change log èªªæ˜ï¼Œé»˜èªåˆªé™¤äº† metadata.selfLink å­—æ®µï¼Œç„¶è€Œéƒ¨åˆ†æ‡‰ç”¨ä»ç„¶ä¾è³´æ–¼æ­¤åŠŸèƒ½ï¼Œä¾‹å¦‚ nfs-client-provisionerã€‚å¦‚æœä»ç„¶è¦ç¹¼çºŒä½¿ç”¨é€™äº›æ‡‰ç”¨ï¼Œéœ€è¦é‡æ–°å•Ÿç”¨selfLinkã€‚\nè§£æ±ºæ–¹å¼ ä¿®æ”¹ kube-apiserver æ–‡ä»¶\n1 vim /etc/kubernetes/manifests/kube-apiserver.yaml åœ¨ command ä¸‹çš„åƒæ•¸åŠ ä¸Š --feature-gates=RemoveSelfLink=false\n1 2 3 4 5 6 7 8 9 10 11 spec: containers: - command: - kube-apiserver - --advertise-address=10.1.5.141 - --allow-privileged=true - --au... . . . **- --feature-gates=RemoveSelfLink=false** å„²å­˜ç·¨è¼¯å¾Œï¼Œkube-apiserver ä¾¿æœƒè‡ªå‹•é‡å•Ÿã€‚\n","date":"2021-08-29T21:47:00Z","permalink":"http://localhost:1313/p/kubernets-nfs-client-provisioner-pending-in-creating-pvc/","title":"k8s v1.20 nfs-client-provisioner å‰µå»º pvc æ™‚åœåœ¨ Pending"},{"content":"ç´€éŒ„åœ¨ç¾æœ‰ kubernetes 1.17 é›†ç¾¤å‡ç´šåˆ° 1.20 çš„éç¨‹ã€‚\nç’°å¢ƒèªªæ˜ é›†ç¾¤é…ç½® ä¸»æ©Ÿå ç³»çµ± IP Address vip Server Load Balancer (SLB) 10.1.5.140 master1 CentOS 7.8 10.1.5.141 Worker1 CentOS 7.8 10.1.5.142 ç•¶å‰é‹è¡Œç‰ˆæœ¬ çµ„ä»¶ ç‰ˆæœ¬ kubeadm v1.17.13 kubelet v1.17.13 kubectl v1.17.13 Container Runtime - Docker v20.10.7 etcd V3.4.3-0 kube-apiserver v1.17.17 kube-controller-manager v1.17.17 kube-proxy v1.17.17 kube-scheduler v1.17.17 coredns v1.6.5 pause v3.1 æŸ¥çœ‹ç‰ˆæœ¬çš„å‘½ä»¤å¦‚ä¸‹\n1 2 3 4 kubeadm version kubelet --version kubectl version kubectl get node master1 -o yaml Kubernetes Non-Active Branch History ç›®å‰ä½¿ç”¨çš„ 1.17 ç‰ˆå·²åœ¨ 2021-01-13 EOLï¼Œfinal patch release åœ¨ 1.17.17ï¼Œè«‹åƒè€ƒå®˜æ–¹ repo èªªæ˜ã€‚\næº–å‚™å‡ç´š ç‰ˆæœ¬å‡ç´šçš„å¿…è¦ å°æ–¼ Kubernetes é›†ç¾¤çš„ä½¿ç”¨è€…ï¼š æ›´æ–°çš„ç‰ˆæœ¬èƒ½æœ‰æ›´æ–°çš„åŠŸèƒ½ã€æ›´åŠ å…¨é¢çš„å®‰å…¨è£œä¸ä»¥åŠè«¸å¤šçš„bugfixã€‚\nå°æ–¼ Kubernetes é›†ç¾¤çš„é‹ç¶­è€…ï¼š é€šéé›†ç¾¤å‡ç´šåŠŸèƒ½å¯ä»¥æ‹‰é½Šæ‰€ç®¡ç†çš„é›†ç¾¤ç‰ˆæœ¬ï¼Œæ¸›å°‘é›†ç¾¤ç‰ˆæœ¬çš„ç¢ç‰‡åŒ–ï¼Œå¾è€Œæ¸›å°‘ç®¡ç†æˆæœ¬å’Œç¶­è­·æˆæœ¬ã€‚\nå‡ç´šæ³¨æ„äº‹é … å‡ç´šåƒ…æ”¯æŒä¸€å€‹å°ç‰ˆæœ¬è™Ÿã€‚ä¹Ÿå°±æ˜¯èªªï¼Œåªèƒ½å¾ 1.7 å‡ç´šåˆ° 1.8 çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæˆ–æ˜¯å¾ 1.17.13 å‡ç´šåˆ° 1.17.17ï¼Œè€Œä¸èƒ½å¾ 1.7 ç›´æ¥å‡ç´šåˆ° 1.9ã€‚ ä¸€æ¬¡æ›´æ–°ä¸€å€‹ç¯€é»ï¼Œç¢ºä¿ Kubernetes åŠŸèƒ½ä¸æœƒå› ç‚ºæ›´æ–°è€Œä¸­æ–·ã€‚ å‡ç´šå¾Œæ‰€æœ‰å®¹å™¨éƒ½æœƒè¢«é‡å•Ÿï¼Œé¿å…æœå‹™ä¸­æ–·ï¼Œéœ€ç¢ºä¿æ‡‰ç”¨ç¨‹å¼ä½¿ç”¨é€²éšçš„ Kubernetes API å»ºç«‹ï¼Œå¦‚ Deploymentï¼Œæˆ–æ˜¯åˆ©ç”¨å‰¯æœ¬æ©Ÿåˆ¶ã€‚ kubeadm upgrade ä¸æœƒå½±éŸ¿å·¥ä½œè² è¼‰ï¼Œåªæœƒæ¶‰åŠKubernetes å…§éƒ¨çš„çµ„ä»¶ï¼Œä½†ç‚ºä¿éšªèµ·è¦‹ï¼Œä»ç„¶å¯ä»¥å…ˆå‚™ä»½ etcd çš„ç‹€æ…‹ã€‚ å‚™ä»½ etcd Snapshot åˆç¨±ç‚ºå¿«ç…§ï¼Œå°±åƒç…§ç›¸ä¸€æ¨£ï¼Œåœ¨æŸå€‹æ™‚é–“é»ï¼Œå°‡ç¡¬ç¢Ÿç›®å‰çš„æ•´å€‹ç‹€æ…‹å„²å­˜èµ·ä¾†ï¼Œä»¥ä½œç‚ºå°‡ä¾†é‚„åŸçš„å‚™ä»½ä¾æ“šã€‚ Snapshot æ˜¯å¹¾ä¹æ‰€æœ‰çš„å„²å­˜æœå‹™è¨­å‚™éƒ½æœƒæä¾›çš„åŠŸèƒ½ï¼Œå°±åƒæ˜¯å¹«ä½ ç¡¬ç¢Ÿä¸Šçš„è³‡æ–™ç…§å¼µåƒä¸€æ¨£ï¼ŒæŠŠé€™å€‹ç›®å‰çš„ç‹€æ…‹è¨˜éŒ„ä¸‹ä¾†ï¼Œä»¥å‚™å°‡ä¾†é‚„åŸä¹‹ç”¨ã€‚\netcd çš„å‚™ä»½æœ‰å…©ç¨®æ–¹å¼ï¼š\nä½¿ç”¨ etcdctl snapshot save é€²è¡Œå‚™ä»½ 1 $ kubectl -n kube-system exec -it etcd-master1 -- sh -c \u0026#34;ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key etcdctl --endpoints=https://127.0.0.1:2379 snapshot save /var/lib/etcd/snapshot1.db\u0026#34; ä½¿ç”¨ etcdctl snapshot statusæŸ¥çœ‹å‚™ä»½\n1 $ kubectl -n kube-system exec -it etcd-master1 -- sh -c \u0026#34;ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key etcdctl --endpoints=https://127.0.0.1:2379 snapshot status -w table /var/lib/etcd/snapshot.db\u0026#34; æª¢è¦–å‚™ä»½æª”\n1 2 3 4 5 6 $ ls /var/lib/etcd/ member snapshot.db $ mkdir ~/backup $ cp /var/lib/etcd/snapshot.db ~/backup/\t# å‚™ä»½ etcd æ ¸å¿ƒè³‡æ–™æª”æ¡ˆ $ cp -r /etc/kubernetes/pki/etcd $HOME/backup/ ä½¿ç”¨ docker etcd image é€£ç·šé€²å…¥ etcd å…§éƒ¨ä¸‹ command 1 2 3 4 5 6 7 8 9 10 11 $ mkdir -vp /data/backup $ docker run --rm \\ -v /data/backup:/backup \\ -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \\ --env ETCDCTL_API=3 \\ registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0 \\ /bin/sh -c \u0026#34;etcdctl --endpoints=https://192.168.12.226:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \\ --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\ snapshot save /backup/etcd-snapshot-1.19.13.db\u0026#34; é–‹å§‹å‡ç´š 1.17.13 ~ 1.18.20 ä¸»ç¯€é» ç¢ºèªå¯å‡ç´šç‰ˆæœ¬èˆ‡å‡ç´šæ–¹æ¡ˆ\n1 $ yum list --showduplicates kubeadm --disableexcludes=kubernetes é€šéä»¥ä¸Šå‘½ä»¤æŸ¥è©¢åˆ° 1.18 ç•¶å‰æœ€æ–°ç‰ˆæ˜¯ 1.18.20-0 ç‰ˆæœ¬ã€‚master å¦‚æœ‰ä¸‰å€‹ç¯€é»å¯å…ˆå¾ master3 ç¯€é»é–‹å§‹å‡ç´šã€‚\nyum å‡çº§ kubernetes æ’ä»¶\n1 $ yum install kubeadm-1.18.20-0 kubelet-1.18.20-0 kubectl-1.18.20-0 --disableexcludes=kubernetes é¨°ç©ºç¯€é»æª¢æŸ¥é›†ç¾¤æ˜¯å¦å¯å‡ç´š\n1 2 $ kubectl drain master1 --ignore-daemonsets $ kubeadm upgrade plan å‡ç´š\n1 $ kubeadm upgrade apply v1.18.20 é‡å•Ÿ kubelet å–æ¶ˆç¯€é»ä¿è­·\n1 2 3 4 $ systemctl daemon-reload $ systemctl restart kubelet $ kubectl uncordon master1 $ kubectl get nodes ç¶“æ¸¬è©¦å¾Œé›–ç„¶æ¸¬è©¦ç’°å¢ƒçš„é›†ç¾¤çš„ master åªæœ‰ä¸€å€‹ï¼Œä½†å‡ç´šå–®ä¸€é€™å€‹ master å¾Œå¥½åƒä¸æœƒé€ æˆ Pod çš„å½±éŸ¿ã€‚\nå·¥ä½œç¯€é» åˆ‡æ›åˆ° worker nodeï¼Œyumå‡çº§kubernetesæ’ä»¶\n1 2 3 # åœ¨ worker node ç¯€é»æ“ä½œ $ ssh worker $ yum install kubeadm-1.18.20-0 kubelet-1.18.20-0 kubectl-1.18.20-0 --disableexcludes=kubernetes å°‡ç¯€é»æ¨™è¨˜ç‚ºä¸å¯èª¿åº¦ä¸¦é€å‡ºå·¥ä½œè² è¼‰ã€‚\n1 2 3 4 5 6 7 # master1 $ kubectl drain worker1 --ignore-daemonsets # å¯ä»¥çœ‹è¦‹èˆ‡ä¸‹é¡ä¼¼çš„è¼¸å‡ºï¼š node/ip-172-31-85-18 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-dj7d7, kube-system/weave-net-z65qx node/ip-172-31-85-18 drained å‡ç´š\n1 2 # worker1 $ kubeadm upgrade node é‡å•Ÿkubelet ä¸¦å–æ¶ˆå°ç¯€é»çš„ä¿è­·\n1 2 3 4 5 6 7 # worker1 $ systemctl daemon-reload $ systemctl restart kubelet # master1 $ kubectl uncordon worker1 # é€šéå°‡ç¯€é»æ¨™è¨˜ç‚ºå¯èª¿åº¦ï¼Œè®“ç¯€é»é‡æ–°ä¸Šç·šï¼Œå¦‚æœæœ‰åšä¿è­·ç¯€é»çš„æ­¥é©Ÿçš„è©±åœ¨åšã€‚ $ kubectl uncordon worker1 å¾ 1.18.20 å‡ç´šåˆ° 1.19.12 é‡è¤‡ä»¥ä¸Šå‹•ä½œ\nå¾ 1.19.12 å‡ç´šåˆ° 1.20.10 é‡è¤‡ä»¥ä¸Šå‹•ä½œ\nReference https://cloud.tencent.com/developer/article/1848150 https://v1-18.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ https://xie.infoq.cn/article/6e47d96d4c4a4c6d47852adc8 https://www.796t.com/article.php?id=260536 for etcd backup ","date":"2021-08-22T21:29:00Z","permalink":"http://localhost:1313/p/k8s-upgrade/","title":"Kubernetes å‡ç´šç´€éŒ„"},{"content":"ConfigMap ConfigMap ä»¥ key-vaule çš„æ–¹å¼ç”¨ä¾†æè¿°ç³»çµ±ç›¸é—œè¨­å®šï¼Œæ‰€æœ‰èˆ‡æ‡‰ç”¨ç¨‹å¼ç›¸é—œçš„éæ•æ„Ÿæ€§æœªåŠ å¯†çš„è³‡è¨Šå¯æ”¾åœ¨ ConfigMap å…§ã€‚è€Œå¦‚æœ‰æ•æ„Ÿæ€§è³‡æ–™ï¼Œå‰‡éœ€é€é Secretã€‚\nä¸»è¦ç›®çš„ ä¸»è¦ç›®çš„æ˜¯å°‡æ‡‰ç”¨ç¨‹å¼èˆ‡è¨­å®šè§£è€¦ï¼ŒConfigMap èˆ‡ Pod å°‡å€‹åˆ¥å–®ç¨å­˜åœ¨æ–¼ k8s å¢é›†ä¸­ï¼Œç•¶ Pod éœ€è¦ä½¿ç”¨ ConfigMap æ™‚æ‰éœ€è¦å°‡ ConfigMap æ›è¼‰åˆ° Pod å…§ä½¿ç”¨ã€‚è§£è€¦çš„å¥½è™•æœ‰ï¼š\nä¾¿æ–¼ç®¡ç† å½ˆæ€§é«˜ï¼Œå¯æ›è¼‰ä¸åŒçš„ ConfigMap åˆ° Pod å…§ä½¿ç”¨ï¼›æˆ–æ˜¯åŒä¸€å€‹ ConfigMap æ›è¼‰åˆ°å¤šå€‹ Podã€‚ ç”¨æ³• Kubernetes çš„ ConfigMap é€é kubectl create æˆ– kubectl apply ä¾†å»ºç«‹ã€‚\n1 2 $ kubectl create configmap [è³‡æºåå­—] [ä¾†æºåƒæ•¸] $ kubectl apply condfigmap.yaml ä½¿ç”¨ kubectl create å»ºç«‹ ä½¿ç”¨ kubectl create å¯ä»¥å¾æª”æ¡ˆè·¯å¾‘ã€æª”æ¡ˆæˆ–æ˜¯ literal value ä¾†å»ºç«‹ configMapã€‚\n\u0026ndash;from-file 1 2 3 4 5 6 7 8 # å»ºç«‹åç‚º myConfã€è³‡æ–™ä¾†æºæ˜¯æŸè·¯å¾‘ä¸‹æ‰€æœ‰æª”æ¡ˆçš„ configMap $ kubectl create configmap myConf --from-file=/path/for/config/file/ # å»ºç«‹åç‚º myConfã€è³‡æ–™ä¾†æºæ˜¯ä¸€å€‹æª”æ¡ˆçš„ configMap $ kubectl create configmap myConf --from-file=/path/to/app.properties # å»ºç«‹åç‚º myConfã€è³‡æ–™ä¾†æºæ˜¯å¤šå€‹æª”æ¡ˆçš„ configMap $ kubectl create configmap myConf --from-file=/path/of/app1.properties --from-file=/path/of/app2.properties è¨»é‡‹\nå¦‚æœæ˜¯ä¾†æºæ˜¯æª”æ¡ˆçš„è©±ï¼Œå‰‡ configMap ä¸­çš„ key å°±æœƒæ˜¯æª”åï¼Œvalue å‰‡æ˜¯æª”æ¡ˆå…§å®¹ã€‚\n\u0026ndash;from-literal 1 2 3 4 # å»ºç«‹åç‚º myConfã€åŒ…å«æŒ‡å®šéµå€¼å°çš„ configMap $ kubectl create configmap myConf --from-literal=key1=config1 $ kubectl create configmap myConf --from-literal=key1=config1 --from-literal=key2=config2 å…©å€‹å…±ç”¨ 1 2 3 $ kubectl create configmap myConf --from-file=/path/of/config.conf \\ --from-literal=key1=config1 \\ --from-literal=key2=config2 \u0026ndash;from-env-file ä½¿ç”¨ç’°å¢ƒè®Šæ•¸è¡¨ç¤ºçš„æª”æ¡ˆã€‚ è­¦å‘Š\nè«‹æ³¨æ„ï¼Œvalue å¦‚æœæœ‰ \u0026quot;\u0026quot; å‰‡æœƒè¦–ç‚ºæ˜¯å€¼çš„ä¸€éƒ¨ä»½ã€‚ä¸”å¦‚æœåœ¨åŒå€‹ create ä¸­ä½¿ç”¨å¤šå€‹ \u0026ndash;from-env-file å‰‡æŒ‡æœƒæ‡‰ç”¨æœ€å¾Œä¸€å€‹ã€‚\nä½¿ç”¨ kubectl apply yaml æª”æ¡ˆå»ºç«‹ æº–å‚™ yaml æª”\n1 2 3 4 5 6 7 8 9 10 11 12 --- apiVersion: v1 kind: ConfigMap metadata: name: myConf data: key1: config1 key2: config2 app.properties: | property.1 = value1 property.2 = value2 property.3 = value3 ä½ˆç½²\n1 $ kubectl apply -f configmap.yaml æŸ¥çœ‹ ConfigMap å»ºç«‹å®Œæˆå¾Œå¯ä»¥ä½¿ç”¨ kubectl get æˆ–æ˜¯ kubectl describe çš„æ“·å– configMap çš„å…§å®¹ã€‚\n1 $ kubectl get configmaps myConf -o yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2021-01-04T18:52:05Z name: myConf namespace: default resourceVersion: \u0026#34;516\u0026#34; uid: b4952dc3-d670-11e5-8cd0-68f728db1985 data: key1: config1 key2: config2 app.properties: | property.1 = value1 property.2 = value2 property.3 = value3 å°‡ ConfigMap æ›è¼‰åˆ° pod ä½¿ç”¨ ç•¶æˆç’°å¢ƒè®Šæ•¸ä½¿ç”¨ pod çš„ yaml æª”å¦‚ä¸‹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: testenv spec: containers: - name: test image: tomcat:8 imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $(KEY1_ENV)\u0026#34; ] env: - name: KEY1_ENV valueFrom: configMapKeyRef: name: myConf key: key1 å°‡ pod è·‘èµ·ä¾†å¾Œï¼ŒConfigMap myConf ä¸­çš„ key1 çš„ value å°±æœƒåšç‚ºç’°å¢ƒè®Šæ•¸ KEY1_ENV çš„å€¼ã€‚\næ›è¼‰æˆ volume pod çš„ yaml æª”å¦‚ä¸‹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: testvolume spec: containers: - name: test image: tomcat: 8 imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/keys\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: myConf è¨»é‡‹\nä½¿ç”¨ volume å°‡ ConfigMap ä½œç‚ºæ–‡ä»¶æˆ–ç›®éŒ„ç›´æ¥æ›è¼‰ï¼ŒConfigMap ä¸­æ¯ä¸€å€‹ key-value éµå€¼å°éƒ½æœƒç”Ÿæˆä¸€å€‹æ–‡ä»¶ï¼Œkey ç‚ºæ–‡ä»¶åï¼Œvalue ç‚ºå…§å®¹ã€‚\nå¦ä¸€ç¨®æ–¹å¼ï¼Œåªæ›è¼‰æŸå€‹ keyï¼Œä¸¦æŒ‡å®šç›¸å°è·¯å¾‘ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: testvolume spec: containers: - name: test image: tomcat:8 imagePullPolicy: IfNotPresent volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: myConf items: - key: key1 path: /path/to/key1 # key1 æœƒæ”¾åœ¨ mountPath /etc/config/path/to ä¸‹ã€‚ - key: app.properties path: app.properties # å¦‚æœ path èˆ‡ key ç›¸åŒï¼Œå‰‡æœƒç›´æ¥æŠŠ app.properties æ–‡ä»¶æ”¾åœ¨ mountPath ä¸‹ã€‚ Reference https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ https://www.cnblogs.com/pu20065226/p/10690628.html ","date":"2021-01-08T14:26:00Z","permalink":"http://localhost:1313/p/kubernets-configmap/","title":"ConfigMap å»ºç«‹åŠæ›è¼‰"},{"content":"K8s Obejcts å¸¸ç”¨çš„åŸºæœ¬ objects Pod\nPod æœ‰å…©ç¨®é¡å‹ï¼šæ™®é€š Pod å’Œéœæ…‹ Pod (static pod)ã€‚éœæ…‹ Pod å³ä¸é€šé K8S èª¿åº¦å’Œå‰µå»ºï¼Œç›´æ¥åœ¨æŸå€‹å…·é«”çš„ Node æ©Ÿå™¨ä¸Šé€šéå…·é«”çš„æ–‡ä»¶ä¾†å•Ÿå‹•ã€‚æ™®é€š Pod å‰‡æ˜¯ç”± K8S å‰µå»ºã€èª¿åº¦ï¼ŒåŒæ™‚æ•¸æ“šå­˜æ”¾åœ¨ etcd ä¸­ã€‚ Service\nå¯ä»¥èªç‚ºæ˜¯ pod çš„åå‘ä»£ç†ï¼Œè² è²¬æ¥æ”¶å®¢æˆ¶ç«¯è«‹æ±‚ï¼ŒæŠŠè«‹æ±‚è½‰çµ¦ podã€‚å› ç‚ºæ¯å€‹ pod éƒ½æœ‰è‡ªå·±çš„å…§éƒ¨ ipï¼Œä½†æ˜¯ deployment çš„ pod çš„ ip æ˜¯æœ‰å¯èƒ½è®Šçš„ (pod æ›æ‰æˆ–è¤‡è£½)ï¼Œæ‰€ä»¥éœ€è¦ service ä¾†åšé¡ä¼¼ä¸­é–“è€…çš„æŠ½è±¡å­˜åœ¨ã€‚Service æŒ‘é¸ã€é—œè¯ Pod çš„æ–¹å¼ç‚ºåŸºæ–¼ Label Selector é€²è¡Œå®šç¾©ã€‚é€šé type åœ¨ ServiceSpecä¸­æŒ‡å®šå¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼å…¬é–‹æœå‹™ï¼š ClusterIP (default)ï¼šåªæœ‰å…§éƒ¨ IPï¼Œåªèƒ½å¾é›†ç¾¤å…§è¨ªå•æœå‹™ã€‚ NodePortï¼šå·¥ä½œæ–¼æ¯å€‹ç¯€é»çš„ä¸»æ©Ÿ IP ä¹‹ä¸Šï¼Œå¯ä»¥å¾é›†ç¾¤å¤–éƒ¨è¨ªå•æœå‹™ã€‚ClusterIP çš„è¶…é›†ã€‚ LoadBalancerï¼šåœ¨ç•¶å‰é›†ç¾¤ä¸­å‰µå»ºä¸€å€‹å¤–éƒ¨è² è¼‰å¹³è¡¡å™¨ï¼ŒæŠŠå¤–éƒ¨è«‹æ±‚è² è¼‰å‡è¡¡è‡³å¤šå€‹Node ä¸»æ©Ÿ IP çš„ NodePort ä¹‹ä¸Šï¼Œç‚ºè©²æœå‹™åˆ†é…ä¸€å€‹å›ºå®šçš„å¤–éƒ¨ IPã€‚NodePort çš„è¶…é›†ã€‚ ExternalName-externalNameï¼šé€šéåœ¨ Cluster çš„ DNS Server æ·»åŠ ä¸€ç­† CName Recordï¼Œä½¿ç”¨æŒ‡å®šåç¨±ï¼ˆåœ¨ yaml ä¸­è¨­å®šï¼‰å…¬é–‹æœå‹™ï¼Œä¸ä½¿ç”¨ä»£ç† (kube-proxy)ï¼Œè€Œæ˜¯é€é kube-dnsã€‚æ­¤é¡å‹åœ¨ kubernetes 1.7 ç‰ˆæœ¬æœ‰æä¾›ï¼Œä½†æ˜¯è¦ kube-dns version è¦åœ¨ 1.14.9 ä»¥ä¸Šï¼Œå¦å‰‡æœƒé‡åˆ° Resolve External Name issueã€‚ä¸»è¦æ˜¯ç‚ºäº†è®“ä¸åŒ namespace ä¸­çš„ Service å¯ä»¥åˆ©ç”¨ ExternalName è¨­å®šçš„å¤–éƒ¨åç¨±é€£åˆ°å…¶å®ƒçš„ namespace ä¸­çš„ Serviceã€‚ Label\næ¨™ç±¤ç”¨æ–¼å€åˆ†å°è±¡ï¼Œä½¿ç”¨æ¨™ç±¤å¼•ç”¨å°è±¡è€Œä¸å†æ˜¯ IPã€‚Label ä»¥éµå€¼å°çš„å½¢å¼å­˜åœ¨ï¼Œæ¯å€‹å°è±¡å¯ä»¥æœ‰å¤šå€‹æ¨™ç±¤ï¼Œé€šéæ¨™ç±¤å¯ä»¥é—œè¯å°è±¡ã€‚ Volume\nå…±äº« Pod ä¸­ä½¿ç”¨çš„æ•¸æ“šã€‚ Namespace\nå¯ä»¥æŠ½è±¡ç†è§£ç‚ºä¸€ç¾¤å°è±¡çš„é›†åˆã€‚ High-level objects (Controllers) å»ºç«‹åœ¨åŸºæœ¬å°è±¡çš„åŸºç¤ä¸Šï¼Œæä¾›äº†é™„åŠ çš„åŠŸèƒ½å’Œä¾¿åˆ©æ€§ï¼š\nReplicaSetï¼šç¢ºä¿é‹è¡ŒæŒ‡å®šæ•¸é‡çš„ podï¼Œå®˜æ–¹å»ºè­°ä½¿ç”¨ Deployment ä¾†è‡ªå‹•ç®¡ç†ã€‚ Deploymentï¼šæœ€å¸¸è¦‹çš„éƒ¨å±¬ Pod çš„æ–¹æ³•ï¼Œå¯ä»¥ç”¨ä¾†å‰µå»º replicaSetã€‚æ”¯æŒç‰ˆæœ¬è¨˜éŒ„ã€rolling update/backã€æš«åœå‡ç´šç­‰é«˜ç´šç‰¹æ€§ã€‚ StatefulSetï¼šç”¨æ–¼ç®¡ç†å…·æœ‰æŒä¹…æ€§å­˜å„²çš„æœ‰ç‹€æ…‹æ‡‰ç”¨ç¨‹åºã€‚ æœ‰åºéƒ¨å±¬ã€æœ‰åºæ“´å±•ï¼šå³ Pod æ˜¯æœ‰é †åºçš„ï¼Œåœ¨éƒ¨ç½²æˆ–è€…æ“´å±•çš„æ™‚å€™è¦ä¾æ“šå®šç¾©çš„é †åºä¾æ¬¡ä¾æ¬¡é€²è¡Œï¼ˆå³å¾ 0 åˆ° N-1 ï¼Œåœ¨ä¸‹ä¸€å€‹ Pod é‹è¡Œä¹‹å‰æ‰€æœ‰ä¹‹å‰çš„ Pod å¿…é ˆéƒ½æ˜¯ Running å’Œ Ready ç‹€æ…‹ï¼‰ï¼ŒåŸºæ–¼ init containers ä¾†å¯¦ç¾ã€‚æ‰€ä»¥ pod name å›ºå®šä¸”æœƒå¸¶ä¸€å€‹æœ‰åºçš„åºè™Ÿ(app-0, app-1\u0026hellip;)ã€‚ æŒä¹…æ€§å„²å­˜ï¼šåŸºæ–¼ pvc å¯¦ç¾ï¼Œpod é‡æ–°èª¿åº¦å¾Œä»æœƒè·Ÿ volume ä¿æŒé—œè¯ï¼Œ volume ä¸æœƒéš¨ pod åˆªé™¤è€Œåˆªé™¤ã€‚ ç©©å®šç¶²è·¯æ¨™èªŒï¼šåŸºæ–¼ Headless Serviceï¼ˆå³æ²’æœ‰ Cluster IP çš„ Serviceï¼‰ä¾†å¯¦ç¾ï¼ŒPod é‡æ–°èª¿åº¦å¾Œå…¶ PodName å’Œ HostName ä¸è®Šã€‚ DaemonSetï¼šä¿è­‰åœ¨æ¯å€‹ Node ä¸Šéƒ½é‹è¡Œä¸€å€‹å®¹å™¨å‰¯æœ¬ï¼Œå¸¸ç”¨ä¾†éƒ¨ç½²ä¸€äº›é›†ç¾¤çš„æ—¥èªŒã€ç›£æ§æˆ–è€…å…¶ä»–ç³»çµ±ç®¡ç†æ‡‰ç”¨ã€‚ Jobï¼šä¸€æ¬¡æ€§ä»»å‹™ï¼Œé‹è¡Œå®Œå¾ŒPodéŠ·æ¯€ï¼Œä¸å†è‡ªå‹•é‡å»ºã€‚ CronJobï¼šå®šæ™‚ä»»å‹™ã€‚ Resource https://www.cnblogs.com/ccbloom/p/11311286.html#%E5%9F%BA%E7%A1%80%E5%AF%B9%E8%B1%A1 https://www.cnblogs.com/baoshu/p/13124881.html ","date":"2021-01-07T17:38:00Z","permalink":"http://localhost:1313/p/kubernets-object/","title":"Kubernetes Object åŸºæœ¬å°è±¡ä»‹ç´¹"},{"content":"é•·å¤§ä»¥å¾Œéƒ½æ²’æœ‰éçš„å…¨å®¶å‡ºåœ‹æ—…è¡Œï¼Œåœ¨é€™æ¬¡çµ‚æ–¼æˆè¡Œäº†ï¼å¾å…­æœˆæ±ºè­°ç©é–‹å§‹ï¼Œæˆ‘å°±æ“”ä»»å°éŠçš„è§’è‰²ï¼Œè‘—æ‰‹è¦åŠƒäº†æ•´å€‹æ—…ç¨‹ã€‚ä¾†ä¾†å›å›ä¿®æ”¹ä¸ä¸‹åæ¬¡ï¼Œç”šè‡³é€£å°æ‰‹å†Šéƒ½åšäº†ï¼Œé›–ç„¶æœ€å¾Œåªæœ‰è‡ªå·±åœ¨çœ‹ XD\né€™å¹¾å¤©å¤©æ°£éƒ½æ¥µå¥½ï¼Œç¬¬ä¸€å¤©æŠµé”çš„æ™‚å€™å¤šé›²ã€ç¬¬äºŒå¤©æ—©ä¸Šé£„é›¨å¤–å…¶é¤˜çš„éƒ½æ˜¯è±”é™½é«˜ç…§çš„è¶…ç´šå¥½å¤©æ°£ï¼ŒçœŸçš„å¾ˆå¹¸é‹ï¼Œå› ç‚ºé«˜ä¸­æœ‹å‹æ—©æˆ‘ä¸€å€‹ç¦®æ‹œå»å‰›å¥½é‡åˆ°æ¯å¤©éƒ½åœ¨ä¸‹é›¨çš„é¢±é¢¨(QQ)ï¼Œå‡ºç™¼å‰ä¸€å€‹ç¦®æ‹œæ¯å¤©éƒ½ä¸æ™‚çš„çœ‹å››äº”å€‹ä¸åŒçš„æ°£è±¡é å ±å‘¢ï¼Œè¬å¹¸è¬å¹¸ã€‚\nDAY1 ç¬¬ä¸€å¤©åæ—©ä¸Šå…­é»åŠçš„æ¨‚æ¡ƒèˆªç©ºï¼Œæˆ‘åå‡Œæ™¨çš„å®¢é‹åˆ°æ©Ÿå ´ï¼Œæœ¬ä¾†æ‰“ç®—åœ¨æ©Ÿå ´ç¡ä¸€ä¸‹çš„ï¼Œä½†å†·çˆ†ã€‚æ‹”æ˜¨æ™šå¾å¤§é™¸é£›åˆ°æ¡ƒåœ’ç­‰ï¼Œéº»è·ŸäºŒå§Šå‰‡å¾å¤§æ—é–‹è»ŠåŒ—ä¸Šé †é“è¼‰ï¼›å®Œå…¨æ²’ç¡ï¼ŒçœŸåç¬¦å…¶å¯¦çš„ç´…çœ¼ç­æ©Ÿä¸« \u0026#x1f62a; æœ€å–œæ­¡æº–å‚™å‡ºç™¼çš„æ™‚å€™ï¼Œèˆˆå¥®å¾—é›£ä»¥è¨€å–»ï¼Œç„¶å¾Œå°±è¼‰æ›è¡Œæç®±çš„æ™‚å€™æŠŠæ‰‹æ©Ÿå¿˜è¼‰å ±åˆ°æ«ƒå°äº†å“ˆå“ˆå“ˆè¶…ç´šçƒé¾ã€‚å¹¸å¥½åœ¨æ™‚é–“æœ‰é¤˜çš„èµ·é£›å‰ç™¼ç¾äº†ï¼Œæœ‰é©šç„¡éšªåœ°æ‹¿å›ä¾†ã€‚\næ—¥æœ¬æ¯”å°ç£å¿«ä¸€å€‹å°æ™‚ï¼Œæ‰€ä»¥ç•¶åœ°æ™‚é–“ä¹é»æŠµé”é‚£éœ¸æ©Ÿå ´ï¼ŒåŸæœ¬ä»¥ç‚ºå·²ç¶“åšå¥½è¬å…¨æº–å‚™äº†ï¼Œä½†æŠµé”æ™‚é‚„æ˜¯ç™¼ç¾å¿˜äº†æéŒ¢è²·å¥½æ™¯é»å¥—ç¥¨ï¼Œåœ¨æ©Ÿå ´è£œè²·ç¥¨æ™‚ï¼Œå§Šä»–å€‘å»è²·å¿…åƒçš„è±¬è‚‰è›‹é£¯ç³°ï¼Œåƒäº†æ˜å¤ªå­è·Ÿå±±è‹¦ç“œå£å‘³ï¼ŒçœŸçš„å°±å¦‚è©•åƒ¹èªªçš„å¥½åƒï¼Œå¤§å®¶éƒ½å¾ˆå–œæ­¡ï¼Œæ‹”é‚„èªªå›ç¨‹å¯ä»¥å†ä¾†è²· \u0026#x1f601; ç§Ÿè»Š åƒå®Œå¾Œå» ORIX æ¥é§çš„åœ°é»åå»ç‡Ÿæ¥­è™•å–è»Šï¼ˆæå‰åœ¨ TABIRAI æ—¥æ–‡ç¶²é å®šçš„ï¼Œé‚„æ¯”è¼ƒåˆ’ç®—å‘¢ \u0026#x1f44d;ï¼‰ç§Ÿçš„è»Šæ˜¯ NISSAN çš„äº”äººåº§ä¼‘æ—…è»Šï¼Œæ¯”å®¶è£¡çš„ outlander å°ä¸€é»ï¼Œä½†åœ¨æ²–ç¹©ä¾†èªªå¯æ˜¯è¶…ç´šå¤§è»Šå•Šï¼æ”¾çœ¼æœ›å»å¹¾ä¹ä¸ƒæˆçš„è»Šéƒ½æ˜¯æ­£æ–¹å½¢å±è‚¡çš„å°è»Šã€‚å‡ºç™¼å¾Œå…ˆå»éš”å£çš„ OTS è²·ä¾¿å®œçš„ç¬¬å››å¤©çš„ç‰æ³‰æ´é–€ç¥¨å¾Œå°±æ­£å¼å‡ºç™¼äº†ã€‚æ²’é–‹éå³é§•çš„æ‹”é¦–æ¬¡ä¸Šè·¯ä¹Ÿæ˜¯æ‰‹å¿™è…³äº‚çš„ï¼Œåˆ‡æ–¹å‘ç‡ˆéƒ½æœƒä¸å°å¿ƒé–‹åˆ°é›¨åˆ· \u0026#x1f602; è€Œä¸”åœè»Šçš„æ™‚å€™å‰å…©å¤©ä¹Ÿæ˜¯å¾ˆä¸ç¿’æ…£(ä½†å¾Œé¢æ„ˆåœæ„ˆé †)ï¼Œç„¶å¾Œå›ºå®šå‰¯é§•é§›å°éŠçš„æˆ‘åˆ°ç¬¬å››å¤©ä¹Ÿéƒ½é‚„æ˜¯å¿ƒé©šè†½è·³çš„ï¼Œæ€•ç¿’æ…£å·¦åå› æ­¤æ“¦æ’åˆ°è·¯é‚Šé“è·¯(é›–ç„¶é‚„æ˜¯æœ‰æ“¦åˆ°è·¯é‚Šçš„ä¸‰è§’éŒè·Ÿä¸å°å¿ƒé–‹ä¸Šè·¯é‚Šçš„åï¼Œä½†å¹¸è™§éƒ½æ²’æœ‰åˆ®å‚·ï¼Œå¯å–œå¯è³€å¯æ­Œå¯æ³£ \u0026#x1f913;) å¤å®‡åˆ©å³¶ å¤å®‡åˆ©å³¶åœ¨åŒ—é‚Šï¼Œé€™å››å¤©è¨ˆç•«ç”±åŒ—æ…¢æ…¢ç©å›ä¾†ã€‚æ‰€ä»¥å»çš„è»Šç¨‹å¤§æ¦‚æœ‰ä¸€å€‹åŠå°æ™‚ï¼Œé€™ä¹Ÿæ˜¯å”¯ä¸€é–‹é«˜é€Ÿå…¬è·¯çš„è·¯ç¨‹ï¼Œä¸­é–“æœ‰è·¯éåˆ°ä¸€å€‹å¤§å‹ä¼‘æ¯ç«™ä¼‘æ¯ï¼Œçœ‹å¾—åˆ°ä¸€é»é»æµ·æ™¯ï¼Œæ²–ç¹©çœŸçš„æ²’æœ‰ä¸€è™•ä¸ç¾çš„ \u0026#x1f97a; å› ç‚ºæ¯”è¼ƒååƒ»ï¼Œæ‰€ä»¥èšŠèŸ²ç‰¹åˆ¥å¤šï¼Œä½†ä»¤äººæ„å¤–çš„æ˜¯å…¬å»éå¸¸ä¹¾æ·¨ï¼Œä¹Ÿçµ‚æ–¼é«”æœƒåˆ°æ—¥æœ¬æ•´æ½”çš„å…¬å…±å ´æ‰€ã€‚å¾ŒåŠç¨‹æ›æˆ‘é–‹è»Šï¼Œä¸éå·¦å³ç›¸åçœŸçš„è¶…é›£é©æ‡‰çš„å•Šå•Šå•Šï¼Œæˆ°æˆ°å…¢å…¢åœ°é–‹çš„åŠå°æ™‚å¾Œå°±åˆ°äº†å—è©°å±•æœ›æ‰€ã€‚\né€™é‚Šæ˜¯å°åœè»Šå ´ï¼Œå¾é€™å¯ä»¥çœ‹åˆ°æ•´åº§å»¶ä¼¸åˆ°å°å²¸çš„å¤å®‡åˆ©å¤§æ©‹ã€‚å¯ä»¥çš„æ˜¯æ—©ä¸Šçš„çƒé›²é‚„æ²’æ•£ï¼Œé‚„é£„äº†é»æ¯›æ¯›é›¨ï¼›å¹¸å¥½å‡ºç™¼å‰æœ‰åœ¨è¦çš®è²·äº†ä¸‰é ‚æ¼å¤«å¸½ï¼Œé‚„å¸¶äº† NIKE æ£’çƒå¸½ï¼Œæˆ´èµ·ä¾†ä¹Ÿéƒ½å¾ˆè®š \u0026#x1f60e; å› ç‚ºæ‰å‰›åœ¨æ©Ÿå ´åƒé£¯ç³°ï¼Œæ‰€ä»¥ä¸­é¤å°±ç›´å¥”æœ€æœ‰åçš„è¦è¦é£¯ã€‚æ­£å¥½å¤©æ°£ä¹Ÿæ²’æœ‰å¾ˆç†±ï¼Œä¸ç„¶åŸå…ˆæ˜¯æ‰“ç®—æ‰¾ä¸€é–“é¤å»³ï¼Œä¸éè¨ˆåŠƒç¸½æ˜¯è¶•ä¸æ˜¯è®ŠåŒ–ï¼Œä¾†å°±æ˜¯è¦ç•¶å€‹ç¨±è·çš„è§€å…‰å®¢å•Š!!!é€£é»é¤ä¹Ÿä¸ç®¡è©•åƒ¹å¤šèŠ±äº†å¹¾ç™¾å¡Šè²·è¾£å‘³ï¼ŒçµæœçœŸçš„åªå¤šäº†è¾£ç²‰ï¼Œè¶…ä¸å€¼ XDDDD (äº‹å¾Œé‚„æŠŠå…©ç½ç”¨å‰©çš„è¾£ç²‰å¾…åœ¨è»Šä¸Šåˆ°ç¬¬å››å¤©é‚„æ˜¯å®Œå¥½å¦‚åˆçš„æ‰”äº†å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ) ä¸éè¦è¦é£¯æœç„¶é‚„æ˜¯ä¸æ„§å…¶åçš„å¥½åƒï¼ŒåŠ é»çš„ç‰›è‚‰ä¹Ÿè »å«©çš„ï¼Œå¹¸å¥½æœ‰ä¾†ï¼Œå› ç‚ºéº»å¾ˆæœŸå¾…åƒ(å°é˜¿å§¨ä¹Ÿæ˜¯ä¸ä¹…å‰å…¨å®¶ä¾†ç©éæœ‰ç‰¹åœ°è·Ÿå¥¹æéè¦è¦é£¯ \u0026#x1f923; )ï¼Œä¸€èµ·ååœ¨äºŒæ¨“çš„è§€æ™¯éšæ¢¯ï¼Œé‚„èƒ½çœ‹åˆ°å¤å®‡åˆ©æ©‹çš„æµ·æ™¯ï¼Œå¤§å®¶éƒ½å¾ˆæ»¿æ„ \u0026#x1f496; \u0026#x1f496; æ¥è‘—ä¾†åˆ°å¤å®‡åˆ©æµ·æ´‹å¡”ï¼Œå¾å…¥å£è™•å°±å¯ä»¥çœ‹åˆ°é ‚æ¨“çš„å¹¸ç¦é˜ã€‚é›–ç„¶é›²é‚„æ²’é–‹ï¼Œä½†éƒ½é€™éº¼é çš„é–‹è»Šä¾†åè­·äº†ï¼Œå°±ç®—è©•åƒ¹èªªæ˜¯æ»¿æ»¿å•†äººçš„åŒ…è£ï¼Œé‚„æ˜¯è¦ä¾†çœ‹ä¸€ä¸‹çš„ã€‚åç„¡äººè»Šç¹è‘—å¡”ä¸€åœˆåœˆçš„ä¸Šå¡”é ‚ï¼Œå¾ˆæ–°é®®æœ‰è¶£ï¼Œæ²¿é€”éƒ½æ˜¯å„ç¨®å„æ¨£æ•´å¾—æ¼‚äº®çš„æµ·å³¶æ¤ç‰©ï¼Œä¸€é‚Šé‚„èƒ½è½æ‹”è¬›è§£ XDã€‚ä¸‹è»Šå¾Œå…ˆæ˜¯åƒè§€ç³ç‘¯æ»¿ç›®çš„è²æ®¼é¤¨ï¼Œåœ¨èµ°é“ä¸‰å››æ¨“çš„è§€æ™¯å°çœ‹é¢¨æ™¯ã€æ•²æ•²å¹¸ç¦é˜ã€‚é‚„åƒè€ƒ google maps è©•åƒ¹çš„å»ºè­°è²·äº†å¥½åƒçš„å—ç“œå¯é Œå’Œæ³¡èŠ™åƒ \u0026#x1f950; \u0026#x1f60b; é–‹ä¸Šå¤å®‡åˆ©æ©‹æº–å‚™é›¢å³¶æ™‚ï¼Œå¤©ç©ºçµ‚æ–¼å®Œå…¨æ”¾æ™´äº†!!! \u0026#x2600;\u0026#xfe0f; èƒ½åœ¨æ©‹ä¸Šçœ‹åˆ°è—å¤©ç™½é›²ã€é…ä¸Šç­†ç›´å¯¬é—Šçš„å¤§æ©‹ï¼ŒçœŸçš„è¶…ç´šè¶…ç´šå¹¸é‹! \u0026#x1f64c; ç¾éº—æµ·æ°´æ—é¤¨ ç¬¬ä¸€å¤©çš„æœ€å¾Œä¸€ç«™æ˜¯åŸå…ˆæ²’æœ‰è¦å»çš„ç¾éº—æµ·æ°´æ—é¤¨ï¼Œä½†å¹¸å¥½è¡Œå‰è‡¨æ™‚æŠŠ ORION é…’å» æ›æ‰äº†ï¼Œæ°´æ—é¤¨æœç„¶æ˜¯å€‹ç™¾çœ‹ä¸è†©åˆç™‚ç™’çš„åœ°æ–¹ã€‚å› ç‚ºå…¥å ´å·²ç¶“å››é»äº†ï¼Œæ‰€ä»¥é‚„èƒ½é€€æ˜Ÿå…‰ç¥¨çš„åƒ¹å·®ï¼Œè¶…ç´šåˆ’ç®—!!äº”é»æœ‰ä¸€å ´é¯¨é¯Šé¤µé£Ÿç§€ï¼Œååœ¨æœ‰ä½ç½®çš„åŠ‡å ´è£¡é¢ï¼Œä½†å¾€å‰èµ°æ‰ç™¼ç¾æœ‰æ›´å¤§çš„æ™¯è§€ç»ç’ƒå¯ä»¥è§€è³ï¼Œæ‹”éº»éƒ½èªªè¢«è§£èªªå“¡çµ¦é¨™åäº† \u0026#x1f923; \u0026#x1f923; ä¸éèƒ½èˆ’æœçš„åè‘—é‚„æœ‰ä¸€äº›å°ˆæ¥­çš„è§£èªªä¹Ÿæ˜¯å¯ä»¥çš„å•¦ã€‚å‡ºé¤¨ä¹‹å¾Œç·Šæ¥è‘—åˆ°å®¤å¤–çš„æµ·è±šåŠ‡å ´ï¼Œäº”é»åŠçš„å¤ªé™½é‚„æ˜¯è¶…å¤§ï¼Œä½†æ¯”ä¸­ä¸‹åˆå¥½å¾ˆå¤šäº†ï¼Œè€Œä¸”å‚æ™šçš„å ´æ¬¡äººä¹Ÿä¸æœƒè¶…çˆ†ï¼Œå®‰æ’å¾—å¾ˆå®Œç¾ \u0026#x1f973; æµ·è±šæœ‰å¾ˆå¤šæ‰è—ï¼ŒéŸ³æ¨‚ä¹Ÿéƒ½ä¸‹å¾—æ°åˆ°å¥½è™•ï¼Œæ•´å ´è¡¨æ¼”éƒ½æ²’å†·å ´ï¼Œéå¸¸ç²¾é‡‡ï¼æ‹”éº»é‚„èªªæ¯”ä¹‹å‰å»åŒ—æµ·é“çœ‹çš„æµ·è±šè¡¨æ¼”é‚„ç²¾å½©å‘¢ \u0026#x1f633; ä¸éæ°´æ—é¤¨æ¯”æƒ³åƒä¸­å°å¾ˆå¤šï¼Œæ‰€ä»¥çœ‹åˆ°åŸæœ¬æ‡‰è©²è‡ªç”±çš„åœ¨å¤§æµ·å„ªæ¸¸çš„é¯¨é¯Šå’Œå¯æ„›çš„æµ·è±šå€‘ï¼Œåœ¨è®šå˜†å’ŒæŒè²ä¸‹é‚„æ˜¯ä¸ç”±è‡ªä¸»åœ°å¸Œæœ›ä»–å€‘èƒ½é–‹é–‹å¿ƒå¿ƒçš„ï¼Œä¸‹è¼©å­å¦‚æœé‚„æ˜¯é­šï¼Œé‚£å†€æœ›ç‰ å€‘éƒ½èƒ½é¨éŠåœ¨å»£é—Šçš„å¤§æµ·ä¸­ \u0026#x1f97a; D1 æ™šé¤ \u0026amp; æ°‘å®¿ æ™šé¤åŸå…ˆæ˜¯æƒ³å»é™„è¿‘å¾ˆæœ‰åçš„å³¶è±šå®¶åƒå¾ˆå²å®³çš„æ²–ç¹©ç‡’è‚‰éºµï¼Œä½†å®‰æ’çš„æ™‚å€™å¿˜äº†æ³¨æ„ä»–å€‘åªç‡Ÿæ¥­åˆ°ä¸­åˆï¼å¤ªå¯æƒœäº†ã€‚åªå¥½å…ˆä¸Šè·¯å»ç¬¬ä¸€æ™šçš„æ°‘å®¿ï¼Œç‚ºäº†éš”å¤©æ—©èµ·æµ®æ½›æ–¹ä¾¿ï¼Œæ‰€ä»¥ç¬¬ä¸€æ™šçš„è½è…³è™•ç›´æ¥é¸åœ¨æ©ç´ï¼Œè»Šç¨‹éœ€è¦ä¸€å€‹åŠå°æ™‚ï¼Œä¸­é€”æˆ‘é‚„çœ‹éŒ¯å°èˆª \u0026#x1f613;ï¼Œæ‰çµ‚æ–¼åˆ°ã€‚è¶…ï¼é¤“ï¼ä¸éé™„è¿‘æ¯”è¼ƒååƒ»ï¼Œæ‰€ä»¥æ™šé¤åªå¥½åƒé™„è¿‘çš„å‰é‡å®¶ï¼Œé›–ç„¶æ¯”èµ·å°ç£çš„æœ‰è±å¯Œçš„é¸æ“‡ï¼Œä½†è·ŸåŸå…ˆè¨ˆç•«çš„ç‡’è‚‰éºµæœ‰å¾ˆå¤§çš„è½å·®å•Š \u0026#x1f62d; åƒé£½å¾Œé‚„èª¤é—–éš”å£å¤–è§€çœ‹èµ·ä¾†å¾ˆ fancy çš„å°é‹¼ç åº—ï¼Œè£¡é¢çœŸçš„è¶…èª‡å¼µï¼Œå¤§æ¦‚æœ‰ä¸Šç™¾å°æ©Ÿæª¯è€Œä¸”åº§ç„¡è™›å¸­ï¼Œå®Œå…¨å¤§é–‹çœ¼ç•Œã€‚ç¬¬ä¸€æ™šçš„æ°‘å®¿æ˜¯ä¸‰æ™šä¸‹ä¾†æœ€ä¾¿å®œçš„ï¼Œä¾¿å®œåˆ°æˆ‘è¨‚åˆ°å››äººæˆ¿éƒ½ä¸çŸ¥é“ \u0026#x1f974;\nDAY2 éš”å¤©åŸæœ¬è¨ˆç•«å¥½è¦äº”é»åŠå·¦å³èµ·åºŠï¼Œæ®Šä¸çŸ¥èµ·ä¾†çš„æ™‚å€™å·²ç¶“å…­é»åŠäº†ï¼Œæ€¥å¿™åœ°æ”¶æ‹¾è¡Œææ›è£ï¼Œæ‹èœœç²‰åœ¨æ²’æ´—é ­çš„æ²¹ç”°ä¸Š \u0026#x1f92a; \u0026#x1f60f; å†æŠ½ç©ºè·‘å»çœ‹æ‹”éº»èµ·åºŠäº†æ²’ï¼Œçµæœä¹Ÿæ²’æœ‰å“ˆå“ˆå“ˆ \u0026#x1f602; ç´„è«å¿«ä¸ƒé»æ‰å¥½ä¸å®¹æ˜“åˆ°å¤§å»³é€€æˆ¿åƒæ—©é¤ï¼Œæ°‘å®¿æœ‰æä¾›å…è²»æ—©é¤(æ²³ç²‰ã€æ³•åœ‹éºµåŒ…å¤¾è‚‰ã€ç²¥)ï¼Œä»¤äººæ„å¤–çš„æ˜¯ï¼Œé›–ç„¶çœ‹èµ·ä¾†ç°¡å–®å»éå¸¸ç¾å‘³(å…¨å®¶ä¸€è‡´èªåŒ)ï¼ä¸éé‚Šåƒé‚„é‚Šåœ¨æ“”å¿ƒè¦ä¸è¦æ‰“çµ¦æµ®æ½›çš„åº—å®¶èªªè¦å»¶å¾Œä¸€å€‹æ¢¯æ¬¡ï¼Œä½†å¹¸å¥½å‚™é¤é€Ÿåº¦å¾ˆå¿«ï¼Œæˆ‘å€‘ä¹Ÿè¿…é€Ÿåœ°è§£æ±ºå¾Œå‡ºç™¼äº†ï½\næµ·åº•æ¼«æ­¥ è»Šç¨‹åƒ…ååˆ†é˜ï¼ˆæ˜¨å¤©å¿è‘—é¤“èŠ±æ™‚é–“é–‹å¤œè»Šæ˜¯å°çš„ \u0026#x1f918;ï¼‰æŠµé”æ™‚æ­£å¥½å…«é»ï¼Œå¾ˆå¹¸é‹åœ°æ²’æœ‰çœŸçš„é²åˆ°ã€‚æ”¾å¥½æ±è¥¿ä¸¦è‘—è£å®Œç•¢å¾Œï¼Œå…ˆç”±ä¸€ä½å°ç£æ•™ç·´å¸¶æˆ‘å€‘åˆ°å‡ºæµ·å£ï¼Œä¸€é‚Šè¬›è§£æµ·åº•æ¼«æ­¥çš„æ³¨æ„äº‹é …ï¼Œå†ç”±å…©ä½æ—¥æœ¬æ•™ç·´é™ªåŒå‡ºæµ·ã€‚æ—©ä¸Šå¤©æ°£æ²’æœ‰å¤ªé™½é‚„æœ‰æ¯›æ¯›é›¨ï¼Œä¸éå¹¸å¥½æ˜¯æ°´ä¸Šæ´»å‹•ï¼Œæ‰€ä»¥ä¸å—å½±éŸ¿ \u0026#x270c;\u0026#xfe0f; åè‘—å¿«è‰‡(å§?)ï¼Œè¢«å¤§æµ·åŠæµ·é¢¨ç’°ç¹ï¼Œæ•´è‘—äººéƒ½å¿ƒæ› ç¥æ€¡äº†èµ·ä¾†ï¼Œä¹Ÿèƒ½æ„Ÿå—åˆ°å¤§å®¶çš„å¥½å¿ƒæƒ…ã€‚å¤§ç´„ååˆ†é˜å°±åˆ°äº†ç›®çš„åœ°ï¼Œä¸‹å»å‰éº»è¶…ç´šç·Šå¼µï¼Œä½†é‚„æ˜¯é¼“èµ·å‹‡æ°£ä¸‹å»äº† \u0026#x1f4aa;\næµ·åº•æ¼«æ­¥æ˜¯ç”¨å¾ˆé‡çš„é ­ç›”ç½©ä½é ­ï¼Œåœ¨é ­ç›”è£¡æœ‰ç©ºæ°£å¯ä»¥å‘¼å¸ï¼Œè…°ä¸Šæœƒç¶å¾ˆé‡çš„çŸ³é ­é¿å…èº«é«”å¤ªè¼•ã€‚ä¸‹å»å¾ŒçœŸçš„æ˜¯è¢«æ»¿å‘æ»¿è°·çš„é­šåœç¹ä½ï¼Œå°¤å…¶æ˜¯æ‹¿è‘—éºµåŒ…åœ¨é¤µé­šçš„æ™‚å€™!!!è¶…ç´šç¥å¥‡ã€‚é›–ç„¶å°ç£æ•™ç·´æœ‰èªªä»–å€‘è¦ºå¾—æµ·åº•æ¼«æ­¥å¾ˆä¸å€¼å¾— XD ä½†çœ‹å¤§å®¶ç¬‘å¾—å¾ˆé–‹å¿ƒå°±è¦ºå¾—è¶…ç´šå€¼å›ç¥¨åƒ¹ï¼Œè€Œä¸”é€™å€‹æ¢¯æ¬¡çš„æµ·åº•æ¼«æ­¥åªæœ‰æˆ‘å€‘~~ç„¡æ•µæ£’ \u0026#x1f970; æ‹äº†å¹¾å¼µèªè­‰ç…§ä¹‹å¾Œæˆ‘å€‘å°±çµæŸä¸ŠåŠå ´äº†ï¼Œä¸Šä¾†çš„æ™‚å€™å› ç‚ºå£“åŠ›è½‰æ›é‚„æœƒæœ‰é»è€³é³´è·Ÿé ­æšˆï¼Œåœ¨å›å²¸çš„è·¯ä¸Šæ•™ç·´é‚„å€’äº†éº¥èŒ¶çµ¦æˆ‘å€‘å–ã€‚ä¹‹å¾Œèµ°å›åˆ°æ½›æ°´åº—ç­‰ä¸‹åŠå ´æµ®æ½›ï¼Œå¾ˆå¹¸é‹åœ°åœ¨å‡ºç™¼å‰å¤©ç©ºé–‹å§‹æ”¾æ™´äº†ï¼Œè€Œä¸”é‚„æ˜¯è—å¤©ç™½é›²å¤§å¤ªé™½çš„å¥½å¤©æ°£ï¼åœ¨èˆ¹ä¸Šæ•™ç·´é‚„èªªé€™å¯æ˜¯ç½é•å…©å‘¨èƒ½ä¸‹æ°´ä¸¦é€²åˆ°é’ä¹‹æ´çªŸçš„ç¬¬ä¸€å¤©å‘¢ï¼Œä¸Šå…©å‘¨å› ç‚ºé¢±é¢¨æ‰€ä»¥å®Œå…¨ä¸èƒ½ç©æ°´ä¸Šæ´»å‹•ï¼Œæ—…é‹å¤§çˆ†ç™¼ã€‚é€™æ¬¡åè‘—å¿«ä¸€é»çš„èˆ¹(æ‡‰è©²å°±æ˜¯å¿«è‰‡äº†XD)é‚„é…è‘—è¦è¶´çš„éŸ³æ¨‚å‡ºæµ·ï¼Œèˆ¹ä¸Šæ»¿æ»¿çš„éŠå®¢åœ¨åŒæ¢¯æ¬¡ï¼Œä¸åƒæµ·åº•æ¼«æ­¥é‚£æ™‚å€™çˆ½çˆ½çš„åŒ…èˆ¹ \u0026#x1f923;ï¼Œä¸ä¸€æœƒä¾¿åˆ°äº†é’ä¹‹æ´çªŸå‰ã€‚å› ç‚ºæµ®æ½›çš„æ·±åº¦è·Ÿæµ·åº•æ¼«æ­¥å·®å¾ˆå¤šï¼Œå¤§æ¦‚æœ‰äºŒä¸‰åå…¬å°ºæ·±äº†ï¼Œéº»å¯¦åœ¨å¤ªå®³æ€•ï¼Œä¸éæ‹”ä¹Ÿéƒ½ä¸€ç›´å¾…åœ¨æ—é‚Šï¼Œè®šè®šã€‚ç¬¬ä¸€æ¬¡æµ®æ½›ï¼Œå¾€ä¸‹çœ‹å„å¼å„æ¨£çš„é­šé‚„æœ‰çŠç‘šç¤ï¼Œç¾ä¸å‹æ”¶ï¼å‰›é–‹å§‹çš„æ™‚å€™é‚„æœ‰é»æŠ“ä¸åˆ°è¨£ç«…ï¼Œä¸éå¾Œä¾†å°±æ…¢æ…¢ç¿’æ…£äº†ã€‚é€™æ™‚æ®µçš„éŠå®¢å¾ˆå¤šï¼Œæ¸¸çš„æ™‚å€™é‚„æœƒäº’ç›¸è¸¢åˆ°ï¼Œä½†èƒ½ä¸‹æ°´ä¸¦ä¸”é€²åˆ°é’æ´å·²ç¶“æ˜¯å¾ˆæ»¿è¶³çš„äº‹äº†ï¼çµæŸå‰é‚„é¤µäº†ä¸€æ¬¡é­šï¼Œæ•™ç·´ä¹Ÿç›´æ¥æŠŠé˜²æ°´ç›¸æ©Ÿä¸Ÿçµ¦æˆ‘å€‘æ‹ï¼ˆé›–ç„¶ç•«è³ªå¾ˆçˆ›ï¼Œæ‰€ä»¥é’ä¹‹æ´çªŸè£¡çš„ç…§ç‰‡å¾ˆå¤šéƒ½è¶…ç³Š \u0026#x1f605;ï¼‰ä¸éæœ€é‡è¦çš„æ˜¯èƒ½è®“æ‹”éº»åœ¨ 60 æ­²å‰ç•™ä¸‹é›£å¿˜çš„æµ®æ½›å›æ†¶ï¼Œé€™ä¸‰åƒå¡Šä¸€å€‹äººçš„è²»ç”¨çœŸçš„è¶…å€¼å›ç¥¨åƒ¹çš„ï¼çµæŸæµ®æ½›å¾Œï¼Œä¸Šä¾†ç”¨æœ€è¿…é€Ÿçš„é€Ÿåº¦æ´—æ¾¡ï¼ŒåŒæ¢¯æ¬¡çš„æ—¥æœ¬äººå’Œä¸­åœ‹å¥³ç”Ÿç«Ÿç„¶ä¸Šä¾†çš„æ™‚å€™é ­é«®æ˜¯ä¹¾çš„ï¼Œå¯è¦‹æˆ‘å€‘ç©å¾—å¤šå¾¹åº•\u0026#x1f606; ç‰çƒã®ç‰› ä¸­é¤æ’äº†é™„è¿‘æˆ‘æœŸå¾…å·²ä¹…çš„å’Œç‰›ç‡’çƒ¤å•¦ï½ç¶²è·¯ä¸Šèªªç‰çƒã®ç‰›å’Œç¬¬ä¸‰å¤©æ™šä¸Šè¦åƒçš„é˜¿å¤è±¬ç«é‹ï¼ˆé£Ÿå½©é…’æˆ¿ï¼‰å¾ˆé›£æ’ï¼Œå¿…é ˆè¦å…ˆè¨‚ä½ï¼Œç„¡æ„é–“åœ¨çœ‹è©•è«–çš„æ™‚å€™çŸ¥é“äº†ä¿¡ç”¨å¡ç§˜æ›¸çš„æœå‹™ï¼Œä¾¿åœ¨å‡ºç™¼å‰ä¸€å€‹ç¦®æ‹œä½¿ç”¨ï¼Œæ‰“å¾—é‚„ç¨å¾®æœ‰é»æ™šäº†ï¼Œå› ç‚ºé ç´„çµæœè¦ç­‰åˆ°ä¸‰åˆ°äº”å€‹å·¥ä½œå¤©ï¼Œå¹¸è™§å‡ºç™¼å…©å¤©å‰æ”¶åˆ°è¨‚ä½æˆåŠŸçš„é€šçŸ¥ \u0026#x1f97a; å¾æµ®æ½›åº—åˆ°é¤å»³åªè¦è»Šç¨‹äº”åˆ†é˜ï¼Œä¸€é€²åº—å…§è¦è„«é‹ï¼ŒæŠŠé‹å­æ”¾åœ¨ç‰¹è£½çš„ä»¥æœ¨ç‰Œç•¶é‘°åŒ™çš„æœ¨é‹æ«ƒã€‚åƒäº†å…©ä»½åˆé¤ç‰¹é¤ã€ä¸‰ä»½ç‰›äº”èŠ±ã€ä¸€ä»½ç‰›èˆŒï¼›çš„ç¢ºæ˜¯æœ‰å…¥å£å³åŒ–æ„Ÿï¼Œæœ‰æ©Ÿæœƒè¦çœŸçš„è©¦è©¦çœ‹ A5 å’Œç‰›ã€‚ä¸€å€‹äººå¤§æ¦‚å¿«äº”åƒæ—¥å¹£ï¼Œè®“æ‹”è«‹å®¢ï¼Œå¿ƒæ»¿æ„è¶³ \u0026#x1f913; AEON MALL ä¸‹åˆçš„è¡Œç¨‹æ˜¯ AEON MALLï¼Œä¸éæœç„¶è·Ÿæƒ³åƒä¸­ä¸€æ¨£ï¼Œå› ç‚ºæ‹”éº»å¤ªç´¯äº†å°±å¹¾ä¹å¾…åœ¨ä¼‘æ¯å€ä¼‘æ¯ï¼Œåªæœ‰æˆ‘è·ŸäºŒå§Šé€›ï¼Œä½†æ€•å¥¹å€‘ç­‰å¤ªä¹…å°±å¾ˆå¿«çµæŸäº†ã€‚æ‰€ä»¥åœ¨é€™é‚Šçš„æˆ°åˆ©å“å°±å¾ˆå°‘ï¼Œåªæœ‰åœ¨ Uniqlo è²·å¤–å¥—ã€äºŒå§Šè²·äº†é›™é‹å­ï¼Œé‚„å–äº†æ˜Ÿå·´å…‹ï¼Œå°±çµæŸé€™å›åˆäº†ã€‚ D2 æ™šé¤ \u0026amp; æ°‘å®¿ ç¬¬äºŒæ™šçš„ä½çš„åœ°æ–¹åœ¨ç¾åœ‹æ‘ï¼Œé£¯åº—å°±å¦‚åŒæœŸå¾…çš„ä¸€æ¨£æ¼‚äº®ï¼å¹¸å¥½æˆ‘å€‘æ—©æ—©å…¥ä½ï¼Œé‚„æœ‰æ©Ÿæœƒçœ‹è‘—å¤ªé™½ä¸‹å±±ï¼Œå¾æˆ¿é–“çœ‹å‡ºå»çš„æµ·æ™¯è¶…æ¼‚äº®ï¼Œé‚„è·Ÿå§Šä¸€èµ·æ‹äº†è¶…å¤šç¶²ç¾ç…§ \u0026#x1f60e; å¹¸å¥½ç•¶åˆåœ¨æŒ‘é£¯åº—çš„æ™‚å€™æ›äº†å¾ˆå¤šå®¶ï¼Œæ‰åˆä¸‹å®šæ±ºå¿ƒå¤šèŠ±ä¸€é»è¨‚é€™é–“æµ·æ™¯æˆ¿ï¼èˆ’æœçš„ä¼‘æ¯äº†ä¸€ä¸‹ï¼Œä¾¿å‡ºç™¼å»åƒæ™šé¤äº†ï½æœ¬ä¾†æ™šé¤è¦å®‰æ’åœ¨é™„è¿‘æœ‰åçš„è¿´è½‰å£½å¸æˆ–æ‹‰éºµï¼Œä½†éƒ½ä¸å·§çš„å…¬ä¼‘ã€‚è®Šæ›æˆå°ç£ä¹Ÿæœ‰çš„è—å£½å¸ï¼Œä¸éæˆ‘å€‘ä¹Ÿéƒ½é‚„æ²’åƒé \u0026#x1f44f; å¾é£¯åº—èµ°éå»è¦ 20 åˆ†é˜ï¼Œç¾åœ‹æ‘çœŸçš„å°±åƒç¶²è·¯ä¸Šèªªçš„ä¸€æ¨£å®Œå…¨æ²’æ±è¥¿ \u0026#x1f923; æˆ‘å€‘å°±æ…¢æ…¢æ•£æ­¥éå»ï¼Œé«”é©—ä¸€èˆ¬æ—¥æœ¬çš„è¡—é“ä¹Ÿæ˜¯ä¸éŒ¯ã€‚åŸæœ¬æœ‰ç‰¹åˆ¥åœ¨ç¶²è·¯ä¸Šè¨‚ä½ï¼Œä½†å¤ªæ‚ é–’åœ°å‡ºç™¼äº†ï¼Œæ‰€ä»¥éŒ¯éæ™‚é–“ï¼Œå¹¸å¥½ä¸ç”¨ç­‰å¤ªä¹…å°±æ›æˆ‘å€‘äº†ã€‚ç¬¬ä¸€æ¬¡ç©æœ‰åçš„è—å£½å¸æ‰­è›‹å¾ˆå¥½ç©ï¼Œé›–ç„¶æ‰­è›‹åªä¸­ä¸€æ¬¡ã€‚ä¸éé‚„æ˜¯å¾ˆé–‹å¿ƒï¼Œä¸€èµ·å–äº†å†°æ¶¼çš„å•¤é…’ \u0026#x1f37a;ï¼Œä¸€èµ·ä¹¾æ¯ \u0026#x1f942; å›å»åœ°è·¯ä¸Šé †é“å»äº†å¤§æ¨¹è—¥å¦åº—ï¼Œé€›äº†ä¸€ä¸‹å¾Œå…ˆè®“æ‹”éº»å›é£¯åº—ä¼‘æ¯ï¼Œæˆ‘è·Ÿå§Šåˆå¤šå¤§è²·ç‰¹è²·ä¸€å…©å°æ™‚ \u0026#x1f4b8; DAY3 ç¬¬ä¸‰å¤©çš„è¡Œç¨‹æ¯”è¼ƒæ‚ é–’ï¼Œåƒå®Œé£¯åº—æº–å‚™åœ°è¶…ç°¡æ˜“æ—©é¤ï¼ˆå¯é Œæœæ±å’–å•¡ï¼‰å¾Œå°±å‡ºç™¼å»ä¸­åŸåŸäº†ã€‚\nä¸­åŸåŸ \u0026amp; é¦–é‡ŒåŸ æŠµé”å¾Œåº§æ¥é§è»Šåˆ°æœ€é«˜é»ï¼Œå†è®“éŠå®¢æ…¢æ…¢èµ°ä¸‹ä¾†ã€‚ä¸€æ—©è§€å…‰åœ°äººæ½®ä¸å¤šï¼Œç«™åœ¨çŸ³ç‰†ä¸Šå¯ä»¥çœ‹åˆ°æºªé‚Šçš„ä¸­åœ‹å—æµ·å’Œæ±é‚Šçš„å¤ªå¹³æ´‹ï¼Œæ™¯è§€éå¸¸å£¯éº—ã€‚è€Œä¸”çŸ³ç‰†é…ä¸Šä¸€ç‰‡è‰åœ°ä¹Ÿæ˜¯å¾ˆå¥½æ‹ç…§çš„æ™¯é»ã€‚\nç·Šæ¥è‘—çš„é¦–é‡ŒåŸä¹Ÿæ˜¯æ²–ç¹©å¿…åˆ°çš„å¤è¹Ÿä¹‹ä¸€ï¼Œä½†åœ¨é€™è£¡æ¯”è¼ƒå¯æƒœçš„æ˜¯æˆ‘æ’çš„è¡Œç¨‹æ ¹æœ¬æ²’èµ°å®Œå•Šå•Šå•Šï¼Œå¯¦åœ¨æ˜¯å¤ªç†±äº†ï¼Œè€Œä¸”æ—©ä¸Šé£¯åº—çš„æ—©é¤å¯¦åœ¨æ˜¯å¤ªéç°¡ä¾¿(ä¸»è¦æ˜¯æˆ‘è·Ÿå§Šå¤ªæ™šèµ·åºŠï¼Œæ‹”éº»åˆå¤ªæ—©åƒXD)ï¼Œæ‹”å°±èªªæƒ³åƒåˆé¤äº†ï¼Œä¾¿è‰è‰çµæŸé€™å€‹æ™¯é»å»ç‰§å¿—å¸‚å ´è·Ÿåœ‹éš›é€šäº†ã€‚å¦å¤–å°æ’æ›²æ˜¯æˆ‘å€‘ä¹æœˆæ‰å‰›é€ è¨ªé¦–é‡ŒåŸï¼Œåæœˆå°±é¦¬ä¸Šæœ‰æ–°èèªªè¢«ç‡’æ¯€ï¼ŒçœŸçš„å¾ˆå¯æƒœï¼Œä¹Ÿå¾ˆæ…¶å¹¸æˆ‘å€‘æœ‰èµ°éè·¯éæ²’æœ‰éŒ¯éã€‚ ç‰§å¿—å¸‚å ´ \u0026amp; åˆé¤ è·ŸäºŒå§Šéƒ½åƒéä¸€è˜­æ‹‰éºµï¼Œä½†æ‹”éº»æ²’åƒéï¼Œåœ¨åœ‹éš›é€šé‚Šé‚Šæ‰¾åœè»Šå ´åœå¥½è»Šå¾Œä¾¿ç›´ç›´åœ°å¾€ä¸€è˜­å»ã€‚ç”¨è‡ªå‹•è²©è³£æ©Ÿé»é¤ï¼Œä¸€äººåƒä¸€ç¢—æ‹‰éºµè·Ÿæº«æ³‰è›‹ï¼Œé‚„æ˜¯è·Ÿç¬¬ä¸€æ¬¡åœ¨å°ç£åƒçš„ä¸€æ¨£å¥½åƒ \u0026#x1f60b; åƒé£½å¾Œå»é€›å¸‚å ´è·Ÿåœ‹éš›é€šï¼Œåƒäº†äº‹å…ˆè¦åŠƒå¥½åœ°å¡©å±‹å†°æ·‡æ·‹ï¼Œé¡§åæ€ç¾©å°±æ˜¯æŠŠå„å¼å„æ¨£çš„é¹½æ’’åœ¨ç‰›å¥¶å†°æ·‡æ·‹ä¸Šé…è‘—åƒï¼Œé¹½çš„å£å‘³æœ‰ç•ªç´…èŠ±ã€å·§å…‹åŠ›ã€æŠ¹èŒ¶ç­‰ç­‰ï¼Œé…·ï½ æ³¢ä¸Šå®® é€™å€‹ä¹Ÿæ˜¯æˆ‘æœŸå¾…å·²ä¹…çš„æ™¯é»ï¼Œä¸éæŠµé”å¾Œéº»å› ç‚ºè‚šå­å¾ˆç—›(æ‡‰è©²æ˜¯åƒäº†é‡å£å‘³ä¸€è˜­åˆåƒäº†å†°å¯èƒ½é‚„æœ‰é»ä¸­æš‘ï¼Œå‡ºå»ç©é¤é»ä¹Ÿæ˜¯å¾—å¥½å¥½æ’çš„å‘¢)ï¼Œå…¨å®¶è‘—æ€¥åœ°æ‰¾å»æ‰€ \u0026#x1f923; é‚„å¥½æœ‰é©šç„¡éšªçš„æ¸¡éäº†ã€‚äº‹å…ˆæœ‰å…ˆåšåŠŸèª²æŒ‰ç…§æ•™å­¸çš„é€²è¡Œå¾¡æ‰‹æ´—ï¼Œåœ¨é€²æ®¿é–€åƒæ‹œï¼Œå¾ˆè¿…é€Ÿçš„å°±çµæŸäº†ã€‚å› ç‚ºä»Šå¤©å¤©æ°£å¤ªç†±ï¼Œè·³éäº†å¹¾å€‹è¡Œç¨‹ï¼Œæ‰€ä»¥çµæŸå¾—å¾ˆæ—©(æˆ‘å€‘å¯¦åœ¨å¤ªå¥¢ä¾ˆäº†XDæ—©ä¸€å€‹ç¦®æ‹œä¾†çš„æœ‹å‹å“ªå…’éƒ½å»ä¸äº†)ã€‚æ™‚é–“æ‰å¿«ä¸‰é»ï¼Œä¾¿ç›´æ¥å»é£¯åº—è¾¦ç†å…¥ä½ï¼Œé‚„å¥½å¯ä»¥ç›´æ¥ Check inã€‚ D3 æ™šé¤ \u0026amp; é£¯åº— ç¬¬ä¸‰é–“é£¯åº—å› ç‚ºåè½åœ¨é‚£éœ¸å¸‚å€ï¼Œæ‰€ä»¥è¦åŠƒçš„æ™‚å€™ä¹Ÿæ˜¯çŒ¶è±«äº†å¥½ä¹…ï¼Œå„˜ç®¡è·Ÿç¬¬äºŒå¤©çš„ä¸€æ¨£è²´ï¼Œä½†æ˜¯ç©ºé–“å°å¾ˆå¤šï¼Œæˆ¿å¤–å¸‚æ™¯ä¹Ÿå¾ˆæ™®ï¼Œä¸éå› ç‚ºæ˜¯é€£é–çš„ï¼Œæ‰€ä»¥æ•´é«”ä¾†èªªé‚„æ˜¯å¾ˆä¹¾æ·¨ï¼Œéº»ä¹Ÿèªªä¸æœƒéæ•ï¼Œç¡å¾—å¾ˆèˆ’æœ \u0026#x1f44d; å› ç‚ºæ¥è‘—æ²’æœ‰è¡Œç¨‹ï¼Œæ‰€ä»¥åœ¨æˆ¿é–“ç¡äº†è¦ºï¼Œç„¶å¾Œå†å»å¤§å»³å–å€‹é£¯åº—å…è²»æä¾›çš„æ¸…é…’è·Ÿå°é»å¿ƒï¼Œéæ²’å¤šä¹…æ‹”éº»ä¹Ÿä¸€èµ·ä¸‹ä¾†ã€‚é›–ç„¶ç•¶ä¸‹ä¸æ˜¯åœ¨å¤–é ­è§€å…‰ï¼Œä½†å¦‚æ­¤æ‚ é–’æ„œæ„çš„æ™‚å…‰ä¹Ÿæ˜¯å¾ˆäº«å—çš„~ ç´„äº”é»æº–å‚™ä¸€ä¸‹å°±å†æ¬¡å‡ºç™¼å»åƒä¹Ÿæ˜¯æœŸå¾…å·²ä¹…çš„é˜¿å¤è±¬ç«é‹ \u0026#x1f37d;\u0026#xfe0f;\né£Ÿå½©é…’æˆ¿å…¶å¯¦åœ¨ google è©•è«–è£¡çš„å°ç£è©•è«–ä¸å¤šï¼Œä½†æ¯å€‹è©•åƒ¹éƒ½è¶…é«˜ã€‚è€Œä¸”é˜¿å¤è±¬ä¹Ÿæ˜¯åœ¨æ²–ç¹©å¿…åƒçš„ç¾é£Ÿä¹‹ä¸€ï¼Œé‚£æ™‚å€™åœ¨å®‰æ’è¡Œç¨‹çš„æ™‚å€™å› ç‚ºé€™é “ä¹Ÿæ˜¯è¦ä¸€äººäº”åƒæ—¥å¹£ï¼Œå‚³åœ¨ç¾¤çµ„è£¡å•è¦é¸å’Œç‰›é‚„æ˜¯é˜¿å¤è±¬ï¼Œçµæœæ‹”å¾ˆéœ¸æ°£çš„ç›´æ¥èªªéƒ½åƒ \u0026#x1f389; æ•´é–“åº—å…§åªæœ‰æˆ‘å€‘æ˜¯å°ç£äººï¼Œåº—å“¡å¾ˆä»”ç´°åœ°ä»‹ç´¹å„ç¨®åƒæ³•ï¼Œå‰åŠéƒ¨å¹«å¿™æ¶®è‚‰ã€æˆ‘å€‘é‚Šåƒï¼Œæ¡Œé‚Šæœå‹™è¶…åˆ°ä½ï¼å‰èœé‚„æœ‰åƒåˆ°åˆ°è™•çœ‹åˆ°çš„æµ·è‘¡è„ï¼Œå†åŠ ä¸Šä¸€æ•´ç›¤è‚‰è·Ÿä¸€äº›é’èœï¼Œæ‹”å§Šè·Ÿæˆ‘ä¸€äººé‚„é…ä¸€æ¯ ORION å•¤é…’ï¼Œå°±å…«ä¹åˆ†é£½äº†ï¼Œç­‰åƒå®Œè‚‰ä¹‹å¾Œåº—å“¡æœƒå°‡å‰©ä¸‹çš„æ¹¯ç…®æˆç²¥åšå®Œç¾çš„çµå°¾ã€‚åº—å“¡éå¸¸ä¸æµªæˆ‘å€‘çš„é£¯éŒ¢çš„æŠŠæ‰€æœ‰é£Ÿç‰©å¡åˆ°æˆ‘å€‘çš„ç¢—è£¡ï¼ŒçœŸçš„æ˜¯é£½åˆ°é ­é ‚ \u0026#x1f635; ç¸½çµä¾†èªªçœŸçš„å¥½åƒï¼è‚‰çœ‹èµ·ä¾†å¾ˆå¤šæ²¹èŠ±ï¼Œä½†åƒèµ·ä¾†ä¸æœƒå¾ˆå®¹æ˜“è†©ï¼Œå°¤å…¶æ­é…æŸšå­é†¬æ²¹å¾ˆæ¸…çˆ½ã€‚æœ€é‡è¦çš„æ˜¯é£½å¾—å¾ˆè¶…å€¼ \u0026#x1f974;\nåƒé£½å¾Œæ‹”éº»ä¸€æ¨£å›é£¯åº—ä¼‘æ¯ï¼Œæˆ‘è·ŸäºŒå§Šåˆå†æ¬¡å‡ºå¾åˆ°åœ‹éš›é€šè³¼ç‰©ï¼Œè²·äº†ç´€å¿µç£éµï¼Œå…¶å®ƒå› ç‚ºåœ¨ç¾åœ‹æ‘è²·å¾—å¤ å¤šäº†ï¼Œæ‰€ä»¥ä¹Ÿæ²’æœ‰å¤šè²·ä»€éº¼ã€‚å›ç¨‹å¤ªç´¯å°±åè¨ˆç¨‹è»Šå›é£¯åº—äº†ï¼Œæ—¥æœ¬çš„è¨ˆç¨‹è»Šæ˜¯è‡ªå‹•é–€ï¼Œé–‹é—œé–€éƒ½æ˜¯å¸æ©Ÿæ“æ§ï¼Œå¾ˆæ–°å¥‡ã€‚å¦å¤–è¨ˆç¨‹è»Šå¸æ©ŸçœŸçš„ä¸åˆ†åœ‹ç•Œï¼Œéƒ½è¶…è¡ï¼Œé‚„å·®é»åœ¨ä¸€å€‹è·¯å£æ’ä¸Šçªç„¶é‘½å‡ºä¾†çš„æ©Ÿè»Šï¼Œå¹¸å¥½å¹³å¹³å®‰å®‰çš„ï¼ DAY4 ç‰æ³‰æ´ æ—©ä¸Šåƒé£¯åº—å¾ˆè±ç››åˆå¾ˆè²´çš„è‡ªåŠ©å§æ—©é¤å¾Œå†æ‚ é–’åœ°å‡ºç™¼å»ç‰æ³‰æ´ã€‚å¾ˆå¹¸é‹åœ°è¶•ä¸Šåé»åŠçš„æ²–ç¹©å‚³çµ±å¤ªé¼“è¡¨æ¼”ï¼Œæ˜¯å…è²»çš„é‚„å¾ˆç²¾é‡‡ï¼è¶…åˆ’ç®—ã€‚æ´ç©´è£¡çš„é˜ä¹³çŸ³ä¹Ÿå¾ˆå£¯è§€ï¼Œèµ°åœ¨è£¡é¢é‚„æœƒè¢«æ°´æ»´åˆ°ï¼Œæ„Ÿè¦ºé˜ä¹³çŸ³ä¾èˆŠç„¡æ™‚ç„¡åˆ»çš„åœ¨ç”Ÿæˆé•·å¤§ä¸­ï¼Œä¸ç”±å¾—è®šå˜†å¤§è‡ªç„¶çš„é¬¼æ–§ç¥å·¥ï½å¤§ç´„èµ°äº†åŠå°æ™‚ï¼Œå‡ºæ´ç©´å¾Œé»äº†å…©ç¢—å‰‰å†°æ¶ˆæš‘ï¼Œå†åƒè§€ç‹åœ‹æ‘ï¼Œçµ‚æ–¼é«”æœƒåˆ°ç¶²è·¯ä¸Šèªªçš„å•†æ¥­æ„Ÿå¾ˆé‡äº†ï¼Œå€¼å¾—çœ‹çš„æ±è¥¿ä¸¦ä¸å¤šï¼Œæœ‰å°è±¡çš„åªæœ‰ç¨®æ»¿å„å¼å„æ¨£çš„æ°´æœè·Ÿæ¤ç‰©ï¼ˆæ‹”çš„æœ€æ„› XDï¼‰è·Ÿé™³å¹´è›‡é…’ã€‚ å¥§æ­¦å³¶ \u0026amp; åˆé¤ å› ç‚ºå¥§æ­¦å³¶å°±åœ¨æ—é‚Šï¼Œæ‰€ä»¥å°±æŠŠé€™å€‹è²“å³¶ä¹‹ç¨±çš„æ™¯é»æ’é€²å»äº†ï¼Œé †é“å»åƒå€‹æœ‰åçš„å¤©å©¦ç¾…ç•¶åˆé¤ï¼Œé»äº†å¹¾é …ç‚¸ç‰©å’Œæ—é‚Šçš„å…©ç¢—æ²–ç¹©éºµã€‚ä¸éé‚„æ˜¯å¤ªæƒ³çœ‹ç‚ç†±çš„å¤©æ°£ï¼Œåƒé£½å°±å•Ÿç¨‹äº†ï¼Œåªçœ‹åˆ°å…©ä¸‰éš»è²“ \u0026#x1f602; ç€¨é•·å³¶ æ¥è‘—å› ç‚ºé‚„å¾ˆæ—©ï¼Œä¾¿æ±ºå®šåˆ°æ©Ÿå ´é™„è¿‘çš„ OUTLET èµ°èµ°ï¼Œä¸éé€™é‚Šä¸æ¨£ AEON é‚£æ¨£ï¼Œè€Œæ˜¯çœŸçš„ä¸€é–“é–“ä¸€å±¤æ¨“çš„ OUTLETï¼Œæ²’æœ‰æƒ³è²·æ±è¥¿çš„æˆ‘å€‘èµ°äº†ä¸€åœˆå¾Œå°±å›åˆ°è»Šä¸Šç¡è¦ºäº†ï¼Œç´®å¯¦çš„ç¡äº†ä¸€è¦ºå¾Œå°±å‡ºç™¼åˆ°ç€¨é•·å³¶åƒå¹¸ç¦é¬†é¤…å•¦~\nç€¨é•·å³¶è·é›¢å¹¾åˆ†é˜å°±åˆ°äº†ï¼Œé€™è£¡çœŸçš„æ˜¯åº§å¾ˆæ¼‚äº®çš„å°å³¶ï¼Œè—å¤©ç™½é›²å¤§æµ·åœ¨é…ä¸Šå…¨ç™½çš„å»ºç¯‰ï¼Œæœ‰ç¨®ç•°æ–¼æ—¥æœ¬çš„æ­å¼é¢¨æƒ…ã€‚å¹¸ç¦é¬†é¤…é‚£æ™‚å€™å†å‰å…©å‘¨é ç´„çš„æ™‚å€™é‚„æ²’æ³¨æ„åˆ°æˆ‘å€‘è¦åƒçš„æ˜¯å‘¨æœ«ï¼Œé‚„è“„å‹¢å¾…ç™¼çš„åœ¨é›»è…¦å‰å°±ä½æ¶ï¼Œçµæœæ‰ç™¼ç¾å‘¨æœ«ä¸é–‹æ”¾é ç´„ï¼Œå¤ªæç¬‘äº† \u0026#x1f92a;\nè¶•ç·Šç™»è¨˜å¾Œå°±åœ¨åº—å¤–ç­‰å€™ï¼Œå¹¸å¥½é›–ç„¶ä¾†å®¢å¾ˆå¤šï¼Œä½†å¤§ç´„åŠå°æ™‚å¾Œå°±ç­‰åˆ°äº†å®¤å¤–çš„åº§ä½ï¼Œæˆ‘å€‘é»äº†æ°´æœè·ŸéŒ«è˜­ç´…èŒ¶å£å‘³ã€‚å‰é¢å¹¾å£ç›è±”å¥½åƒï¼Œè¶…è»Ÿè¶… Q å½ˆï¼Œå£æ„Ÿéå¸¸ç‰¹åˆ¥ï¼Œå¤§å®¶ä¹Ÿè¦ºå¾—å¥½åƒ \u0026#x270c;\u0026#xfe0f; é‚„å¥½éµå®ˆç¶²è·¯ä¸Šçš„å»ºè­°ï¼Œåªé»äº†å…©å€‹ä¸ç„¶åƒåˆ°å¾Œé¢ä¹Ÿæœƒæœ‰é»è†©ã€‚ å›ç¨‹ åƒé£½å–è¶³å¾Œä¾¿æ­£å¼çš„çµæŸäº†æ²–ç¹©çš„è¡Œç¨‹ï¼Œå»é™„è¿‘é‚„å®Œè»Šå¾Œä¾¿æ­æ¥é§è»Šå»æ©Ÿå ´äº†ã€‚é¦–æ¬¡å˜—è©¦è‡ªé§•ï¼Œæ²’æœ‰ä»»ä½•æ“¦å‚·ï¼Œèƒ½å®‰å…¨çš„çµæŸï¼Œé€™æ˜¯æœ€æ£’ã€æœ€å€¼å¾—æ„Ÿæ©çš„äº‹äº†ã€‚åˆ°æ©Ÿå ´é‚„å¾ˆæ—©ï¼Œæ±ºå®šå†ç”¨é£¯ç³°çµæŸé€™å®Œç¾çš„è¡Œç¨‹ï¼ï¼ä½†åŸä¾†é£¯ç³°åº—å°±åœ¨æˆ‘å€‘å ±åˆ°(ä¸‰æ¨“)çš„æ­£ä¸‹æ–¹ä¸€æ¨“çš„åœ°æ–¹ï¼Œæˆ‘è·Ÿå§Šé‚„å¤§è€é çš„å…ˆèµ°åˆ°å³é‚Šä¸‹æ¨“åœ¨èµ°å›å·¦é‚Šï¼Œå¹¸å¥½å›ç¨‹æ™‚æœ‰æç„¶å¤§æ‚Ÿå“ˆå“ˆå“ˆå“ˆã€‚æ™šé¤ä¸€äººåƒä¸€å€‹é£¯ç³°ï¼Œæ‹”åƒå±±è‹¦ç“œã€éº»åƒæ˜å¤ªå­ã€å§Šåƒç‚¸è¦ã€æˆ‘åƒå³¶è±†è…ï¼Œé †ä¾¿æŠŠéº»é‚£é‚Šæ›å‰©çš„ä¸€åƒå¡Šæ—¥å¹£èŠ±å®Œï¼Œæˆ‘é€™é‚Šæ›å‰©çš„æ—¥å¹£ä¹Ÿåƒ…å‰©å¹¾ç™¾å¡Šï¼Œç®—å¾—è¶…ç´šå‰›å¥½ \u0026#x1f919;\u0026#x1f919;\u0026#x1f919; å¹¸ç¦çš„åƒå®Œä¹‹å¾Œå°±å·®ä¸å¤šå»å ±åˆ°äº†ï¼Œå®Œç¾å®Œç¾ \u0026#x1f44f;\nå¾å…­æœˆçŸ¥é“è¦å…¨å®¶ï¼ˆå°‘äº†å‰›å»ç©å¤§é˜ªå›ä¾†çš„å¤§å§Š \u0026gt;\u0026lt;ï¼‰ä¸€èµ·å‡ºåœ‹ç©ï¼Œå°±ç”¨äº†å¾ˆ(ä¸Š)å¤š(ç­)æ™‚é–“æ’è¡Œç¨‹ã€çœ‹é£¯åº—ã€‚æ’å®Œç¾çš„è¡Œç¨‹å‡ºä¾†è¦ºå¾—æˆå°±æ„Ÿå¥½å¤§å–”ï¼Œçœ‹å¤§å®¶ä¹Ÿéƒ½ç©å¾—å¾ˆé–‹å¿ƒå°±è¦ºå¾—å¿ƒæ»¿æ„è¶³ã€‚æœ‰è¨˜æ†¶ä»¥ä¾†çš„å…¨å®¶å‡ºåœ‹ç©æ˜¯åœ‹å°çš„é¦™æ¸¯è¿ªå£«å°¼ã€‚ä¹‹å¾Œçˆ¸åª½åŠªåŠ›å·¥ä½œã€è€Œæˆ‘å€‘æ±‚å­¸ï¼›å¦‚ä»Šæˆ‘å€‘ä¹Ÿéƒ½æœ‰å·¥ä½œäº†ï¼Œçˆ¸åª½ä¹Ÿè©²åˆ°é€€ä¼‘äº«ç¦çš„å¹´ç´€ã€‚å¸Œæœ›ä»¥å¾Œèƒ½å¸¸å¸¸å¸¶ä»–å€‘å‡ºåœ‹ç© \u0026#x1f496; é•·æ„ˆå¤§æ„ˆèƒ½æ„Ÿå—å®¶è£¡çš„å¥½ï¼Œå’Œå®¶äººä¸€èµ·å‡ºå»ç©ä¹Ÿæ˜¯æœ€æœ€æœ€å¥½ã€æœ€æ£’ï¼\u0026#x1f4aa;\n","date":"2020-11-08T19:00:00+08:00","permalink":"http://localhost:1313/p/okinawa-2019/","title":"å››å¤©ä¸‰å¤œæ²–ç¹©éŠè¨˜âœ¨"}]