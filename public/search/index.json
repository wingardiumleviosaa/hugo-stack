[{"content":"TL; DR 趁著學期結束後，在六月的最後一個禮拜犒賞自己在職碩一結束後來場香港四天三夜遊 (｡í _ ì｡)\n行前 這次從四月提議、到五月底真正決定出發，總共拖了一個月 XD 最後終於在出發前的一個月才找好機票跟飯店，飯店是在去樹林上班的路上看到便宜、後來住了之後超滿意的 OTTO Hotel，兩晚 $5400。後來還決定為了蛋塔多住一晚，而千挑萬選的 168 youth hostel，一晚 $1500，只能說對得起價格 (´・Å・`) 機票的話可惜沒有注意到這幾年都會有的國泰買一送一的活動，就用大概高雄香港快運 $6800、台北長榮 $7900 的來回價格買了。\n然後用了十年的護照也在今年即將到期了，翻舊護照還翻到之前去美國加拿大的出入境章，時間真的過得飛快 ( •́ὤ•̀) 五月底請了一個下午去辦新護照，沒想到外交部就在青島東路，剛好還遇到很久違的大型集會 （青鳥運動）。高中的時候一直很有印象班導對於我們竟然不知道行政院長是誰而驚訝，然後跟我們說身為公民就是需要有公民責任。那時候不懂政治是什麼，但沒想到十幾年後的現在也會如此投身其中。原來這就是長大！\n接著在出發前就又激發了 J 的屬性質，發現了去趣這個好用的行程安排 APP，然後每天狂看遊記分享找景點跟美食，最後共調整了三次的行程最後才底定好 🤣 DAY1 這次終於不是紅眼班機，早上六點出門搭公車去坐機捷綽綽有餘。自從之前去美國首次自己坐飛機後就再也沒獨自坐過，姊甚至是第一次，所以各自從高雄跟桃機出發的我們一路的跟對方說現在在哪個階段了 XD 暑假的人潮果然不可小覷，這時段的飛機班次太多了，導致誤點了半小時才起飛 🥹\n另外還有一個小插曲，入境時因為空姐在飛機上說可以不用填入境小卡，所以在海關閘口前沒填完全部的欄位就急急忙忙地去排隊，殊不知海關還是跟我要了，再殊不知交出沒填完整的小卡還是順利入境了 ᕕ ( ᐛ ) ᕗ\n姊先幫我拿好行李，會合後在機場大廳領事先在 klook 買好的八達通，再去買機場跟市區來回的城巴快線 A21 \u0026amp; A25 套票 HKD$60，從機場到市區的班次很幸運坐到了幾乎是直達的 A25，比坐捷運方便（不需拉行李轉車），而且還比捷運便宜很多！\n左上：桃機滿滿的checkin人潮、右上：來自姐的checkin回報、下：A25空空的舒適車廂\nHashtag b 到了飯店所在的尖沙咀後，先順道去了 hashtag B 買千層蛋塔，幸運的是不用排幾組就很快買到了！買了兩個蛋塔跟一個開心果酥 接著去飯店 checkin，大概在飯店前繞了兩圈才發現入口 XDD 一進飯店就覺得選對了，房間不大，但對於寸土寸金的香港來說已經很舒適了，最棒的是隔音做得很好，兩晚住下來沒找到一個缺點！ 稍作休息後便出發原本的首站六安居吃飯，路上首次體驗了叮叮車。原本想體驗傳統的港式飲茶推車的，殊不知只開早上，幸好還有備案，還是排行程的時候猶豫很久最後捨棄的生記粥品！ 生記粥品專家 點了牛肉跟魚的口味各一碗，粥本身煮到連米粒都看不見，喝起來很清爽又很香！店家坐落於巷弄，門口很不起眼，所以還差點走到馬路上的同名麵店 🤣 原本看網路評價都是需要排隊的，但因為是平日、又是兩點時的非用餐時間，所以除了不用排隊之外，連餐點都打了折！ 中環街市-檸檬王、陳意齋、蘭芳園、泰昌餅家 去搭中環半山手扶梯的路上先去檸檬王跟陳意齋買了伴手禮，接著順利的搭到往上的手扶梯免去爬坡的辛苦，喝了蘭芳園的絲襪奶茶（外帶無需排隊！）、意猶未盡的還在隔壁 LINLEE 買了凍檸茶，再來一顆泰昌餅家的傳統蛋塔（買完還到對巷的某間咖啡廳外的石椅上坐著享用） 在googlemaps上找到石椅的確切位置，感謝 Green Waffle Diner 的貢獻 XDD\n荷李活道 wall mural、crazy of art 沒有在預定行程內，但走沒幾步發現了一個小地標。 大館 - SHARISHARI 舊警署監獄古蹟的遺址變成藝術展覽館，展示了香港殖民地時期的建築風格和歷史。 然後沒細逛很久就跑到隔壁的口袋名單之一的 SHARISHARI 吃冰 XD 點了伯爵奶茶口味，份量十足，裡面的黃豆粉很香、裡面還加了奶凍跟餅乾屑，整理來說還不錯！ 石板街 走中環砵甸乍街去港口的路上的一個拍照景點 - 石板街，這時候腳已經有腫脹的感覺了，還記得在走這條的時候，因為是下坡、又這麼多石板做為障礙，我跟姐都在哀嚎 😂 天星小輪、維多利亞港 接下來是我最期待的行程，從中環坐小渡輪回尖沙咀！天星小輪一次只需要 HKD$5 用八達通通行上船後十分鐘就可以直達尖沙咀，一邊吹風一邊欣賞港口美景，非常推薦！ 在去港口的路上會經過長長的天橋，正好還遇到下班時間，看到許多的正裝上班族通勤，才體會到身處在商業重地之中的繁忙。另外還有路過沒想像中大的香港摩天輪 🎡\n下船後還正好趕上八點的燈光秀，剛好挑到好位置欣賞這世界三大夜景之一的維多利亞港！ 妹記大排檔 回到飯店休息後，九點還是堅持出去吃晚餐，這間妹記大排檔原本不在我做功課的名單內，但是剛好看到為了去香港而加入的 line 社群裡面的人大力讚賞，而很巧的就坐落在飯店的正前方，便也順便來試試香港的大排檔餐廳！ 點了一個公仔麵、炒青菜跟烤乳鴿，結果最好吃的是燙青菜，其次是烤乳鴿 XDD 公仔麵口味太重了沒辦法順利吃完 DAY2 新興食家 第一站早午餐來堅尼地城的港式飲茶新興食家，還沒11點所以不需要排隊，但裡面也幾乎是滿座的情況。因為用餐區小到沒辦法從推車點餐，也因為座位有限，人數少的話絕對會需要併桌，但整體餐點好吃，點餐的服務阿姨態度也跟網路上說的完全不一樣，非常友善！吃飽出來還很幸運的發現外面已經開始在排隊了，再次讚嘆我的排程 d(`･∀･)b 石牆樹群、小紅書籃球場 吃飽後走來附近的兩個景點，一個是石牆樹群，另外一個是去年小紅書爆紅的籃球場拍照景點。在抵達籃球場前，還以為是直接可以遠眺的風景，結果抵達後發現風景得隔著藍球網拍 XDD 能發現的人真的很厲害 怪獸大樓 變形金剛的取景地怪獸大樓是由多棟大廈組成的，從外觀看上去如同被巨大的怪獸吞噬，因而得其名。每層交錯，是典型的香港蝸居，密密麻麻的窗戶裡藏著無數人的故事與日常。 BlueHouse 排行程時無意間看到的地標 BlueHouse，藍屋以前是一所醫院，現今列為香港一級歷史建築物。令人驚喜的是這邊的建築竟然不僅只有藍色，周邊的建築物每一棟都擁有獨特的顏色，與藍屋相互輝映，形成一幅充滿活力的城市風景。 新光戲院、香港街景、路上美食 在從堅尼地城回到中環、再去銅鑼灣上以及回到中環的路上買的小食物有：\nifc mall 裡面的 FineFood 拿破崙蛋糕配 % Arabica 咖啡，特別到這間購物商場裡面去找論壇上推薦的帝苑酒店的 FineFood 櫃位買拿破崙，順便還買了一盒黑鹽口味的蝴蝶酥，這邊真的完全不用排隊，有夠隱密！ 百事吉餅店的蝴蝶酥跟拿破崙派，蝴蝶酥是伴手禮，拿破崙蛋糕回到飯店吃的心得是果然還是找不到小時候在皇朝酒店吃的味道，但有時候我在想，長大後明明吃過這麼多不一樣的拿破崙，總還是覺得皇朝的最棒，也或許是因為小時候吃的時候是暑假在大陸玩的美好時光，所以才那麼難以被取代。 新光戲院旁的涼茶，茶是用碗公裝好然後就站在店家前面喝完，很特別的體驗！然後因為那時候是感冒末期，可是這兩天又放縱的吃了一堆冰品，所以為了彌補喉嚨點了化痰茶 (⁰▿⁰) 銅鑼灣店的 BakeHouse，經過沒人在排隊的另一家熱門蛋塔店，就馬上各買了一顆站在門口吃，剛出爐的蛋塔真的很讚！ 接著是在銅鑼灣走的一堆路上拍到的絕美街景，香港真的是座神奇又矛盾的城市。擁擠的人潮擠滿了每一寸土地，卻又能在車水馬龍的街道上感受到寬闊的自由。古老的建築和現代的高樓大廈並肩而立，前面才看到灰暗密集的社區、馬上又有色彩飽和的街景。果真是要自由行才能深入感受到城市的魅力。 譚仔三哥米線 晚餐吃飯店附近譚仔三哥的過橋米線，兩個人一碗再加上土匪雞翼。終於可以理解為什麼香港人這麽喜歡吃了！榮登我香港行的前三！ 廟街夜市、澳洲牛奶公司 吃飽後坐公車去廟街夜市逛逛，夜市相較於台灣遜色很多，沒有特別的食物。幸好還可以順便去附近的澳洲牛奶公司吃個燉奶當點心。燉奶點冰的，吃起來就是很濃郁的奶酪，好好吃！ 回程還順便去逛了 Baleno，姊買了一件排汗 T恤，這間簡直就是台灣的佐丹奴嘛 XD\nDAY3 今天要從很棒的 OTTO 退房了 T_T 早上趁姐在準備的時候，先去珍妮小熊扛四盒餅乾當伴手禮。九點半就到了，沒想到人已經排了好大一圈，不過幸好結帳流程很快，所以十點一到沒多久就順利買到了！ 退房後再回到珍妮小熊的同一棟住商大樓 XD 第二間青旅也在同一棟，因為下午三點才能入住，幸好可以先免費寄放行李 ヽ(●´∀`●)ﾉ\n麥文記麵家 不小心沒注意到開店時間，還沒 12 點就到了麥文記，在附近晃晃回去後剛好營業，點了鮮蝦雲吞、牛腩撈麵還有芥蘭菜，鮮蝦跟網路評價敘述一樣的新鮮好吃，另外芥蘭菜意外鮮脆～ 吃飽出來後一樣發現在排隊了，優秀的時間管理能力 (⁎⁍̴̛ᴗ⁍̴̛⁎)‼\n佳佳甜品 正餐吃完後來隔壁吃甜品，雖然需要排隊，不過也剛好在等的同時下起一場雨，可惜的是楊枝甘露賣完了，所以點了芝麻糊拼杏仁露還有木瓜銀耳。芝麻糊很香，但是是可以想像的味道。 彩虹邨籃球場 來到彩虹站的彩虹邨，籃球場很特別的位在停車上頂樓，在高度的籃球場剛好是一個很好的平台將五顏六色的大樓收進相機畫面裡！然後功課沒做仔細，在社區裡繞兩圈沒找到籃球場到底在哪，就在放棄之際，看到有很明顯是遊客的人爬上停車場頂樓，才發現原來在上面 ლ(´•д• ̀ლ\n旺角的各種街 來到旺角的各種街上吃東西\n啊一檸檬茶、茶救星球苦瓜檸檬茶，在香港嘗試各種牌子的檸檬茶 XD 結論是第一天喝的林里最好喝，啊一完全沒味道啊！然後苦瓜檸檬茶蠻特別的。 花園街市場奇趣餅家，這間是無意間在市場裡面經過的生意很好的餅店，買了紅豆麻糬餅跟光酥餅，紅豆麻糬餅中規中矩，光酥餅的部份吃不太懂 XDD 還記得吃不完拿來當隔天帶蛋塔回去塞縫隙的防撞墊 =v= 晶華冰廳菠蘿油，不愧是網上評論中香港最佳菠蘿油！直接不用排隊就外帶買到一顆剛出爐的，外表菠蘿酥的部分極脆，奶油很香！好好吃 (๑´ㅂ`๑) 蛋仔記雞蛋仔，香港的特色小吃，中規中矩的味道跟口感，吃起來就跟在台灣吃的一樣 ^_^ 富豪雪糕，沒有特別紀錄雪糕車出沒的位置，但真的很容易遇到，這邊應該是剛好司機臨時停車回來，被前面的客人攔截，我們跟著排，買完就開走了 🤣 傳統的香草雪糕，但吃起來意外清爽～ 紀錄旺角的熱鬧街頭，還有在旺角特地走了一個上海街 618 的景點，這邊有點像松山文創，一樓還有樂團在演奏，整棟樓都是販賣一些文青的小物。 帝苑拿餅 從旺角直接搭公車回到尖沙咀的帝苑酒店拿一個禮拜前在網路上訂好的蝴蝶酥～買了兩盒巧克力跟兩袋原味，巧克力很貴，但回家享用後才理解他的價值，餅乾的部分很脆，但上面的巧克力居然是濕潤的、甜度剛好，還好有買兩盒！\n在去的路上剛好可以欣賞白天的維多利亞港口～ 168 Hostel 傍晚回到青旅，終於可以 checkin 看看內裝到底長怎樣了，首先青旅是在一棟很有年代感的住商大樓裡面，多有年代感可以參考珍妮小熊排隊的照片環境 XD 幸好早上來探過路，不難找到位置。青旅內部雖也看起來老舊，但所幸環境很整潔，還有飲水機可以使用。 炯記燒臘 原本這趟行程要吃有名的甘牌燒鵝，但第二天看到經過時的人潮就果斷放棄，所以留最後一個晚上在附近找燒臘店吃。點了兩份雙拼選了四種不一樣的燒臘，叉燒、油雞、腩仔、燒鵝，燒鵝果然是最厲害的！飯也是粒粒分明，加上鵝油很香很好吃。 DAY4 BakeHouse 留到第四天的最終目的就是去旅館附近的 bakehouse 帶蛋塔回台灣！！！所以早上六點半起床，不到七點就去店門口排隊了，到的時候已經有兩個人在門口等候了，等到八點一開門，後面目測應該排了 30 個人有，果然還是早起的鳥兒有蟲吃 🥹 買了兩盒外加一顆可頌當早餐\n回程 - A21、香港機場翠園 買完蛋塔就到飯店對面的車站牌等 A21 回機場，出門就發現下起大雨，幸好要回程啦！回程的 A21 人就非常多，還好我們上車的尖沙咀站在前段，還有位置可以坐到機場。\n到機場後去吃了最後一餐港式，翠園，奶油豬仔包剛出爐外表酥脆、內裡鬆軟，然後麵食點了牛肉河粉還有酸辣蝦米粉，湯頭都很好喝，還有機會絕對要再點酸辣蝦米粉！！！ THE END 完美的結束香港的美食之旅啦～行程排得都很滿意，該吃的都有吃到，沒有排太多隊，不鬆不緊的景點，很棒！ ♥(´∀` )\n","date":"2024-11-01T15:00:00+08:00","image":"http://localhost:1313/p/hongkong-2024/cover_hu7953674859307773612.jpg","permalink":"http://localhost:1313/p/hongkong-2024/","title":"四天三夜香港遊記🥮"},{"content":"TL; DR 本文記錄在 Kubernetes 集群上部署 Apache Streampark 的步驟，Streampark 是一個開源項目，簡化了在 Kubernetes 上部署和管理 Flink 應用程序。\n客製化 Streampark image 主要要達成以下目的\n因需要在 Streampark 中建立 flink 集群，建立集群需指定 FLINK_HOME，但預設的 image 沒有包含 flink，所以需要將會用到的 flink 版本一次包好。 由於 Apache 專案與 mysql jdbc 驅動的 license 不相容，須額外下載 mysql jdbc 驅動，並將其放在 $STREAMPARK_HOME/lib 下。 前往官網下載最新版本 https://streampark.apache.org/download/ ，並準備以下 Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 FROM alpine:3.20 as deps-stage COPY apache-streampark_*-*-bin.tar.gz / WORKDIR / RUN tar zxvf apache-streampark_*-*-bin.tar.gz \\ \u0026amp;\u0026amp; mv apache-streampark_*-*-bin streampark FROM apache/flink:1.17.2 as flink-1.17 FROM apache/flink:1.18.1 as flink-1.18 FROM apache/flink:1.19.1 as flink-1.19 FROM docker:dind WORKDIR /streampark COPY --from=deps-stage /streampark /streampark ENV NODE_VERSION=16.1.0 ENV NPM_VERSION=7.11.2 RUN apk add openjdk8 \\ \u0026amp;\u0026amp; apk add maven \\ \u0026amp;\u0026amp; apk add wget \\ \u0026amp;\u0026amp; apk add vim \\ \u0026amp;\u0026amp; apk add bash \\ \u0026amp;\u0026amp; apk add curl ENV JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk ENV MAVEN_HOME=/usr/share/java/maven-3 ENV PATH $JAVA_HOME/bin:$PATH ENV PATH $MAVEN_HOME/bin:$PATH RUN wget \u0026#34;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; tar zxvf \u0026#34;node-v$NODE_VERSION-linux-x64.tar.gz\u0026#34; -C /usr/local --strip-components=1 \\ \u0026amp;\u0026amp; rm \u0026#34;node-v$NODE_VERSION-linux-x64.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs \\ \u0026amp;\u0026amp; curl -LO https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl \\ \u0026amp;\u0026amp; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl RUN mkdir -p ~/.kube # Copy Flink installations from base images COPY --from=flink-1.17 /opt/flink /streampark/flink-1.17 COPY --from=flink-1.18 /opt/flink /streampark/flink-1.18 COPY --from=flink-1.19 /opt/flink /streampark/flink-1.19 # Download and install JDBC connector RUN wget -P /streampark/lib https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.31/mysql-connector-j-8.0.31.jar EXPOSE 10000 打包鏡像\n1 docker buildx build -t streampark-flink:2.1.4 . 部署 MySQL ::: info Streampark 使用 h2, pgsql, mysql 其中一個 Database 存放 metadata，本例使用 MySQL。 :::\n準備 init.sql 檔案\n在下載的 Streampark 目錄中找 mysql 的 DDL 檔\n主要需要建立的是 mysql-schema.sql 以及 mysql-data.sql，合併兩個 sql 檔合併，並以 init.sql 命名。\n1 cat ./incubator-streampark-2.1.4/streampark-console/streampark-console-service/src/main/assembly/script/schema/mysql-schema.sql ./incubator-streampark-2.1.4/streampark-console/streampark-console-service/src/main/assembly/script/data/mysql-data.sql \u0026gt; init.sql 建立 mysql init.sql 的 configMaps\n1 kubectl create configmap mysql-initdb-config --from-file=init.sql -n streampark 建立 mysql statefulset\nstreampark-mysql-sts.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 --- apiVersion: v1 kind: Service metadata: name: mysql-service namespace: streampark spec: type: NodePort selector: app: mysql ports: - protocol: TCP port: 3306 targetPort: 3306 nodePort: 30306 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: streampark spec: serviceName: \u0026#39;mysql-service\u0026#39; replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: securityContext: fsGroup: 1000 runAsUser: 1000 containers: - name: mysql image: mysql:8.3 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#39;streampark\u0026#39; - name: MYSQL_USER value: \u0026#39;streampark\u0026#39; - name: MYSQL_PASSWORD value: \u0026#39;streampark\u0026#39; volumeMounts: - mountPath: /var/lib/mysql name: mysql-storage - mountPath: /docker-entrypoint-initdb.d name: initdb-scripts volumes: - name: initdb-scripts configMap: name: mysql-initdb-config volumeClaimTemplates: - metadata: name: mysql-storage namespace: streampark spec: accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] storageClassName: \u0026#39;ceph-block\u0026#39; resources: requests: storage: 10Gi 1 kubectl apply -f streampark-mysql-sts.yaml 建立 secret 以供後面 Streampark 部署使用\n1 2 3 kubectl create secret generic streampark-mysql \\ --namespace=streampark \\ --from-literal=mysql-root-password=streampark Streampark 部署 透過 helm 產生部署檔 在上一步驟下載的 Streampark 中包含了 helm chart 可用於 Kubernetes 的部署，\n進入 helm/streampark 目錄修改 values.yaml ，以註解說明本文修改的地方\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 image: repository: \u0026#39;harbor.sdsp-stg.com/base/streampark-flink\u0026#39; pullPolicy: \u0026#39;IfNotPresent\u0026#39; tag: \u0026#39;2.1.4\u0026#39; pullSecret: \u0026#39;\u0026#39; rbac: create: true spec: container: env: [ { name: TZ, value: \u0026#39;Asia/Taipei\u0026#39; }, { name: LANG, value: en_US.UTF-8 }, # 指定要使用的 DOCKER HOST，詳情請參考部署階段說明 { name: DOCKER_HOST, value: \u0026#39;tcp://172.21.18.51:2375\u0026#39; }, { name: LANGUAGE, value: en_US:en }, { name: LC_ALL, value: en_US.UTF-8 }, ] replicaCount: 1 containerPort: 10000 name: rest affinity: {} nodeSelector: {} tolerations: [] # 加大 resource limit 的資源限制 resources: { limits: { memory: \u0026#39;8Gi\u0026#39;, cpu: \u0026#39;4\u0026#39; }, requests: { memory: \u0026#39;1Gi\u0026#39;, cpu: \u0026#39;1\u0026#39; }, } livenessProbe: enabled: true initialDelaySeconds: \u0026#39;90\u0026#39; periodSeconds: \u0026#39;30\u0026#39; timeoutSeconds: \u0026#39;20\u0026#39; failureThreshold: \u0026#39;3\u0026#39; successThreshold: \u0026#39;1\u0026#39; readinessProbe: enabled: true initialDelaySeconds: \u0026#39;90\u0026#39; periodSeconds: \u0026#39;30\u0026#39; timeoutSeconds: \u0026#39;20\u0026#39; failureThreshold: \u0026#39;3\u0026#39; successThreshold: \u0026#39;1\u0026#39; # 定義 ingress，並使用 Prefix 的方式建立，另外移除不必要的 annotation (後續需要手動修改 yaml 加上 ingressClassName ingress: enabled: true host: \u0026#39;streampark.sdsp-stg.com\u0026#39; path: \u0026#39;/\u0026#39; pathType: \u0026#39;Prefix\u0026#39; # 因使用 ingress 對外開放，就將 service 改為 ClusterIP service: type: \u0026#39;ClusterIP\u0026#39; name: \u0026#39;streampark-service\u0026#39; streamParkDefaultConfiguration: create: true append: true streamParkServiceAccount: create: true annotations: {} name: \u0026#39;streampark\u0026#39; 設定 ./streampark/conf/streampark-console-config 目錄下的配置檔\n主要移除了用不到的 application-h2.yml 、 application-pgsql.yml 、 application-sso.yml 檔案\n並且修改 application.yml 以及 application-mysql.yml 內容\n1 2 3 4 # application.yml 修改 database 為 mysql，其於保持預設 spring: profiles: active: mysql 1 2 3 4 5 6 7 # application-mysql.yml 更新連線資訊 spring: datasource: username: root password: streampark driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://mysql-service.streampark.svc.cluster.local:3306/streampark?useSSL=false\u0026amp;useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;allowPublicKeyRetrieval=false\u0026amp;useJDBCCompliantTimezoneShift=true\u0026amp;useLegacyDatetimeCode=false\u0026amp;serverTimezone=GMT%2B8 產生 YAML 檔\n1 helm template streampark/ -n streampark -f streampark/values.yaml --output-dir ./result 產生的 YAML 檔會放在 result 目錄下。\n客製化部署檔 產生 kube config 的 configmaps 供 Streampark 使用 Kubernetes 部署 flink cluster\n1 kubectl create configmap my-kube-config --from-file=$HOME/.kube/config -n streampark 編輯 result/ingress.yaml ，指定 ingressClassName\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: streampark namespace: streampark labels: app.kubernetes.io/name: streampark app.kubernetes.io/version: \u0026#39;2.1.4\u0026#39; app.kubernetes.io/managed-by: Helm helm.sh/chart: streampark-2.1.4 spec: ingressClassName: nginx rules: - host: streampark.sdsp-stg.com http: paths: - backend: service: name: streampark-service port: name: rest path: / pathType: Prefix 編輯 result/streampark.yaml 新增 initContainers 測試 MySQL 資料庫連線、綁入 kube config、以及持久化專案目錄。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 apiVersion: apps/v1 kind: Deployment metadata: name: streampark namespace: streampark labels: app.kubernetes.io/name: streampark app.kubernetes.io/version: \u0026#39;2.1.4\u0026#39; app.kubernetes.io/managed-by: Helm helm.sh/chart: streampark-2.1.4 spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: streampark template: metadata: labels: app.kubernetes.io/name: streampark spec: serviceAccountName: streampark initContainers: - name: db-service-check image: mysql:8.3 command: - sh - \u0026#39;-c\u0026#39; - mysqladmin ping -h mysql-service.streampark.svc.cluster.local -uroot -p\u0026#39;${MYSQL_ROOT_PASSWORD}\u0026#39; --silent env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: streampark-mysql key: mysql-root-password containers: - image: harbor.sdsp-stg.com/base/streampark-flink:2.1.4 name: streampark imagePullPolicy: Always ports: - name: rest containerPort: 10000 protocol: TCP env: - name: TZ value: Asia/Taipei - name: LANG value: en_US.UTF-8 - name: LANGUAGE value: en_US:en - name: LC_ALL value: en_US.UTF-8 - name: DOCKER_HOST value: tcp://172.21.18.51:2375 securityContext: privileged: false command: [ \u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;bash ./bin/streampark.sh start_docker\u0026#39;, ] livenessProbe: exec: command: [ \u0026#39;curl\u0026#39;, \u0026#39;-s\u0026#39;, \u0026#39;http://localhost:10000/actuator/health/liveness\u0026#39;, ] initialDelaySeconds: 90 periodSeconds: 30 timeoutSeconds: 20 successThreshold: 1 failureThreshold: 3 readinessProbe: exec: command: [ \u0026#39;curl\u0026#39;, \u0026#39;-s\u0026#39;, \u0026#39;http://localhost:10000/actuator/health/readiness\u0026#39;, ] initialDelaySeconds: 90 periodSeconds: 30 timeoutSeconds: 20 successThreshold: 1 failureThreshold: 3 volumeMounts: - name: streampark-default-config-volume mountPath: /streampark/conf - name: kube-config-volume mountPath: /root/.kube - name: streampark-storage-volume mountPath: /opt resources: limits: cpu: \u0026#39;4\u0026#39; memory: 8Gi requests: cpu: \u0026#39;1\u0026#39; memory: 1Gi volumes: - name: streampark-default-config-volume configMap: name: streampark-console-config items: - key: application.yml path: application.yml - key: application-mysql.yml path: application-mysql.yml - key: logback-spring.xml path: logback-spring.xml - key: kerberos.yml path: kerberos.yml - key: spy.properties path: spy.properties - key: ValidationMessages.properties path: ValidationMessages.properties - name: kube-config-volume configMap: name: my-kube-config items: - key: config path: config - name: streampark-storage-volume persistentVolumeClaim: claimName: streampark-pvc 部署 在部署前需要先找一台有 docker 的機器將 docker daemon 公開出來，原因是因為當使用 streampark application mode 部署 flink 的時候，需要設定 DOCKE_HOST，並且還需要透過 docker 去執行打包以及推送鏡像。\n進入到欲公開使用的 docker host，編輯 /etc/docker/daemon.json\n1 2 3 { \u0026#34;hosts\u0026#34;: [\u0026#34;tcp://0.0.0.0:2375\u0026#34;,\u0026#34;unix:///var/run/docker.sock\u0026#34;] } 接著重啟 docker sudo systemctl restart docker，就可以發現到 docker post 被公開了。\n回到 Kubernetes 開始正式部署\n1 kubectl apply -f ./result ","date":"2024-08-09T10:10:17+08:00","permalink":"http://localhost:1313/p/data-apache-streampark-k8s-deployment/","title":"Apache Streampark Deployment on Kubernetes"},{"content":"TL; DR 目前的 Kubernetes HA 架構是使用 Nginx 當 Loadbalancer，所以在使用 Kubespray 部署時可以設定 api server 的 endpoint 為 external LB。\ninventory/sample/group_vars/all/all.yml\n1 2 3 4 5 ## External LB example config ## apiserver_loadbalancer_domain_name: \u0026#34;elb.some.domain\u0026#34; # loadbalancer_apiserver: # address: 1.2.3.4 # port: 1234 所以在部署 ansible playbook 前需要多修改 group_vars\n1 vi inventory/mycluster/group_vars/all/all.yml 將上面的 External LB example config 拿掉註解，並設定\n1 2 3 4 5 apiserver_loadbalancer_domain_name: \u0026#34;k8s.sdsp-dev.com\u0026#34; # 設定 nginx IP 以及轉發的 port loadbalancer_apiserver: address: 172.20.37.19 port: 6443 補充 nginx 的 stream config\nReference https://www.youtube.com/watch?v=u_1f3WyvtQE ","date":"2024-06-10T11:36:21+08:00","permalink":"http://localhost:1313/p/k8s-kubespray-exteranl-loadbalancer/","title":"Kubespray Setup for External Load Balancer"},{"content":"Introduction Kafka 生態系除了 Kafka Broker 外，還有 Kafka Connect, Kafka Bridge, Mirror Maker … 等，透過 Operator 來架設會比需要寫多的 manifest 或裝多個 helm chart 來的好用。本篇選擇 GitHub 上 Kafka Operator project 星星數最多的 Strimzi，下表為 Stimzi 提供可以部屬的資源列表。\nStrimzi Operator Deployment Best Practice 將 Strimzi Operator 安裝在其管理的 Kafka Cluster 及其他 Kafka component 不同的 namespace 中，以確保資源和配置的明確分離。 一座 Kubernetes 只安裝單一 Strimzi Operator 來管理所有 Kafka 實例。 更新 Strimzi Operator 和支援的 Kafka 版本，以反映最新的功能和增強功能。 安裝 Operator 1 2 kubectl create ns kafka kubectl create -f \u0026#39;https://strimzi.io/install/latest?namespace=kafka\u0026#39; -n kafka 部署 kafka cluster 1 2 3 4 5 wget https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.41.0/strimzi-0.41.0.tar.gz tar zxvf strimzi-0.41.0.tar.gz cd strimzi-0.41.0 cp examples/kafka/kraft/kafka.yaml . vi kafka.yaml 修改部署文件，KRaft 模式部署 Kafka 叢集需要使用 KafkaNodePool 資源，所以上面兩個 node pool 的 yaml 是必須部署的資源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: controller labels: strimzi.io/cluster: my-cluster spec: replicas: 3 roles: - controller storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi kraftMetadata: shared deleteClaim: false class: ceph-csi-rbd-hdd # 替換成現有的 storage class --- apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: broker labels: strimzi.io/cluster: my-cluster spec: replicas: 3 roles: - broker storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi kraftMetadata: shared deleteClaim: false class: ceph-csi-rbd-hdd # 替換成現有的 storage class --- apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata: name: my-cluster annotations: strimzi.io/node-pools: enabled strimzi.io/kraft: enabled spec: kafka: version: 3.7.1 metadataVersion: 3.7-IV4 listeners: - name: plain port: 9092 type: internal tls: false - name: external port: 9094 type: nodeport # 新增對外 nodeport 服務 tls: false configuration: bootstrap: nodePort: 32100 # 指定 bootstrap 占用的 nodeport，broker 如不一一指定的話，Operator 會自動指派 brokers: - broker: 0 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8091 - broker: 1 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8092 - broker: 2 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8093 config: offsets.topic.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 default.replication.factor: 3 min.insync.replicas: 2 entityOperator: topicOperator: {} userOperator: {} 開始部署\n1 kubectl -n kafka apply -f kafka.yaml 查看 kafka 以及 kafka node pool customized resource\n為 Kafka 設定 Load Balance Nginx Load Balance 的負載轉發規則須將 Bootstrap 以及 Brokers 所占用的所有 Worker Node 的 Node Port。\n以下是 Bootstrap port 的設定範例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 upstream tcp9094 { server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } 部署架構 註釋\n在設置 external 的設定時，需要也把每個 broker 的 nodeport 指定好，否則當客戶端存在於網段外部時，會遇到 Disconnected from node 1 due to timeout 的錯誤。\n例如：\n1 2 3 4 5 [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 2 disconnected. [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Cancelled in-flight API_VERSIONS request with correlation id 2 due to node 2 being disconnected (elapsed time since creation: 7ms, elapsed time since send: 7ms, request timeout: 3600000ms) [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 1 disconnected. [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Cancelled in-flight API_VERSIONS request with correlation id 3 due to node 1 being disconnected (elapsed time since creation: 4ms, elapsed time since send: 4ms, request timeout: 3600000ms) [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 0 disconnected. 或是\n1 2 3 4 5 6 7 8 9 10 11 [2024-09-18 16:28:17,666] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 4 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 5 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 6 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 7 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,670] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 8 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:47,774] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 12 on topic-partition mocktest-0, retrying (2147483645 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:29:17,883] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 15 on topic-partition mocktest-0, retrying (2147483644 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation ","date":"2024-06-01T12:45:10+08:00","permalink":"http://localhost:1313/p/k8s-strimzi-kafka-kraft-cluster/","title":"[Kubernetes] 使用 Strimzi 安裝 Kafka Kraft 集群"},{"content":"TL; DR 本篇文章記錄使用 bitnami helm chart 安裝 kafka Kraft mode 的對外集群。\n準備 values.yaml 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 global: storageClass: ceph-csi-rbd-hdd heapOpts: \u0026#34;-Xmx6g -Xms6g\u0026#34; listeners: interbroker: name: INTERNAL containerPort: 9092 protocol: PLAINTEXT controller: name: CONTROLLER containerPort: 9093 protocol: PLAINTEXT client: name: CLIENT containerPort: 9095 protocol: PLAINTEXT external: containerPort: 9094 protocol: PLAINTEXT name: EXTERNAL controller: replicaCount: 3 persistence: size: 50Gi broker: replicaCount: 3 persistence: size: 300Gi externalAccess: enabled: true controller: forceExpose: false service: type: NodePort ports: external: 9094 nodePorts: - 30494 - 30594 - 30694 useHostIPs: true broker: service: type: NodePort ports: external: 9094 nodePorts: - 30194 - 30294 - 30394 useHostIPs: true volumePermissions: enabled: true rbac: create: true kraft: clusterId: M2VhY2Q3NGQ0NGYzNDg2YW 部署 1 helm upgrade --install -name kafka bitnami/kafka --namespace kafka -f values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 Kafka can be accessed by consumers via port 9095 on the following DNS name from within your cluster: kafka.kafka.svc.cluster.local Each Kafka broker can be accessed by producers via port 9095 on the following DNS name(s) from within your cluster: kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-1.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-broker-0.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-1.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-2.kafka-broker-headless.kafka.svc.cluster.local:9095 為 Kafka 設定 Load Balance 部署完畢後可以看到每個 broker / controller 皆使用 NodePort 對外開放，可以為 kafka 設定 Load Balance 以提供外部存取的 client 透過統一的入口點存取。\n目前 Kubernetes 外部已經為集群建立一個 Nginx，我們可以直接使用這台 server 也為 Kafka 的 TCP 流量設置負載轉發。其設定檔如下，將所有會被開到 Node Port 的 Worker Node 節點一次設置上去：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 upstream tcp9094 { server 172.20.37.36:30194 max_fails=3 fail_timeout=30s; server 172.20.37.36:30294 max_fails=3 fail_timeout=30s; server 172.20.37.36:30394 max_fails=3 fail_timeout=30s; server 172.20.37.36:30494 max_fails=3 fail_timeout=30s; server 172.20.37.36:30594 max_fails=3 fail_timeout=30s; server 172.20.37.36:30694 max_fails=3 fail_timeout=30s; server 172.20.37.37:30194 max_fails=3 fail_timeout=30s; server 172.20.37.37:30294 max_fails=3 fail_timeout=30s; server 172.20.37.37:30394 max_fails=3 fail_timeout=30s; server 172.20.37.37:30494 max_fails=3 fail_timeout=30s; server 172.20.37.37:30594 max_fails=3 fail_timeout=30s; server 172.20.37.37:30694 max_fails=3 fail_timeout=30s; server 172.20.37.38:30194 max_fails=3 fail_timeout=30s; server 172.20.37.38:30294 max_fails=3 fail_timeout=30s; server 172.20.37.38:30394 max_fails=3 fail_timeout=30s; server 172.20.37.38:30494 max_fails=3 fail_timeout=30s; server 172.20.37.38:30594 max_fails=3 fail_timeout=30s; server 172.20.37.38:30694 max_fails=3 fail_timeout=30s; server 172.20.37.39:30194 max_fails=3 fail_timeout=30s; server 172.20.37.39:30294 max_fails=3 fail_timeout=30s; server 172.20.37.39:30394 max_fails=3 fail_timeout=30s; server 172.20.37.39:30494 max_fails=3 fail_timeout=30s; server 172.20.37.39:30594 max_fails=3 fail_timeout=30s; server 172.20.37.39:30694 max_fails=3 fail_timeout=30s; server 172.20.37.40:30194 max_fails=3 fail_timeout=30s; server 172.20.37.40:30294 max_fails=3 fail_timeout=30s; server 172.20.37.40:30394 max_fails=3 fail_timeout=30s; server 172.20.37.40:30494 max_fails=3 fail_timeout=30s; server 172.20.37.40:30594 max_fails=3 fail_timeout=30s; server 172.20.37.40:30694 max_fails=3 fail_timeout=30s; server 172.20.37.41:30194 max_fails=3 fail_timeout=30s; server 172.20.37.41:30294 max_fails=3 fail_timeout=30s; server 172.20.37.41:30394 max_fails=3 fail_timeout=30s; server 172.20.37.41:30494 max_fails=3 fail_timeout=30s; server 172.20.37.41:30594 max_fails=3 fail_timeout=30s; server 172.20.37.41:30694 max_fails=3 fail_timeout=30s; server 172.20.37.42:30194 max_fails=3 fail_timeout=30s; server 172.20.37.42:30294 max_fails=3 fail_timeout=30s; server 172.20.37.42:30394 max_fails=3 fail_timeout=30s; server 172.20.37.42:30494 max_fails=3 fail_timeout=30s; server 172.20.37.42:30594 max_fails=3 fail_timeout=30s; server 172.20.37.42:30694 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } 部署 Kafka UI 1 2 3 4 5 6 7 8 9 10 11 yamlApplicationConfig: kafka: clusters: - name: platform bootstrapServers: kafka-broker-headless:9092 auth: type: disabled management: health: ldap: enabled: false 1 helm install -name kafka-ui kafka-ui/kafka-ui -f values.yaml --namespace kafka ","date":"2024-06-01T11:35:00+08:00","permalink":"http://localhost:1313/p/k8s-bitnami-kafka-kraft-helm-chart/","title":"[Kubernetes] 使用 bitnami helm chart 安裝 kafka kraft 集群"},{"content":"建立 PostgreSQL HA 叢集 直接透過 bitnami 的 postgresql-ha helm chart 安裝\n1 helm repo add bitnami https://charts.bitnami.com/bitnami 可以透過指定 values.yaml 安裝\n1 helm install pgsql -f values.yaml bitnami/postgresql-ha -n platform 或是直接在安裝時指定要替換的參數，因為要修改的參數只有 storage class，所以直接用此命令安裝\n1 helm install pgsql bitnami/postgresql-ha -n platform --set global.storageClass=rook-ceph-block 安裝完成後會跳提示，可以取得 DB 的登入密碼，以及說明如何連線。\n1 2 3 4 5 6 7 8 9 10 11 12 13 To get the password for \u0026#34;postgres\u0026#34; run: export POSTGRES_PASSWORD=$(kubectl get secret --namespace platform pgsql-postgresql-ha-postgresql -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) To get the password for \u0026#34;repmgr\u0026#34; run: export REPMGR_PASSWORD=$(kubectl get secret --namespace platform pgsql-postgresql-ha-postgresql -o jsonpath=\u0026#34;{.data.repmgr-password}\u0026#34; | base64 -d) To connect to your database run the following command: kubectl run pgsql-postgresql-ha-client --rm --tty -i --restart=\u0026#39;Never\u0026#39; --namespace platform --image docker.io/bitnami/postgresql-repmgr:16.0.0-debian-11-r15 --env=\u0026#34;PGPASSWORD=$POSTGRES_PASSWORD\u0026#34; \\ --command -- psql -h pgsql-postgresql-ha-pgpool -p 5432 -U postgres -d postgres To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace platform svc/pgsql-postgresql-ha-pgpool 5432:5432 \u0026amp; psql -h 127.0.0.1 -p 5432 -U postgres -d postgres update values.yaml 如果安裝後有需要修改設定，可以再改完 values.yaml 後用 upgrade 指令進行更新。\n1 helm upgrade -f values.yaml pgsql bitnami/postgresql-ha -n platform 建立 PGADMIN4 UI postgresql-ha helm chart 不包含 PGADIMN UI，另外使用 yaml 部署\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 apiVersion: apps/v1 kind: Deployment metadata: name: pgadmin namespace: platform spec: replicas: 1 selector: matchLabels: app: pgadmin template: metadata: labels: app: pgadmin spec: containers: - env: - name: PGADMIN_DEFAULT_EMAIL value: asus.sdsp@gmail.com - name: PGADMIN_DEFAULT_PASSWORD value: \u0026#34;!QAZxsw2\u0026#34; - name: PGADMIN_LISTEN_PORT value: \u0026#34;8001\u0026#34; - name: PGADMIN_CONFIG_WTF_CSRF_CHECK_DEFAULT value: \u0026#34;False\u0026#34; - name: PGADMIN_CONFIG_WTF_CSRF_ENABLED value: \u0026#34;False\u0026#34; - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION value: \u0026#34;False\u0026#34; image: dpage/pgadmin4:7.8 imagePullPolicy: IfNotPresent name: pgadmin ports: - name: tcp8001 containerPort: 8001 --- kind: Service apiVersion: v1 metadata: namespace: platform name: pgadmin spec: type: ClusterIP ports: - name: http protocol: TCP port: 8001 targetPort: tcp8001 selector: app: pgadmin --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: pgadmin namespace: platform annotations: kubernetes.io/ingress.class: nginx #cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;3600\u0026#34; nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;3600\u0026#34; spec: tls: - hosts: - pgadmin.sdsp-stg.com secretName: domain-cert-sdsp-stg.com-prod rules: - host: pgadmin.sdsp-stg.com http: paths: - path: / pathType: Prefix backend: service: name: pgadmin port: number: 8001 Error: The CSRF tokens do not match 如果 PGADMIN 啟用時遇到以下錯誤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2023-11-15 03:03:39,842: ERROR pgadmin: 400 Bad Request: The CSRF tokens do not match. Traceback (most recent call last): File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 261, in protect validate_csrf(self._get_csrf_token()) File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 115, in validate_csrf raise ValidationError(\u0026#34;The CSRF tokens do not match.\u0026#34;) wtforms.validators.ValidationError: The CSRF tokens do not match. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \u0026#34;/venv/lib/python3.11/site-packages/flask/app.py\u0026#34;, line 1821, in full_dispatch_request rv = self.preprocess_request() ^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;/venv/lib/python3.11/site-packages/flask/app.py\u0026#34;, line 2313, in preprocess_request rv = self.ensure_sync(before_func)() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 229, in csrf_protect self.protect() File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 264, in protect self._error_response(e.args[0]) File \u0026#34;/venv/lib/python3.11/site-packages/flask_wtf/csrf.py\u0026#34;, line 307, in _error_response raise CSRFError(reason) 可以加上三個環境變數解決：\nPGADMIN_CONFIG_WTF_CSRF_CHECK_DEFAULT=\u0026ldquo;False\u0026rdquo; PGADMIN_CONFIG_WTF_CSRF_ENABLED=\u0026ldquo;False\u0026rdquo; PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION=\u0026ldquo;False\u0026rdquo; ","date":"2024-05-25T10:57:04+08:00","permalink":"http://localhost:1313/p/k8s-install-ha-postgresql/","title":"[Kubernetes] 安裝高可用 PostgreSQL"},{"content":"安裝 kubectl 可直接參考官網，依照環境選擇想要的安裝方法安裝。\n準備 config 檔案 將兩座 k8s config，存在任意目錄下\n1 2 3 $ mkdir k8s-config $ vi dev $ vi stg 將多個 config 合併 1 2 $ export KUBECONFIG=~/k8s-config/dev:~/k8s-config/stg $ kubectl config view --flatten 將指令返回的 config 複製到 kubernetes 的預設 config 目錄 .kube 下\n1 $ vi ~/.kube/config 並重新修改環境變數\n1 $ export KUBECONFIG=~/.kube/config 成功使用 就能直接透過 kubectl config 指令管理 context 了\n1 $ kubectl config get-contexts 切換存取 Kubernets 叢集\n1 $ kubectl config use-context kubernetes-admin@kubernetes-dev 提示\n其實 config 並不一定要合在一起，分開然後使用環境變數指定多個 config 也可以。看自己的習慣~\n","date":"2024-05-21T10:45:00+08:00","permalink":"http://localhost:1313/p/k8s-kubectl-access-multiple-cluster/","title":"[Kubernetes] kubectl 存取多叢集"},{"content":"TL; DR 紀錄使用 Helm Chart 安裝 Ingress Nginx Controller 以及 Cert Manager 並設置 TLS 憑證的過程。\nIngress Nginx Controller 準備 helm chart 1 2 3 4 5 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm pull ingress-nginx/ingress-nginx tar -zxvf ingress-nginx-xxx.tgz cd ingress-nginx 編輯 values.yaml 設定 kind 類型更改為：DaemonSet 1 2 3 # -- Use a `DaemonSet` or `Deployment` kind: DaemonSet # -- Annotations to be added to the controller Deployment or DaemonSet service template 的 type 改為 NodePort 1 2 3 4 5 6 7 8 9 10 11 12 13 service: # -- Enable controller services or not. This does not influence the creation of either the admission webhook or the metrics service. enabled: true external: # -- Enable the external controller service or not. Useful for internal-only deployments. enabled: true # -- Annotations to be added to the external controller service. See `controller.service.internal.annotations` for annotations to be added to the internal controller service. annotations: {} # -- Labels to be added to both controller services. labels: {} # -- Type of the external controller service. # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types type: NodePort 安裝 1 helm install ingress-nginx --namespace ingress-nginx --create-namespace Cert Manager 準備 helm chart 及安裝 1 2 3 4 5 6 helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true 取得 Cloudflare API Token 將 API Token 的 Permission 設為 Zone.Zone, Zone.DNS ；Resources 設為 All zones。\n建立 Cluster Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: Secret metadata: name: cloudflare-api-token namespace: cert-manager type: Opaque stringData: api-token: uNlI5Slnd-NqqyUX9iNoVlJ2jh57kFAzT5DxKE2E --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prd namespace: cert-manager spec: acme: email: asus.sdsp@gmail.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-prd solvers: - selector: dnsZones: - \u0026#34;sdsp-dev.com\u0026#34; - \u0026#34;*.sdsp-dev.com\u0026#34; dns01: cloudflare: email: asus.sdsp@gmail.com apiTokenSecretRef: name: cloudflare-api-token key: api-token 建立 wildcard 憑證 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: wildcard-sdsp-dev-prd namespace: cert-manager spec: dnsNames: - \u0026#34;sdsp-dev.com\u0026#34; - \u0026#34;*.sdsp-dev.com\u0026#34; issuerRef: kind: ClusterIssuer name: letsencrypt-prd secretName: wildcard-sdsp-dev-prd 更改 ingress-nginx-controller 的 default ssl cert 1 kubectl edit daemonset.apps/ingress-nginx-controller -n ingress-nginx 在 containers 下的 args 加上上面 cert-manager 中創建的 ca\n1 - --default-ssl-certificate=cert-manager/wildcard-sdsp-dev-prd 測試 建立服務的 ingress 資源 假設環境下已有一個 deployment 資源 vitepress\n為 vitepress service 建立 ingress 資源，以供對外存取。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: labels: app.kubernetes.io/name: vitepress name: vitepress-sdsp-dev namespace: dev annotations: nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; spec: ingressClassName: nginx tls: - hosts: - doc.sdsp-dev.com rules: - host: doc.sdsp-dev.com http: paths: - backend: service: name: vitepress port: number: 80 path: /.* pathType: Prefix 並在 Cloudflare 設定 DNS A Record 即可，因為在 Kubernetes 集群前面放了一個 Nginx 做負載均衡，所以可以直接將 Cloudflare DNS Record 的 IP Address 設為 172.20.37.33。\n補充 Kubernetes + Nginx HA 架構圖 提示\n其他常見的 HA 架構為使用 HA Proxy + keepalive\n","date":"2024-05-19T22:20:00+08:00","permalink":"http://localhost:1313/p/k8s-ingress-nginx-cert-manager-setup/","title":"[Kubernetes] Ingress Nginx and Cert Manager Setup"},{"content":"問題 rook-ceph 集群顯示 HEALTH WARN，其原因為 clock skew detected on mon.c, mon.d。\n1 kubectl -n rook-ceph describe cephcluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Status: Ceph: Capacity: Bytes Available: 1174133465088 Bytes Total: 1181116006400 Bytes Used: 6982541312 Last Updated: 2024-04-16T01:21:15Z Details: MON_CLOCK_SKEW: Message: clock skew detected on mon.c, mon.d Severity: HEALTH_WARN Fsid: caec8fab-28a0-464d-9079-463ddbb7c4e3 Health: HEALTH_WARN Last Changed: 2024-04-15T11:31:39Z Last Checked: 2024-04-16T01:21:15Z 解決 原本使用 ntpdate 強制每台都與指定的 ntp server 更新，但還有另一個方法是修改 mon clock drift allowed 預設參數\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 kubectl edit configmap rook-config-override -n rook-ceph -o yaml apiVersion: v1 data: config: | [global] mon clock drift allowed = 1 kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: rook-ceph-cluster meta.helm.sh/release-namespace: rook-ceph creationTimestamp: \u0026#34;2024-04-12T05:38:23Z\u0026#34; labels: app.kubernetes.io/managed-by: Helm name: rook-config-override namespace: rook-ceph ownerReferences: - apiVersion: ceph.rook.io/v1 blockOwnerDeletion: true controller: true kind: CephCluster name: rook-ceph uid: a23004c1-fe54-4905-b9bf-af21a7b2a8ea resourceVersion: \u0026#34;994524\u0026#34; uid: 57ad721c-390a-4999-aa00-9f64e2497310 root@node1:~# kubectl get configmap rook-config-override -n rook-ceph -o yaml apiVersion: v1 data: config: | [global] mon clock drift allowed = 1 kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: rook-ceph-cluster meta.helm.sh/release-namespace: rook-ceph creationTimestamp: \u0026#34;2024-04-12T05:38:23Z\u0026#34; labels: app.kubernetes.io/managed-by: Helm name: rook-config-override namespace: rook-ceph ownerReferences: - apiVersion: ceph.rook.io/v1 blockOwnerDeletion: true controller: true kind: CephCluster name: rook-ceph uid: a23004c1-fe54-4905-b9bf-af21a7b2a8ea resourceVersion: \u0026#34;994524\u0026#34; uid: 57ad721c-390a-4999-aa00-9f64e2497310 新增了以下內容：\n1 2 3 config: | [global] mon clock drift allowed = 1 此處的時間可隨自身的時間差設置，在0.5到1s之間，不建議設定過大的值\n接著刪除 mon Pod 使其載入新的設定文件\n1 2 3 4 kubectl -n rook-ceph delete pod $(kubectl -n rook-ceph get pods -o custom-columns=NAME:.metadata.name --no-headers| grep mon) pod \u0026#34;rook-ceph-mon-a-559d8b5866-dcmcz\u0026#34; deleted pod \u0026#34;rook-ceph-mon-c-bfdbb5598-96kv8\u0026#34; deleted pod \u0026#34;rook-ceph-mon-d-c9ff49c58-ml65k\u0026#34; deleted 查看狀態已恢復健康值\n1 2 3 kubectl -n rook-ceph get cephcluster NAME DATADIRHOSTPATH MONCOUNT AGE PHASE MESSAGE HEALTH EXTERNAL FSID rook-ceph /var/lib/rook 3 3d19h Ready Cluster created successfully HEALTH_OK caec8fab-28a0-464d-9079-463ddbb7c4e3 Reference https://blog.csdn.net/m0_59615922/article/details/131459393 ","date":"2024-05-01T22:20:00+08:00","permalink":"http://localhost:1313/p/k8s-rook-ceph-clock-skew-detected/","title":"[Kubernetes] Rook Health Warn with Clock Skew"},{"content":"TL; DR 原本使用 Kubernetes 的三個 worker node 節點建立了擁有三個 node 的 rook，但因為資源不足的緣故，將其中一個節點由原本的 VM 改成以實機的方式加入集群，導致硬碟原本是以 /dev/sdb 的方式加入 OSD，但現在需要改成 /dev/sda。\n先將 rook-ceph-operator 停用 1 kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0 修改 rook cluster 配置，指定節點使用 sda 硬碟。 1 2 3 4 5 6 7 8 9 10 # kubectl edit cephclusters.ceph.rook.io -n rook-ceph rook-ceph storage: flappingRestartIntervalHours: 0 nodes: - devices: - name: sda name: node5 store: {} useAllDevices: true useAllNodes: true 部署 ceph toolbox 1 kubectl apply -f toolbox.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 # toolbox.yaml apiVersion: apps/v1 kind: Deployment metadata: name: rook-ceph-tools namespace: rook-ceph # namespace:cluster labels: app: rook-ceph-tools spec: replicas: 1 selector: matchLabels: app: rook-ceph-tools template: metadata: labels: app: rook-ceph-tools spec: dnsPolicy: ClusterFirstWithHostNet serviceAccountName: rook-ceph-default containers: - name: rook-ceph-tools image: quay.io/ceph/ceph:v18.2.2 command: - /bin/bash - -c - | # Replicate the script from toolbox.sh inline so the ceph image # can be run directly, instead of requiring the rook toolbox CEPH_CONFIG=\u0026#34;/etc/ceph/ceph.conf\u0026#34; MON_CONFIG=\u0026#34;/etc/rook/mon-endpoints\u0026#34; KEYRING_FILE=\u0026#34;/etc/ceph/keyring\u0026#34; # create a ceph config file in its default location so ceph/rados tools can be used # without specifying any arguments write_endpoints() { endpoints=$(cat ${MON_CONFIG}) # filter out the mon names # external cluster can have numbers or hyphens in mon names, handling them in regex # shellcheck disable=SC2001 mon_endpoints=$(echo \u0026#34;${endpoints}\u0026#34;| sed \u0026#39;s/[a-z0-9_-]\\+=//g\u0026#39;) DATE=$(date) echo \u0026#34;$DATE writing mon endpoints to ${CEPH_CONFIG}: ${endpoints}\u0026#34; cat \u0026lt;\u0026lt;EOF \u0026gt; ${CEPH_CONFIG} [global] mon_host = ${mon_endpoints} [client.admin] keyring = ${KEYRING_FILE} EOF } # watch the endpoints config file and update if the mon endpoints ever change watch_endpoints() { # get the timestamp for the target of the soft link real_path=$(realpath ${MON_CONFIG}) initial_time=$(stat -c %Z \u0026#34;${real_path}\u0026#34;) while true; do real_path=$(realpath ${MON_CONFIG}) latest_time=$(stat -c %Z \u0026#34;${real_path}\u0026#34;) if [[ \u0026#34;${latest_time}\u0026#34; != \u0026#34;${initial_time}\u0026#34; ]]; then write_endpoints initial_time=${latest_time} fi sleep 10 done } # read the secret from an env var (for backward compatibility), or from the secret file ceph_secret=${ROOK_CEPH_SECRET} if [[ \u0026#34;$ceph_secret\u0026#34; == \u0026#34;\u0026#34; ]]; then ceph_secret=$(cat /var/lib/rook-ceph-mon/secret.keyring) fi # create the keyring file cat \u0026lt;\u0026lt;EOF \u0026gt; ${KEYRING_FILE} [${ROOK_CEPH_USERNAME}] key = ${ceph_secret} EOF # write the initial config file write_endpoints # continuously update the mon endpoints if they fail over watch_endpoints imagePullPolicy: IfNotPresent tty: true securityContext: runAsNonRoot: true runAsUser: 2016 runAsGroup: 2016 capabilities: drop: [\u0026#34;ALL\u0026#34;] env: - name: ROOK_CEPH_USERNAME valueFrom: secretKeyRef: name: rook-ceph-mon key: ceph-username volumeMounts: - mountPath: /etc/ceph name: ceph-config - name: mon-endpoint-volume mountPath: /etc/rook - name: ceph-admin-secret mountPath: /var/lib/rook-ceph-mon readOnly: true volumes: - name: ceph-admin-secret secret: secretName: rook-ceph-mon optional: false items: - key: ceph-secret path: secret.keyring - name: mon-endpoint-volume configMap: name: rook-ceph-mon-endpoints items: - key: data path: mon-endpoints - name: ceph-config emptyDir: {} tolerations: - key: \u0026#34;node.kubernetes.io/unreachable\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 5 進入 toolbox 執行移除 sdb osd 操作 1 kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash 手動移除對應osd\n1 2 3 4 5 6 7 8 9 ceph osd set noup ceph osd down 0 ceph osd out 0 # 查看数据均衡进度， 等待数据均衡完成 ceph -w # 均衡資料完成後移除對應的osd ceph osd purge 0 --yes-i-really-mean-it ceph auth del osd.0 ceph osd crush remove node5 檢查ceph狀態以及osd狀態\n1 2 ceph -s ceph osd tree 移除 pod，並判斷刪除對應的 job 1 kubectl delete deploy -n rook-ceph rook-ceph-osd-0 進入 node5 節點，並清除磁盤資料 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/env bash DISK=\u0026#34;/dev/sdb\u0026#34; # Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean) # You will have to run this step for all disks. sgdisk --zap-all $DISK # These steps only have to be run once on each node # If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks. ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove % # ceph-volume setup can leave ceph-\u0026lt;UUID\u0026gt; directories in /dev (unnecessary clutter) rm -rf /dev/ceph-* lsblk -f rm -rf /var/lib/rook/* 重新將 rook-ceph-operator 啟用，node5 sda osd 便會自動新增 1 kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0 Reference https://blog.csdn.net/fancy106/article/details/121706998 ","date":"2024-04-30T14:45:00+08:00","permalink":"http://localhost:1313/p/k8s-rook-delete-osd-and-add-on-same-node/","title":"[Kubernetes] Rook 刪除原先 /dev/sdb 的 OSD 並重新新增同個節點的 /dev/sda 的 OSD"},{"content":"TL; DR 使用 Rook Helm Chart 在 Kubernetes 上安裝 Rook Operator 以及 Ceph 集群，並透過 CSI 使用 PVC。\nPrerequisites Kubernetes：支持 v1.25 至 v1.29 版本 Ceph Node Resource Requiement：不同組件需要不同的硬體資源大小，請參考 SUSE 文檔 了解具體要求。 提示\n測試發現，三個工作節點配置為 4 core vCPU / 8GB RAM 會導致安裝失敗，升級至 8 core vCPU / 16GB RAM 後才能順利運行。\n至少需要以下一種本地儲存類型：\n原始硬碟（無分區或格式化文件系統） 原始硬碟分區（無格式化文件系統） LVM 邏輯卷（無格式化文件系統） 在已存在的 Storage Class 中，以 block level 型態提供的 Persistent Volumes RBD：\nCeph 需要使用具有 RBD 模組的 Linux kernel。許多 Linux 發行版都已經包含了 RBD 模組，但不是所有的發行版都有，使用下面 command 確認及載入\n1 2 3 4 5 6 lsmod | grep rbd # 若無返回，則執行下面載入 sudo modprobe rbd sudo vim /etc/modules-load.d/rbd.conf\t# 文件名任意，以.conf 結尾即可 rbd\t#内容寫 rbd 即可 Installation Step 整體流程如下\n安裝 Rook Operator 部署 Ceph 叢集 驗證 Ceph 集群 使用 Ceph 存儲 Install Rook Operator 1 2 helm repo add rook-release https://charts.rook.io/release helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph 資訊\n如果叢集是一個三節點的叢集（1 個 Master + 2 個 Worker），那麼 Master 節點也需要作為工作負載節點使用，可以去掉欲安裝的 master 節點的污點：\n1 kubectl taint node node1 node-role.kubernetes.io/master:PreferNoSchedule- Install Rook Cluster 準備 values.yaml\n1 2 3 4 5 operatorNamespace: rook-ceph cephClusterSpec: dashboard: enabled: true ssl: false 安裝 cluster\n1 helm install --namespace rook-ceph rook-ceph-cluster rook-release/rook-ceph-cluster -f values.yaml Verify the Ceph Cluster Installation 1 kubectl -n rook-ceph get cephcluster 當健康狀態返回 HEALTH_OK\n1 kubectl -n rook-ceph get all 且所有部屬元件皆正常 Running 後\n便可直接進入下一步的佈署 pvc 資源測試\nDeploy a Validation PVC and Pod 查看 StorageClass\n1 kubectl get sc 使用 block level storage 宣告 PVC\n1 2 3 4 5 6 7 8 9 10 11 12 13 # pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc namespace: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: ceph-block 1 kubectl apply -f pvc.yaml 當 STATUS 返回 Bound 就成功了。\nCompletely Clean Rook 當安裝失敗要重新安裝時，需要執行以下步驟，才能重新安裝。\nUninstall all rook resource and delete the namespace\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 helm uninstall rook-ceph-cluster -n rook-ceph # 測試後發現刪除 chart 之後不會刪除自動刪除相應資源，需額外刪除 kubectl -n rook-ceph delete deployment.apps/rook-ceph-crashcollector-node4 kubectl -n rook-ceph delete deployment.apps/rook-ceph-crashcollector-node5 kubectl -n rook-ceph delete deployment.apps/rook-ceph-crashcollector-node6 kubectl -n rook-ceph delete deployment.apps/rook-ceph-mgr-a kubectl -n rook-ceph delete deployment.apps/rook-ceph-mgr-b kubectl -n rook-ceph delete deployment.apps/rook-ceph-mon-b kubectl -n rook-ceph delete deployment.apps/rook-ceph-mon-a kubectl -n rook-ceph delete deployment.apps/rook-ceph-mon-c kubectl -n rook-ceph delete deployment.apps/rook-ceph-mds-ceph-filesystem-a kubectl -n rook-ceph delete deployment.apps/rook-ceph-mds-ceph-filesystem-b kubectl -n rook-ceph delete job.batch/rook-ceph-csi-detect-version kubectl -n rook-ceph delete job.batch/rook-ceph-osd-prepare-node4 kubectl -n rook-ceph delete job.batch/rook-ceph-osd-prepare-node5 kubectl -n rook-ceph delete job.batch/rook-ceph-osd-prepare-node6 kubectl -n rook-ceph delete service/rook-ceph-exporter kubectl -n rook-ceph delete service/rook-ceph-mgr kubectl -n rook-ceph delete service/rook-ceph-mgr-dashboard kubectl -n rook-ceph delete service/rook-ceph-mon-a kubectl -n rook-ceph delete service/rook-ceph-mon-b kubectl -n rook-ceph delete service/rook-ceph-mon-c kubectl -n rook-ceph delete service/rook-ceph-rgw-ceph-objectstore # 除了 k8s 預設的資源外，還有以下的 CRD 資源須刪除 kubectl -n rook-ceph delete cephobjectstore ceph-objectstore \u0026amp; kubectl -n rook-ceph delete cephfilesystem ceph-filesystem \u0026amp; kubectl -n rook-ceph delete cephblockpool ceph-blockpool \u0026amp; kubectl -n rook-ceph delete cephcluster rook-ceph \u0026amp; kubectl -n rook-ceph get cephobjectstores.ceph.rook.io ceph-objectstore -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - kubectl -n rook-ceph get cephblockpools.ceph.rook.io ceph-blockpool -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - kubectl -n rook-ceph get cephfilesystems.ceph.rook.io ceph-filesystem -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - kubectl -n rook-ceph get cephcluster.ceph.rook.io rook-ceph -o json | jq \u0026#39;.metadata.finalizers = null\u0026#39; | kubectl -n rook-ceph apply -f - # 移除 configmap kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;: []}}\u0026#39; kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;: []}}\u0026#39; kubectl delete configmap -n rook-ceph rook-ceph-csi-config kubectl delete configmap -n rook-ceph rook-ceph-csi-mapping-config kubectl delete configmap -n rook-ceph rook-ceph-operator-config kubectl delete configmap -n rook-ceph rook-ceph-pdbstatemap # 移除 Operator helm uninstall rook-ceph -n rook-ceph # 移除 CRD for CRD in $(kubectl get crd -n rook-ceph | awk \u0026#39;/ceph.rook.io/ {print $1}\u0026#39;); do kubectl get -n rook-ceph \u0026#34;$CRD\u0026#34; -o name | xargs -I {} kubectl patch -n rook-ceph {} --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;: []}}\u0026#39;; done kubectl get crd | grep rook.io | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete crd # 最終刪除 namespace kubectl delete ns rook-ceph Clear Rook data from Ceph node and zap the storage devices\n假設存儲設備是 /dev/sdb 磁盤，需要刪除該設備所有數據。\n1 2 3 4 5 6 7 8 9 10 # remove data sudo rm -rvf /var/lib/rook # zapping devices DISK=\u0026#34;/dev/sdb\u0026#34; sudo sgdisk --zap-all $DISK sudo dd if=/dev/zero of=\u0026#34;$DISK\u0026#34; bs=1M count=100 oflag=direct,dsync # SSDs may be better cleaned with blkdiscard instead of dd sudo blkdiscard $DISK sudo partprobe $DISK ","date":"2024-03-30T10:45:00+08:00","permalink":"http://localhost:1313/p/k8s-rook-installation/","title":"[Kubernetes] Rook 1.13 (Ceph on Kubernetes) 安裝紀錄"},{"content":"TL; DR 紀錄使用 Kubespray 透過 Ansible 快速部署 Kubernetes 集群。\n環境 Ansible 控制節點 Ubuntu 22.04 LTS 2 CPU 4GB RAM 20GB Disk 3 個 master 節點 Ubuntu 22.04 LTS 2 CPU 4GB RAM 20GB Disk 3 個 worker 節點 Ubuntu 22.04 LTS 4 CPU 8GB RAM 80GB Disk Kubespray 最低要求 Ansible 節點：1024 MB、1 個 CPU 與 20 GB 磁碟空間 Master：1500 MB RAM、2 個 CPU 和 20 GB 可用磁碟空間 Worker：1024 MB、2 個 CPU、20 GB 可用磁碟空間 每個節點上的互聯網連接 擁有 sudo 管理員權限 配置 Ansible 控制節點 安裝所需套件 1 2 3 4 5 sudo apt update sudo apt install git python3-pip -y git clone https://github.com/kubernetes-incubator/kubespray.git cd kubespray sudo pip install -r requirements.txt 複製執行 ssh 金鑰 1 2 3 4 5 6 7 ssh-keygen # 如果在 ~/.ssh/ 下沒有金鑰創建的話 ssh-copy-id ula@192.168.0.48 ssh-copy-id ula@192.168.0.152 ssh-copy-id ula@192.168.0.225 ssh-copy-id ula@192.168.0.233 ssh-copy-id ula@192.168.0.131 ssh-copy-id ula@192.168.0.241 準備 Ansible Host 清單 1 2 cp -rfp inventory/sample inventory/mycluster declare -a IPS=(192.168.0.48 192.168.0.152 192.168.0.225 192.168.0.233 192.168.0.131 192.168.0.241)CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]} 修改清單，設置 master node 及 worker node 各 3 個\n1 vi inventory/mycluster/hosts.yaml 修改 K8s 部署變數 1 vi inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml 主要修改以下選項：\n1 2 3 4 kube_version: v1.29.2 kube_network_plugin: calico # default kube_pods_subnet: 10.233.64.0/18 # default kube_service_addresses: 10.233.0.0/18 # default 修改安裝套件 1 vi inventory/mycluster/group_vars/k8s_cluster/addons.yml 主要開啟以下功能\n1 2 3 4 helm_enabled: true dashboard_enabled: true ingress_nginx_enabled: true ingress_nginx_host_network: true 配置 Ansible 遠端節點 設定要 ssh 遠端執行的帳號免密碼使用 sudo\n1 echo \u0026#34;ula ALL=(ALL) NOPASSWD:ALL\u0026#34; | sudo tee /etc/sudoers.d/ula k8s 事前環境設定 回到 Ansible 控制節點，透過遠端執行一次處理 Kubernetes 安裝前的環境基本設定。\n1 2 3 4 5 6 7 8 9 10 cd kubespray # 禁用防火牆 ansible all -i inventory/mycluster/hosts.yaml -m shell -a \u0026#34;sudo systemctl stop firewalld \u0026amp;\u0026amp; sudo systemctl disable firewalld\u0026#34; # 禁用 IPv4 轉發 ansible all -i inventory/mycluster/hosts.yaml -m shell -a \u0026#34;echo \u0026#39;net.ipv4.ip_forward=1\u0026#39; | sudo tee -a /etc/sysctl.conf\u0026#34; # 禁用 swap ansible all -i inventory/mycluster/hosts.yaml -m shell -a \u0026#34;sudo sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab \u0026amp;\u0026amp; sudo swapoff -a\u0026#34; 開始部署 1 ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml 完成後查看執行結果是否成功。 操作集群 登入第一個 master node，切換到 root，就可以透過 kubectl 命令操作集群了!\n1 2 sudo su - kubectl get nodes Reference https://linux.cn/article-15675-1.html ","date":"2024-03-23T16:45:00+08:00","permalink":"http://localhost:1313/p/k8s-kubespray-installation/","title":"[Kubernetes] 使用 Kubespray 快速部署 K8s 集群"},{"content":"TL; DR Kong 有不同部署架構，本文僅示範單一的 kong gateway 節點在 Kubernetes 上的安裝方式。透過官方的 helm chart 部署，包含 kong admin、 kong proxy 以及原生的 Kong Admin UI - Kong Manager。\n安裝步驟 建立 namesapce 1 kubectl create ns kong 建立 postgresql database 1 2 3 4 5 6 7 8 9 10 11 # postgre-values.yaml image: tag: 10.23.0 # 11 版後 kong 不相容 global: storageClass: \u0026#34;rook-ceph-block\u0026#34; postgresql: auth: postgresPassword: \u0026#34;kong\u0026#34; username: \u0026#34;kong\u0026#34; password: \u0026#34;kong\u0026#34; database: \u0026#34;kong\u0026#34; 1 helm install kong-pg -f postgre-values.yaml bitnami/postgresql -n kong 建立 kong (含 kong gateway \u0026amp; kong manager) 主要修改 kong-values.yaml 的項目如下：\n設定外部 PostgreSQL Database 關閉 Ingress Controller，如果要建立 ingress 資源，統一使用 Ingress Nginx Controller 關閉所有開啟的服務的 https port，統一使用 http，並透過 Ingress 加上原先已存在的 Cert-Manager Certificate 暴露服務 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 # kong-values.yaml env: database: \u0026#34;postgres\u0026#34; pg_host: \u0026#34;kong-pg-postgresql.kong.svc.cluster.local\u0026#34; pg_port: 5432 pg_user: kong pg_password: kong pg_database: kong # 中間略 admin: enabled: true type: ClusterIP loadBalancerClass: annotations: {} labels: {} http: enabled: true servicePort: 8001 containerPort: 8001 parameters: [] tls: enabled: false servicePort: 8444 containerPort: 8444 parameters: - http2 client: caBundle: \u0026#34;\u0026#34; secretName: \u0026#34;\u0026#34; ingress: enabled: true ingressClassName: nginx # TLS secret name. tls: domain-cert-sdsp-stg.com-prod # Ingress hostname hostname: kong-admin.sdsp-stg.com annotations: {} # Ingress path. path: / pathType: ImplementationSpecific # 中間略 proxy: enabled: true type: ClusterIP loadBalancerClass: nameOverride: \u0026#34;\u0026#34; annotations: {} labels: enable-metrics: \u0026#34;true\u0026#34; http: enabled: true servicePort: 80 containerPort: 8000 parameters: [] tls: enabled: false servicePort: 443 containerPort: 8443 parameters: - http2 appProtocol: \u0026#34;\u0026#34; stream: [] ingress: enabled: true ingressClassName: nginx annotations: {} labels: {} hostname: kong.sdsp-stg.com path: / pathType: ImplementationSpecific hosts: [] # TLS secret(s) tls: domain-cert-sdsp-stg.com-prod # 中間略 ingressController: enabled: false # 中間略 manager: enabled: true type: ClusterIP loadBalancerClass: annotations: {} labels: {} http: enabled: true servicePort: 8002 containerPort: 8002 parameters: [] tls: enabled: false servicePort: 8445 containerPort: 8445 parameters: - http2 ingress: enabled: true ingressClassName: nginx tls: domain-cert-sdsp-stg.com-prod hostname: kong-manager.sdsp-stg.com annotations: {} path: / pathType: ImplementationSpecific 1 2 helm repo add kong https://charts.konghq.com helm install kong-dev -f kong-values.yaml kong/kong -n kong ","date":"2024-03-16T16:44:00Z","permalink":"http://localhost:1313/p/single-kong-gw-deploy-on-kubernetes/","title":"[kong] Single Gateway Deployment on Kubernetes"},{"content":"TL; DR 在公司部屬 Ingress 資源後，發現一直沒法法吃到指定的憑證，結果才發現是因為 wildcard 的問題。\nProblem 公司 Kubernetes 環境的 ingress gateway 有預設的 tls 憑證 (*.southeastasia.azure.wistron.com)，我自己申請的憑證為 *.wistron.com，想要指定給 tls 路徑為 abc.southeastasia.azure.wistron.com 的網站，但部署後連網頁發現吃的憑證會是預設的，而不是指定的。\nReason 使用 wildcard 的憑證，只能吃到第一階的域名，比如說如果有 *.example.com 的 SSL 證書，那麼僅適用於 www.example.com 或 XXXX.example.com 等主機，不適用於 demo.app1.example.com 等主機。\nReference https://stackoverflow.com/questions/26744696/ssl-multilevel-subdomain-wildcard ","date":"2023-07-23T17:45:00+08:00","permalink":"http://localhost:1313/p/ingress-%E6%8C%87%E5%AE%9A%E4%BA%86-tls-%E6%86%91%E8%AD%89%E5%8D%BB%E5%90%83%E5%88%B0%E4%B8%8D%E6%AD%A3%E7%A2%BA%E7%9A%84/","title":"[Ingress] 指定了 TLS 憑證，卻吃到不正確的"},{"content":"TL; DR 因為疫情，距離上一次去沖繩居然已過了將近四年。Covid-19 從一開始的半信半疑到大爆發的人人恐慌，兩年多過去，人們開始與病毒共處。今年全球旅遊終於都開放啦! 而今年連假又特別多，年初一直有打算出國，甚至連日幣都趁貶值的時候換起來放了，拖到三月中才終於決定要在端午連假安排出國。但殊不知六月的日本根本不是適合去 XDD 3/12 臨時改變目的地，也從本來的自住行改成跟團旅遊，花了三天討論，最後找到一個剛好卡在連假又團費一個人不包含小費兩萬一超便宜的馬來西亞五天四夜遊。原本擔心的行程跟住宿品質，在去玩後沒想到都在期望值以上!\n行前準備 馬幣在台灣屬於冷門貨幣，所以銀行賣出 7.多 買入 5.多 的匯率完全不划算，才發現大家如果要換馬幣都在當地換。幸好刷了兩天背包客棧，直接跟一個剛從馬來西亞回來的包友用匯率 7 換到 $2500 的馬幣，還可以在台北面交，面交完順便去東區逛街治裝 ٩(ˊᗜˋ*)و 另外也順利在出發前買好網卡，兩張 5 天 10 G 開熱點分享綽綽有餘!\nDAY 1 - 馬六甲 起飛 雖然是跟團，但早上六點半的班機也幾乎是紅眼班機了，拔麻姐他們一點半從嘉義出發。早上臨時決定叫車去，本來用大都會 app 媒合了一個上午未果，後來客服打來最後幸好有媒合到車 (。•́︿•̀。)，從淡水竹圍(馬上用了新公司的全薪事假請 (⑉¯ꇴ¯⑉) ) 回到中和還能收行李洗澡睡覺三點半出發～雖然太興奮了也是睡睡醒醒的 XD 一到機場去五樓跟拔麻姐會合在 711 吃早(宵?)餐(夜?)，連假的機場果然超熱鬧，一大清早就擠滿了要出國玩的人們，空氣都瀰漫著開心的氛圍! 四點半到集合地點會合，這團總共有 19 位團員~有四小分隊! 登機需要搭接駁車在機場中間上飛機，一上飛機就立馬補個眠，不過飛機餐馬上就送來了，吃了蕃茄海鮮麵，在看部電影，再去戴個隱形眼鏡梳妝打扮一下，四個半小時的飛程很快就到了!\n休息站 因為從吉隆坡下飛機後到入境後大約 12 點，第一天的行程要直接開往麻六甲，兩個半小時的車程太久，當地導遊 (國外跟團原來除了領隊外，還會有當地導遊) 在路上先安插了一站休息站讓我們先充饑，在這裡吃了椰漿飯、蜂蜜吐司配生雞蛋、河粉、咖哩包、炸雞、美祿~ 隔壁還有熱情的印度人分享當地水果龍宮果。\n荷蘭紅屋、聖保羅教堂 第一站到荷蘭殖民時所建設的荷蘭紅屋，望眼放去是清一色的紅色建築，有種來到淡水紅毛城的錯覺 XDD 接著莫名其妙地坐著羞恥的腳踏花車，邊大聲的播著蝦趴的音樂 (你是我的花朵之類的 XDDD) 羞恥的到達下一站聖保羅教堂。跟團就是會有這個莫名其妙但媽媽可能很喜歡的行程哈哈哈 聖保羅教堂遺址處，古城已然滄桑變遷，可以感受到歷史的厚重。 往上走到到制高點，可以眺望馬六甲的風景。\n在這邊可以見證了馬六甲在葡萄牙、荷蘭、英國等殖民勢力之間的更迭。\n海峽清真寺 接著來到建在人工島上的海峽清真寺，太陽映照下的景色很美。為了尊重當地的宗教習俗，女性進入清真寺僅能露出臉部，於是乎就體驗了首次的伊斯蘭服飾 🤣。 晚餐 \u0026amp; 飯店 晚餐吃馬六甲娘惹村的合菜，飯店住 ECO Tree，因為沒有三人房型，所以是雙人床再加一張行軍床，幸好行軍床很好睡 (`･∀･)b 晚上一家人走到附近的商場，然後在全家買了隻榴槤冰吃~!\nDAY 2 - 馬六甲 雞場街 一早來到馬六甲市中心的一條雞場街，買一些伴手禮， 觀光有故事的咖啡館，另外總是對這種涼茶沒什麼抗拒力 中途在一間體驗娘惹服的茶館吃了五顏六色的娘惹糕 還吃了榴槤泡芙 (各種榴槤 XDDD)，接著也在這邊享用午餐雞飯粒。 Malaysia Heritage Studios 這個景點是以馬來西亞文化遺產為主題，園區內有13個州的馬來傳統房屋，每個房屋都意外的好拍 (❛◡❛✿) 但天氣實在是太熱了，大家都逛很快，最後變成擠在入口的商店小屋吹冷氣，殊不知全玻璃的牆面一點涼感都沒有，所以我們一家在時間還沒結束前就上遊覽車吹冷氣了 (๑¯∀¯๑)\n另外在離開景點後，特地路過水果攤，終於吃到榴槤山竹啦!!!!!!!! 晚餐 \u0026amp; 飯店 晚餐是在去飯店的路上吃的有點台式的合菜，飯店住 Lexis 紅花系列的飯店 (非五星級那個 🤣)，但一樣在很酷的水上，然後廁所在進門處的設計實在是太新奇了??? DAY 3 - 吉隆坡 第三天終於遇到熱帶型氣候了，幸好雨都來得快去得快。\n國家皇宮（Istana Negara） 這邊是馬來西亞最高元首的宮邸，只能在外觀參觀拍照，也有騎著馬的憲兵駐守。\n中餐終於吃到馬來西亞必吃的肉骨茶，吃完還在店門口外再買意外好吃的椰子雪花冰 ( ˘•ω•˘ ) 獨立廣場 今天是很政治的一天 XDD 獨立廣場是英國統治結束後宣布獨立的地方，旁邊為保留百年的印刷局外觀，館內展示一些吉隆坡的歷史，還有縮尺的大型城市模型。另外還有在國旗杆前的 0 公里的石碑，代表吉隆坡的市中心本身! (不知道最後一張導遊的手機畫質花生什麼事 XDDD ???\n國家銀行博物館 展示各式各樣的舊錢幣、紀念幣還有各國錢幣等，是馬來西亞錢幣發展史的研究中心。\n(上圖表示找到新台幣 ( • ̀ω•́ )\n雙子星 先到四季酒店的商店街吃晚餐，再自由時間回到 LLC 城中城陽光廣場購物中心逛街。 然後是在雙子星大廈前留影 接著在吉隆坡市中心穿梭前往 Saloma Link 彩色行人天橋，印象最深刻的就是斑馬線綠燈長度短!到!不!行!，每次過馬路都得奔跑 XDDD 飯店 最後兩晚的飯店都住接待大廳像霍格華茲的 Bespoke Hotel Puchong 蒲種定制酒店，最令人滿意的它的地理位置，大門過馬路後有一個賣山竹榴槤的水果攤，後門有一間印度料理，連續兩晚都吃了甩餅 (´ڡ`) 最後一天晚上還因為要把換的馬來幣全花光，在附近的便利商店狂掃零食，最後在床上擺出本次的戰利品，有點壯觀吶 ( ° ▽°)ノ DAY 4 - 吉隆坡 黑風洞 是印度教聖地，要進入黑風洞前有色彩繽紛的階梯。裡面是高聳的洞穴和壯觀的印度教神像，外面被猴子跟鴿子包圍著，果然是聖地! 另外還大開眼見的看到苦行遊行。 觀光完還去美食商店街喝了燕窩跟椰子 (´ڡ`) REXKL 迷宮書店 位於當地茨廠街的複合式商場 REXKL 內，書店的佈局非常獨特，就像一個迷宮一樣，照拍起來書香味都出來了 (ㆆᴗㆆ) 茨廠街、鬼仔巷 接著步行到這兩個地方，茨廠街就是那種亞洲都一定會有的仿精品街 Σ(☉▽☉\u0026ldquo;a 鬼仔巷的建築風格融合了馬來、華人、印度和歐洲的元素。巷道兩旁林立著餐廳，有著粉彩建築。 The LINC KL 來購物中心內部踩點打卡景點，包括百年老樹、4萬多隻的彩紅紙鶴跟彩虹階梯! 午餐晚餐 午餐(左上)又是吃合菜，晚餐吃亞羅街夜市吃黃亞華燒烤餐廳的燒雞翼及青檸話梅汁，再小逛一下這條小夜市，然後又又又是榴槤粉條 XDD。 DAY 5 - 吉隆坡 粉紅清真寺 這邊不得不來個 photo dump，因為第一次看到粉得這麼漂亮的建築，超級夢幻 (｡í _ ì｡) 今天天氣又變超棒，太陽大到不行 XD 很幸運沒遇到禮拜，所以可以進去參觀，內裝天花板也是美的驚人! 在洗手間旁的河畔風景也很漂亮 返程 回程的最後把剩下的零錢全部花在機場星巴克，用得可謂是一毛不剩，完美的結束五天四夜的馬來西亞遊了 (˶´U`˵) 心得 在此行之前，因為彈性度不高所以原本對跟團旅遊挺排斥的。但這趟下來，終於體會到跟團的好處了，不用煩惱交通及行程，導遊還會依照景點位置彈性安排，讓路途更加順暢，只要當天開心出遊、然後安心回家即可~ 下次自由行不方便的國家，就直接無腦跟團啦!!\n","date":"2023-06-08T15:00:00+08:00","permalink":"http://localhost:1313/p/malaysia-2023/","title":"五天四夜馬來西亞遊記🏝️"},{"content":"TL;DR 本篇文章記錄如何在 aplpine base 的 container 中於每月的最後一個禮拜日執行指定任務。\nSoluiton 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: batch/v1 kind: CronJob metadata: name: hello namespace: test spec: schedule: \u0026#34;0 8 * * 0\u0026#34; jobTemplate: spec: template: spec: containers: - name: hello image: alpine:3.16 resources: limits: cpu: \u0026#34;2\u0026#34; memory: 4000Mi imagePullPolicy: Always command: - /bin/sh - -c - \u0026#39;[ $(date +%m) -ne $(date -d \u0026#34;@$(($(date +%s) + 604800))\u0026#34; +%m) ] \u0026amp;\u0026amp; echo Hello from the Kubernetes cluster\u0026#39; restartPolicy: OnFailure 坑 原本前面判斷的語法是寫在 ubuntu 下 run 得好好的 [ $(date +%m) -ne $(date -d +7days +%m) ]，但是殊不知搬到 alpine container 後會報錯 date: invalid date '+7days'。所以改寫成 apline 環境中認得的 -d 格式！\nReference https://stackoverflow.com/questions/71651529/schedule-cronjob-for-last-day-of-the-month-using-kubernetes https://unix.stackexchange.com/a/522622 ","date":"2022-10-23T19:08:00Z","permalink":"http://localhost:1313/p/cronjob-for-last-day-of-the-month-using-kubernetes/","title":"使用 alpine base image 在指定時間定期執行 CronJob"},{"content":"TL;DR 在 windows 環境的 wsl2 Ubuntu 上跑了 Minikube 要用來測試 kubernetes 的應用佈屬，但一直卡在拉取公司內部 harbor 鏡像的時候報 x509: certificate signed by unknown authority 錯誤。\n嘗試在 docker desktop 的 docker engine 加上 insecure-registries 的參數但是 minikube 不曉得為什麼吃不到，也試過把憑證放到 /etc/ssl/certs 下但一樣不認得 =__=? 總之最後才發現 minikube 的 docker 跟 docker desktop 的 docker 環境是分開的。\nSolution 1 vi ~/.minikube/machines/\u0026lt;PROFILE_NAME\u0026gt;/config.json (in my case ~/.minikube/machines/minikube/config.json) add private repo on InsecureRegistry attribute (json path: HostOptions.EngineOptions.InsecureRegistry) 1 2 minikube stop minikube start Then, change the Docker daemon from Minikube\n1 eval $(minikube docker-env) Reference https://stackoverflow.com/questions/38748717/can-not-pull-docker-image-from-private-repo-when-using-minikube https://tachingchen.com/tw/blog/build-docker-image-in-minikube-vm/ https://stackoverflow.com/questions/52310599/what-does-minikube-docker-env-mean ","date":"2022-10-23T19:07:00Z","permalink":"http://localhost:1313/p/minikube-pull-harbor-image/","title":"Minikube Pull Image from Private Repository in WSL2"},{"content":"TL;DR 在上一篇文章中試了在 alpine docker container 中使用 non root user 跑 crond，但將 build 好的 docker image 搬到 kubernetes 給 deployment 的 pod 使用時，卻會出現 initgroup operation not permitted 的錯誤。\n踩坑踩了整整三天，該改的權限都改了，最後終於找到 supercronic 這個酷東西 T_T\nDockerfile 這邊的 dockerfile 直接先下載好 supercronic build 好的 binary，再 COPY 進 image 中，也可以參考 installation instruction 在 build 的階段下載。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # We want to populate the module cache based on the go.{mod,sum} files. COPY go.mod . COPY go.sum . RUN go mod download COPY . . # Build the Go app RUN go build -o ./out/ccoe-bot . # Start fresh from a smaller image FROM harbor.wistron.com/base_image/alpine:3.12 USER root RUN apk update \u0026amp;\u0026amp; addgroup --gid 1000 ccoebot \u0026amp;\u0026amp; adduser --disabled-password --ingroup ccoebot --uid \u0026#34;1000\u0026#34; ccoebot COPY --from=build_base --chown=ccoebot:ccoebot /tmp/ccoe-bot/out/ccoe-bot /home/ccoebot/app/ccoe-bot COPY bin/git-sync /usr/bin COPY bin/terragrunt /usr/bin COPY bin/terrascan /usr/bin COPY bin/supercronic /usr/bin COPY --chown=ccoebot:ccoebot init.sh /home/ccoebot/app USER ccoebot RUN git clone https://gitlab.wistron.com/ccoe/terrascan_policy.git /home/ccoebot/terrascan_policy WORKDIR /home/ccoebot/terrascan_policy RUN terrascan init -c terrascan-config.toml WORKDIR /home/ccoebot/.terrascan RUN git config --bool branch.master.sync true \u0026amp;\u0026amp; git branch -D HEAD # set scheduler for git-sync RUN echo \u0026#34;*/10 * * * * cd /home/ccoebot/.terrascan \u0026amp;\u0026amp; date \u0026gt;\u0026gt; /home/ccoebot/app/sync.log \u0026amp;\u0026amp; /usr/bin/git-sync \u0026gt;\u0026gt; /home/ccoebot/app/sync.log\u0026#34; \u0026gt;\u0026gt; /home/ccoebot/app/mycron # This container exposes port 8080 to the outside world EXPOSE 8080 # Run the binary program produced by `go install` #CMD [\u0026#34;/app/ccoe-bot\u0026#34;] # repack the ccoe-bot and crond to init.sh CMD [\u0026#34;/home/ccoebot/app/init.sh\u0026#34;] init.sh 這裡又是另一個要注意的地方，因為 supercronic 也是一個要跑的使用者程序，故這個案例同時會有兩個程序需要在 CMD 裡面一同跑起，使用下面的寫法完成在同個 container 中跑兩個程序。\n1 2 3 4 5 6 7 #!/bin/bash /usr/bin/supercronic /home/ccoebot/app/mycron \u0026amp; P1=$! /home/ccoebot/app/ccoe-bot \u0026amp; P2=$! wait $P1 $P2 或是\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # Start the first process /usr/bin/supercronic /home/ccoebot/app/mycron \u0026amp; # Start the second process /home/ccoebot/app/ccoe-bot \u0026amp; # Wait for any process to exit wait -n # Exit with status of process that exited first exit $? kubernetes deployment yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion: apps/v1 kind: Deployment metadata: name: dev-ccoebot namespace: atlantis spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: dev-ccoebot strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: spec: containers: - image: harbor.wistron.com/k8sprdwhqccoe/ccoe-bot:nonroot imagePullPolicy: IfNotPresent name: dev-ccoebot resources: requests: memory: \u0026#39;256Mi\u0026#39; cpu: \u0026#39;512m\u0026#39; limits: memory: \u0026#39;1024Mi\u0026#39; cpu: \u0026#39;1024m\u0026#39; securityContext: runAsUser: 1000 runAsGroup: 1000 allowPrivilegeEscalation: false privileged: false readOnlyRootFilesystem: false runAsNonRoot: true stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsPolicy: ClusterFirst imagePullSecrets: - name: hbsrt restartPolicy: Always schedulerName: default-scheduler securityContext: runAsUser: 1000 runAsGroup: 1000 fsGroup: 1000 runAsNonRoot: true terminationGracePeriodSeconds: 30 Result Reference 拯救我用 supercronic 的文章, 感謝 alesk 大\nhttps://gist.github.com/alesk/33b716f04cdce0751473b8232405dc32 Run Mutiple Process in Container: https://docs.docker.com/config/containers/multi-service_container/ https://stackoverflow.com/a/56663151 ","date":"2022-10-23T19:04:00Z","permalink":"http://localhost:1313/p/run-crond-as-non-root-in-alpine-container-by-pod-or-deployment/","title":"Run Crond as Non Root in Alpine Container by Pod/Deployment"},{"content":"簡介 官方定義:\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n― offical site Kubernetes 又稱為 k8s，最初由 google 用 golang 開發而後釋出的專案。用於操作自動化容器，包括部署，調度和節點集群間擴展。\n架構 上圖為一個簡易的 Kubernetes Cluster，通常一個 Cluster 中會有多個 Master 作為備援，但為了簡化我們只顯示一個。\n運作方式 使用者透過 User Command（kubectl）建立 Pod 時經過使用者身份的認證後，再將指令傳遞到 Master Node 中的 API Server，API Server 會把指令備份到 etcd 。接下來 controller-manager 會從 API Server 收到需要創建一個新的 Pod 的訊息，並檢查如果資源許可，就會建立一個新的 Pod。最後 Scheduler 在定期訪問 API Server 時，會詢問 controller-manager 是否有建置新的 Pod，如果發現新建立的 Pod 時，Scheduler 就會負責把 Pod 配送到最適合的一個 Node 上面。\n節點與組件 Master Node 為 Kubernetes 叢集的控制台，負責管理集群、協調所有活動，包含的元件如下：\nAPI Server API Server 管理 Kubernetes 的所有 api interface，用來和集群中的各節點通訊並進行操作。\nScheduler Scheduler 是 Pods 調度員，監視新建立但還沒有被指定要跑在哪個 Worker Node 上的 Pod，並根據每個 Node 上面資源去協調出一個最適合放置的對象給該 Pod。\nController Manager 負責管理並運行 Kubernetes controller 的組件，controller 是許多負責監視 Cluster 狀態的 Process，又可分為下列不同的種類\nNode controller - 負責通知與回應節點的狀態 Replication controller - 負責每個複寫系統內維持設定的 Pod 數量 End-Point controller - 負責端點的服務發布 Service Account \u0026amp; Token controller - 負責創建服務帳戶與新生成的 Namespace 的 API 存取 Token etcd 用來存放 Kubernetes Cluster 的資料作為備份，當 Master 因為某些原因而故障時，我們可以透過 etcd 幫我們還原 Kubernetes 的狀態。\nWorker Node 為 Kubernetes 的 runtime 執行環境，包含的元件如下：\nPod Kubernetes pod 是 Kubernetes 管理的最小單元，裡面包含一個或多個 container，可視為一個應用程式的邏輯主機。 同一個 Pod 中的 Containers 共享相同資源及網路，彼此透過 local port number 溝通。pod 運行在私有隔離的網絡上，默認情況下在同一集群的其他 pod 和 service 中可見，但是外部不可見，需要藉助 service 暴露給外部。\nKubelet Kubelet 接受 API server 的命令，用來啟動 pod 並監測狀態，確保所有 container 都在運行。它每隔幾秒鐘向 master node 提供一次 heartbeat。如果 replication controller 未收到該消息，則將該節點標記為不正常。\nKube Proxy 進行網路連線的 forwarding，負責將 request 轉發到正確的 container。\nResource https://blog.sensu.io/how-kubernetes-works https://medium.com/@C.W.Hu/kubernetes-basic-concept-tutorial-e033e3504ec0 https://ithelp.ithome.com.tw/articles/10202135 ","date":"2022-09-07T21:13:00Z","permalink":"http://localhost:1313/p/kubernets-basic/","title":"Kubernetes Introduction"},{"content":"問題 在使用 kustomize 配置 Kubernetes 資源時，Kustomization 定義的 ConfigMap 無法正確的渲染到 CronJob 資源中。 原 yaml 檔如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # 建立 templatevar 文件 cat \u0026lt;\u0026lt;EOF \u0026gt;templatevar FOO=Bar EOF # 建立 cronjob 文件 cat \u0026lt;\u0026lt;EOF \u0026gt;cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: hello namespace: infrase spec: schedule: \u0026#34;25,45,05 * * * *\u0026#34; concurrencyPolicy: Replace jobTemplate: spec: template: spec: containers: - name: hello image: busybox:1.28 resources: limits: cpu: \u0026#34;1\u0026#34; memory: 500Mi imagePullPolicy: IfNotPresent securityContext: runAsNonRoot: true runAsUser: 1000 allowPrivilegeEscalation: false command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster \u0026amp;\u0026amp; cat /config/templatevar volumeMounts: - mountPath: /config/ name: templatevar volumes: - name: templatevar configMap: name: templatevar restartPolicy: OnFailure EOF # 建立 kustomization 文件 cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - cronjob.yaml configMapGenerator: - name: templatevar files: - templatevar EOF 使用 kustomize 渲染後，可以看到 CronJob 中指定的 ConfigMap 沒有正確吃到 configMapGenerator 所產生的檔案。\n1 kubectl kustomize ./ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 apiVersion: v1 data: templatevar: \u0026#34;FOO=Bar\u0026#34; kind: ConfigMap metadata: name: templatevar-tk9cdghbt6 namespace: infrase --- apiVersion: batch/v1 kind: CronJob metadata: name: hello namespace: infrase spec: jobTemplate: spec: template: spec: containers: - command: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster \u0026amp;\u0026amp; cat /config/templatevar image: busybox:1.28 imagePullPolicy: IfNotPresent name: hello securityContext: allowPrivilegeEscalation: false runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /config/ name: templatevar imagePullSecrets: - name: hbsrt restartPolicy: OnFailure volumes: - configMap: name: templatevar name: templatevar schedule: 57 * * * * 解決方式 需要在 kustomiztion 檔案中指定 namespace\n1 2 3 4 5 6 7 8 9 10 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: infrase resources: - cronjob.yaml configMapGenerator: - name: templatevar files: - templatevar Reference https://github.com/kubernetes-sigs/kustomize/issues/1301 ","date":"2022-09-06T21:02:00Z","permalink":"http://localhost:1313/p/kustomization-render-failed-in-cronjob/","title":"Kustomize Can’t Render the ConfigMap Hashing Name to CronJob Resource"},{"content":"前言 原先集群使用的 helm chart 為 stable/nginx-ingress，而此 helm chart 已經被棄用，若 nginx 維持在舊版的話，之後新的漏洞修補都無法被含括。\n此篇記錄如何將集群上面跑的 nginx-ingress-controller 換成新的版本的 chart ingress-nginx/ingress-nginx。\nCurrent stable/nginx-ingress 原先 nginx-ingress 使用的版本\n1 2 3 4 5 6 7 8 $ kubectl exec -it -n nginx-ingress nginx-ingress-controller-585bb7f5b4-2nlzz -- /nginx-ingress-controller ------------------------------------------------------------------------------- NGINX Ingress controller Release: v0.34.1 Build: v20200715-ingress-nginx-2.11.0-8-gda5fa45e2 Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.19.1 ------------------------------------------------------------------------------- 查詢該 helm chart 有沒有更新的版本可直接更新，但結果如下，目前集群安裝的已經是該 chart 的最新版本了，且已標示 deprecated 不會再維護。\n1 2 3 4 5 6 7 8 9 $ helm search repo stable/nginx-ingress --versions NAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 1.41.3 v0.34.1 DEPRECATED! An nginx Ingress controller that us... stable/nginx-ingress 1.41.2 v0.34.1 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.41.1 v0.34.1 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.41.0 v0.34.0 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.40.3 0.32.0 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.40.2 0.32.0 An nginx Ingress controller that uses ConfigMap... stable/nginx-ingress 1.40.1 0.32.0 An nginx Ingress Install ingress-nginx/ingress-nginx 安裝operator\n1 2 3 4 $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update $ kubectl create ns ingress-nginx $ helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx 完成後應該會看到以下輸出\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 [root@master1 ~]# helm install ingress-nginx ingress-nginx/ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx NAME: ingress-nginx LAST DEPLOYED: Wed Aug 18 13:41:42 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running \u0026#39;kubectl --namespace default get services -o wide -w ingress-nginx-controller\u0026#39; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls 驗證安裝 1 2 3 4 5 6 7 8 9 $ kubectl exec -it -n ingress-nginx ingress-nginx-controller-b65df6fbb-jx4tf -- /nginx-ingress-controller --version ------------------------------------------------------------------------------- NGINX Ingress controller Release: v0.48.1 Build: 30809c066cd027079cbb32dccc8a101d6fbffdcb Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.20.1 ------------------------------------------------------------------------------- Create ingress resource 準備 ingress resource 的 yaml 檔，請注意雖然上方的安裝成功的訊息有示範 ingress 的 yaml，但因為 networking.k8s.io/v1beta1 已在 Kubernetes 1.19+ 被棄用，如果維持使用，則會遇到 Warning: [networking.k8s.io/v1beta1](http://networking.k8s.io/v1beta1) Ingress is deprecated in v1.19+, unavailable in v1.22+; use [networking.k8s.io/v1](http://networking.k8s.io/v1) Ingress的錯誤，所以參考ingress-nginx(https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/) 的官網，改成以下格式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myservicea annotations: # use the shared ingress-nginx kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: myservicea.foo.org http: paths: - path: / pathType: Prefix backend: service: name: myservicea port: number: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myserviceb annotations: # use the shared ingress-nginx kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: myserviceb.foo.org http: paths: - path: / pathType: Prefix backend: service: name: myserviceb port: number: 80 刪除舊的 ingress controller 確認所有流量都已經導到新的 controller 後，就可以把舊的 stable/nginx-ingress 的 controller 刪掉了。\n1 $ helm uninstall nginx-ingress Reference https://jfrog.com/blog/migrate-nginx-from-stable-helm-charts-with-chartcenter/ 補充：如果要實現 zero-downtime 的部屬，可以參考這篇文章 https://medium.com/codecademy-engineering/kubernetes-nginx-and-zero-downtime-in-production-2c910c6a5ed8 ","date":"2022-05-22T21:17:00Z","permalink":"http://localhost:1313/p/helm-migrate-stable-nginx-ingress-to-ingress-nginx/","title":"Helm Migrate stable/nginx-ingress to ingress-nginx"},{"content":"cordon、drain 和 delete 三個命令都會使 kubernetes node 停止被調度，本篇記錄如何優雅的刪除節點。\nDrain the Node 使用 kubectl drain 將 Node 狀態變更為維護模式，該 Node 上面的 Pod 就會轉移到其他 Node 上。\n1 kubectl drain worker3 --ignore-daemonsets --delete-local-data kubectl drain 操作會將指定節點上的 Pod 刪除，並在可調度節點上面起一個對應的 Pod。當舊 Pod沒有被正常刪除的情況下，新 Pod 不會起來。例如舊 Pod 一直處於Terminating 狀態。所以可以強制刪除該 Pod。\n1 2 3 kubectl get pod all -A -o wide | grep worker3 kubectl delete pods -n my-kafka-project my-cluster-zookeeper-2 --force Delete the Node 1 kubectl delete nodes worker3 delete 是一種暴力刪除 node 的方式，會強制關閉容器進程以驅逐 pod。 基于 node 的自註冊功能，恢復調度則重啟 kubelet 服務即可。\n1 systemctl restart kubelet Drain v.s Cordon cordon 停止調度（不可調度，從 K8S 集群隔離） 只會將 node 標識為SchedulingDisabled 不可調度狀態。新創建的資源，不會被調度到該節點。而舊有的 pod 不會受到影響，仍正常對外提供服務。\n1 2 3 4 # 禁止調度 kubectl cordon \u0026lt;nodeName\u0026gt; # 恢復調度 kubectl uncordon \u0026lt;nodeName\u0026gt; drain 驅逐節點（先不可調度，然後排乾 Pod） 會驅逐 Node 上的 pod 資源到其他節點重新創建。接著，將節點調為 SchedulingDisabled 不可調度狀態。\n1 2 3 4 # 禁止調度 kubectl drain \u0026lt;nodeName\u0026gt; --force --ignore-daemonsets --delete-local-data # 恢復調度 kubectl uncordon \u0026lt;nodeName\u0026gt; --force 當一些 pod 不是經ReplicationController, ReplicaSet, Job, DaemonSet 或者 StatefulSet 管理的時候就需要用 \u0026ndash;force 來強制執行(例如 kube-proxy) --ignore-daemonsets 無視DaemonSet 管理下的 Pod。因為deamonset 會忽略 unschedulable 標籤，因此 deamonset 控制器控制的 pod 被刪除後可能馬上又在此節點上啟動起來，這樣就會成為死循環，因此這裡忽略daemonset。 --delete-local-data 如果有 mount local volumn 的 pod，會強制殺掉該 pod。 drain 驅逐流程：先在 Node 節點優雅關閉並刪除 pod，然後再在其他 Node 節點創建該 pod。所以為了確保 drain 驅逐 pod 過程中不中斷服務，必須保證要驅逐的 pod 副本數大於 1，並且採用了 anti-affinity 將這些 pod 調度到不同的 Node 節點上。 Reference https://www.cnblogs.com/kevingrace/p/14412254.html ","date":"2022-05-19T11:36:00Z","permalink":"http://localhost:1313/p/kubernetes-delete-worker-node/","title":"[Kubernetes] 停止調度 / 刪除節點"},{"content":"本篇文章記錄怎麼使用 cert-manager 為對外的 istio gateway 加上 https。\n憑證分類 自簽憑證：某些不需要被公開存取、但希望達到資料傳輸能加密的內部服務，可以使用自簽憑證，Client 去存取的時候自己帶上 CA 憑證去驗證即可，例如 HashiCorp Vault, AWS RDS TLS 連線\u0026hellip;等。\n第三方 CA 機構簽發憑證：如果是公開的網路服務，就必須透過正規的 CA 機構來簽發，如需要收費的 Digicert, SSL.com, Symantec\u0026hellip;等，或是免費的 Let’s Encrypt。\ncert-manager cert-manager 是基於 Kubernetes 所開發的憑證管理工具，它可以可以幫忙發出來自各家的 TLS 憑證，例如上面所提到的 ACME (Let’s Encrypt), HashiCorp Vault, Venafi 或是自己簽發的憑證，而且它還可以確保 TLS 憑證一直維持在有效期限內。\nAbove Reference\nInstall 1 2 3 4 5 6 $ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.4.0/cert-manager.yaml $ kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c6866597-zw7kh 1/1 Running 0 2m cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m Issuer Issuer 用來頒發憑證，分為兩種資源類型：\nissuer：只作用於特定 namespace ClusterIssuer：作用於整個 k8s 集群 cert-manager 有支援幾種的 issuer type：\nCA: 使用 x509 keypair 產生 certificate，存在 kubernetes secret Self Signed: 自簽 certificate ACME: 從 ACME (ex. Let\u0026rsquo;s Encrypt) server 取得 ceritificate Vault: 從 Vault PKI backend 頒發 certificate Venafi: Venafi Cloud Above Refenrence\nCreate Issuer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: # 創建的簽發機構的名稱，後面創建證書的時候會引用 name: letsencrypt-prod spec: acme: # 證書快過期的時候會有郵件提醒，不過 cert-manager 會利用 acme 協議自動給我們重新頒發證書來續期 email: ulahsieh@nexaiot.com privateKeySecretRef: # Name of a secret used to store the ACME account private key 指示此簽發機構的私鑰將要存儲到哪個 Secret 中 name: letsencrypt-prod # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory solvers: # 指示簽發機構使用 HTTP-01 的方式進行 acme 協議(還可以用 DNS 方式，acme 協議的目的是證明這台機器和域名都是屬於你的，然後才准許給你頒發證書) - http01: ingress: class: istio 備註：\nletsencrypt 可以看到網路上有兩個不同名字的設定，一個是 letsencrypt-staging 用於測試，一個是 letsencrypt-prod 用於 production. 1 2 $ kubectl apply -f clusterIssuer.yaml $ kubectl describe clusterissuers.cert-manager.io 做完的時候發生了 server misbehaving 錯誤\n1 2 3 4 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ErrInitIssuer 8m2s (x3 over 8m7s) cert-manager Error initializing issuer: Get \u0026#34;https://acme-v02.api.letsencrypt.org/directory\u0026#34;: dial tcp: lookup acme-v02. api.letsencrypt.org on 10.96.0.10:53: server misbehaving 查了一下發現是 dns 解析的問題，便去自建的 dns server 改 /etc/named.conf 檔，發現在 option 中少加了 forwarders 欄位。\nforwarders 是指當本 DNS 解析不了的域名，要轉給誰來解析的意思，通常轉給再上一層，也就是外網本身的 DNS，簡單來說可直接使用 8.8.8.8，並添加 allow-query any;，讓集群內的網段都能來使用。\n改好之後回到 k8s 集群再次查看 clusterissuer 是否可以建成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 [root@k8sm1 cert-manager]# kubectl describe clusterissuers.cert-manager.io Name: letsencrypt-prod Namespace: Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: cert-manager.io/v1 Kind: ClusterIssuer Metadata: Creation Timestamp: 2021-06-21T12:12:47Z Generation: 1 Resource Version: 104200852 Self Link: /apis/cert-manager.io/v1/clusterissuers/letsencrypt-prod UID: f0e9ecc6-9a50-491c-af78-b88670885e18 Spec: Acme: Email: ulahsieh@nexaiot.com Preferred Chain: Private Key Secret Ref: Name: letsencrypt-prod Server: https://acme-v02.api.letsencrypt.org/directory Solvers: http01: Ingress: Class: istio Status: Acme: Last Registered Email: ulahsieh@nexaiot.com Uri: https://acme-v02.api.letsencrypt.org/acme/acct/127768449 Conditions: Last Transition Time: 2021-06-21T12:13:35Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u0026lt;none\u0026gt; Create Certificate 透過 Issuer 申請 Certificate 憑證\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # cat istio-cert.yaml apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: istio-cert namespace: istio-system spec: commonName: convergence.nexmasa.com dnsNames: - convergence.nexmasa.com secretName: istio-cert issuerRef: name: letsencrypt-prod kind: ClusterIssuer 說明：\nspec.secretName 指示證書最終存到哪個 Secret 中 spec.issuerRef.kind 值為 ClusterIssuer 說明簽發機構不在本namespace 下，而是在全局 spec.issuerRef.name 我們創建的簽發機構的 Issuer 名稱 spec.dnsNames 指示該證書的可以用於哪些域名 踩坑囉 部屬 certificate 後一直卡在跟 letsencrypt issueing 這塊，錯誤訊息是:\n1 2 3 4 5 6 $ kubectl describe certificate -n istio-system istio-cert $ kubectl get event -n istio-system 30s Normal Issuing certificate/istio-cert Issuing certificate as Secret does not exist 29s Normal Generated certificate/istio-cert Stored new private key in temporary Secret resource \u0026#34;istio-cert-hrrgb\u0026#34; 29s Normal Requested certificate/istio-cert Created new CertificateRequest resource \u0026#34;istio-cert-89tj8\u0026#34; 3s Warning Failed certificate/istio-cert The certificate request has failed to complete and will be retried: Failed to wait for order resource \u0026#34;istio- cert-89tj8-2838533447\u0026#34; to become ready: order is in \u0026#34;invalid\u0026#34; state: 試了很久，發現官網有寫 debug 過程，才發現要去看 challenge 的 log\nhttps://cert-manager.io/docs/faq/acme/\n1 2 3 4 5 6 7 8 $ kubectl describe challenges.acme.cert-manager.io -n istio-system istio-cert-22gl9-2838533447-3373762545 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Started 32m cert-manager Challenge scheduled for processing Normal Presented 32m cert-manager Presented challenge using HTTP-01 challenge mechanism Warning Failed 32m cert-manager Accepting challenge authorization failed: acme: authorization error for convergence.nexmasa.com: 400 urn:ietf:params:acme:error: dns: DNS problem: NXDOMAIN looking up A for convergence.nexmasa.com - check that a DNS record exists for this domain 然後找了一下解答發現，letsencrypt 只適合用在網際網路存取到的 DNS 啊 \u0026hellip; = =\nhttps://github.com/jetstack/cert-manager/issues/3543\nThis error comes from Let\u0026rsquo;s Encrypt which cannot reach your domain (it tries to as .int is a public known TLD). In order for Let\u0026rsquo;s Encrypt to work they need to have public access to verify the ownership of your domain. For internal only domains you might want to look into using an internal CA.\nhttps://serverfault.com/questions/1048678/check-that-a-dns-record-exists-for-this-domain\ndomain names that are in the global DNS tree\n不過這邊還是可以記錄幾篇可以參考使用 letsencrypt 的文章\nhttps://www.qikqiak.com/k8strain/istio/cert-manager/ https://medium.com/intelligentmachines/istio-https-traffic-secure-your-service-mesh-using-ssl-certificate-ac20ec2b6cd6 改建自簽憑證 建立 Issuer 1 2 3 4 5 6 7 8 9 kubectl apply -f \u0026lt;(echo \u0026#34; apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: istio-system spec: selfSigned: {} \u0026#34;) 建立 Certificate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f \u0026lt;(echo \u0026#39; apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-crt namespace: istio-system spec: secretName: tls-secret duration: 175200h renewBefore: 12h issuerRef: kind: Issuer name: selfsigned commonName: \u0026#34;convergence.nexmasa.com\u0026#34; isCA: true dnsNames: - \u0026#34;convergence.nexmasa.com\u0026#34; \u0026#39;) 查看憑證及金鑰的有效性 1 kubectl get secrets/tls-secret -n istio-system -o \u0026#34;jsonpath={.data[\u0026#39;tls\\.crt\u0026#39;]}\u0026#34; | base64 -d | openssl x509 -text -noout 1 kubectl get secrets/tls-secret -n istio-system -o \u0026#34;jsonpath={.data[\u0026#39;tls\\.key\u0026#39;]}\u0026#34; | base64 -D | openssl rsa -check 為服務加上憑證 修改 istio gateway，加上 https 的 protocol 並指定上面建立的 Secret Name。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: converg-api-gw namespace: converg-api spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: Port-80-M55Sa number: 80 protocol: HTTP - hosts: - \u0026#39;*\u0026#39; port: name: https number: 443 protocol: HTTPS tls: credentialName: tls-secret mode: SIMPLE ","date":"2022-05-19T09:11:00Z","permalink":"http://localhost:1313/p/add-https-to-istio-gw/","title":"為對外的 istio gateway 加上 https"},{"content":"環境說明 Kubernetes 1.20.10 Calico 3.23 錯誤描述 部署 calico 網絡後狀態雖為 Running，但 container 都無法成功運作，calico-controller 以及放在每個節點的 calico-node 皆無法初始化，如下圖\ncalico-kube-controllers 日誌 1 2 3 4 2022-05-17 01:11:09.503 [FATAL][1] main.go 120: Failed to initialize Calico datastore error=Get \u0026#34;https://10.254.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\u0026#34;: context deadline exceeded 2022-05-17 01:11:09.962 [INFO][1] main.go 94: Loaded configuration from environment config=\u0026amp;config.Config{LogLevel:\u0026#34;info\u0026#34;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:\u0026#34;\u0026#34;, DatastoreType:\u0026#34;kubernetes\u0026#34;} W0822 01:11:09.963646 1 client_config.go:615] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. 2022-05-17 01:11:09.964 [INFO][1] main.go 115: Ensuring Calico datastore is initialized calico-node 日誌 1 2 3 4 5 2022-05-17 01:11:14.102 [WARNING][69] felix/health.go 211: Reporter is not ready. name=\u0026#34;async_calc_graph\u0026#34; 2022-05-17 01:11:14.102 [WARNING][69] felix/health.go 211: Reporter is not ready. name=\u0026#34;int_dataplane\u0026#34; 2022-05-17 01:11:14.103 [WARNING][69] felix/health.go 173: Health: not ready 2022-05-17 01:11:14.540 [INFO][69] felix/watchercache.go 180: Full resync is required ListRoot=\u0026#34;/calico/resources/v3/projectcalico.org/kubernetesendpointslices\u0026#34; 2022-05-17 01:11:14.544 [INFO][69] felix/watchercache.go 193: Failed to perform list of current data during resync ListRoot=\u0026#34;/calico/resources/v3/projectcalico.org/kubernetesendpointslices\u0026#34; error=resource does not exist: KubernetesEndpointSlice with error: the server could not find the requested resource 原因及解法 原因是因為安裝的 Calico 版本不相容於 Kubernetes，依據 Calico 官網 的描述，3.23 版最低支援的 k8s 版本為 1.21，故發生該錯誤。 解決方法是將 Calico 重新部署為 3.21 版，以相容於 k8s 1.20 的環境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 curl -s https://docs.projectcalico.org/manifests/calico.yaml | kubectl delete -f - configmap \u0026#34;calico-config\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;bgpconfigurations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;bgppeers.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;blockaffinities.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;caliconodestatuses.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;clusterinformations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;felixconfigurations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;globalnetworkpolicies.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;globalnetworksets.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;hostendpoints.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamblocks.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamconfigs.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipamhandles.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ippools.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;ipreservations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;kubecontrollersconfigurations.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;networkpolicies.crd.projectcalico.org\u0026#34; deleted customresourcedefinition.apiextensions.k8s.io \u0026#34;networksets.crd.projectcalico.org\u0026#34; deleted clusterrole.rbac.authorization.k8s.io \u0026#34;calico-kube-controllers\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;calico-kube-controllers\u0026#34; deleted clusterrole.rbac.authorization.k8s.io \u0026#34;calico-node\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;calico-node\u0026#34; deleted daemonset.apps \u0026#34;calico-node\u0026#34; deleted serviceaccount \u0026#34;calico-node\u0026#34; deleted deployment.apps \u0026#34;calico-kube-controllers\u0026#34; deleted serviceaccount \u0026#34;calico-kube-controllers\u0026#34; deleted error: unable to recognize \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;PodDisruptionBudget\u0026#34; in version \u0026#34;policy/v1\u0026#34; 重新部署\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 curl -s https://docs.projectcalico.org/v3.21/manifests/calico.yaml | kubectl apply -f - configmap/calico-config created Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created 部署依照環境配置大概需要等幾分鐘不等，過程中可能會造成其他 Pod 被重啟，觀察下來是正常的情況\n全部完成後就可以看到 pod 皆正常運作了\nReference https://projectcalico.docs.tigera.io/archive/v3.21/getting-started/kubernetes/requirements https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/633 ","date":"2022-05-19T08:53:00Z","permalink":"http://localhost:1313/p/calico-running-but-unready/","title":"Calico Running but Unready (Ready 0/1)"},{"content":"在 kubernetes 環境上拉取私有鏡像倉庫 harbor 的 image 時，一直卡在 ImagePullBackOff 的狀態，decribe pod 發現是權限問題導致拉取失敗。\n狀況說明 錯誤訊息如下\n1 Failed to pull image \u0026#34;10.1.5.142:4433/test/findkpsn:9d2e44d2\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: unauthorized: unauthorized to access repository: test/findkpsn, action: pull: unauthorized to access repository: test/findkpsn, action: pull 然而實際上在本機上已經 docker login 成功過了，也可以直接使用 docker pull 拉取，但透過 k8s 拉取仍會失敗。\ndebug 思路 確認在同樣 repo 的 project 下的其他 image 是否也發生同樣的情況 是，同樣 repo 的 project 的其他 image 也相同。 確認在不同的 repo 是否也發生同樣的情況 否，其他 repo 能正常夠過 kubectl 拉取，應能推斷部屬環境上沒問題。 原因 結果是因為沒有把 project 公開 =__=\n意外發現 仍然還是可以讓 project 維持在私有的狀況下，透過 kubectl 拉取。只要在定義資源時，加上 imagePullSecrets 的屬性，值指定為欲創建資源的 namespace 下的 kubernetes.io/dockerconfigjson 的 secret，即可拉取成功。\n創建 docker-registry secret 1 2 3 4 kubectl create secret docker-registry \u0026lt;secretName\u0026gt; \\ --docker-server=DOCKER_REGISTRY_SERVER \\ --docker-username=DOCKER_USER \\ --docker-password=DOCKER_PASSWORD -n \u0026lt;NAMESPACE\u0026gt; deployment 部屬檔 在 spec.template.spec 下新增 imagePullSecrets\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: Deployment metadata: name: test namespace: test labels: app: test spec: replicas: 1 selector: matchLabels: app: test template: metadata: labels: app: test spec: containers: - name: test image: 10.1.5.142:4433/test/test/findkpsn:9d2e44d2 imagePullSecrets: - name: harbor 重新佈署 加上 imagePullSecrets 後，就可以成功拉取私有專案的鏡像了!\nReference https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod ","date":"2022-04-14T19:33:00Z","permalink":"http://localhost:1313/p/image-pull-backoff-after-harbor-login/","title":"已登入 harbor 但 kubelet 仍會 ImagePullBackOff"},{"content":"目前手上有一個監聽 Oracle CDC 的程式跑在以 Debian 為基底的 kubernetes pod 中，會定期因為 Oracle 的錯誤訊息 ORA-12518: TNS 監聽程式無法分發客戶機連線的問題而斷線。此時雖然程式有 error log，但 Pod 的狀態仍然為 Running，只要重啟 Pod 即可重新正常運作。\nORA-12518 首先順便解釋此錯誤的原因 The process of handing off a client connection to another process failed. 參考網路上其他分享：\nstackoverflow ittutorial cnblogs 從根本可以解決的方式如下：\nEdit /etc/systemd/system.conf file and Set DefaultTasksMax to ‘infinity’. dedicated server: 修改 oracle processes \u0026amp; sessions parameters shared server: 修改 oracle dispatcher parameters 然而 因為 IT server 並不在我控管的範圍，所以只能自己手動重啟 Pod。原本是想說寫 cronJob 定期重啟 pod，但在找資料的過程中，發現在 stackoverflow crobjob 問題 的解法中有人提出了直接使用 livenessprobe 解決。\n從設定 livenessProbe 解決 Kubelet 使用 liveness probe（存活探針）來確定何時重啟容器。當應用程序處於運行狀態但無法做進一步操作，liveness 探針將捕獲到 deadlock，重啟處於該狀態下的容器，使應用程序在存在 bug 的情況下依然能夠繼續運行下去。\nexec.Command：要在容器內執行的檢測命令，如果命令執行成功，將返回 0，kubelet 就會認為該容器是活著的並且很健康。如果返回非 0 值，kubelet 就會殺掉這個容器並重啟它。 periodSeconds：liveness probe 多久檢查一次 initialDelaySeconds：首次啟動 pod 後，要延遲多久後執行 liveness probe probe command 要寫啥? 接下來又另一個問題來了，我的 probe 中的檢測命令要寫啥? 因為在手動重啟時，只能從 kubectl logs 為依據，查看有無錯誤訊息。然而現在 command 要執行在容器中，但容器裡面沒辦法直接使用 kubectl 取得應用的 stdout 的訊息。又去堆疊溢位(XD 找到了兩種解決方法。\n在容器裡 curl Kubernete API server 設定 pod 連 kubernetes api server 請參考另外一篇文章記錄。 command 應該就會長成以下，如果 curl 回到的 output 會 grep 到 error 訊息，則重啟。\n1 2 3 4 5 6 7 8 livenessProbe: exec: command: - bash - -c - \u0026#34;curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; -X GET https://kubernetes.default.svc/api/v1/namespaces/converg-it/pods/converg-it-adapter-oracle-8575f54dc6-7lnv6/log?sinceSeconds=100 | grep \u0026#39;error\u0026#39;\u0026#34; initialDelaySeconds: 120 periodSeconds: 60 在容器裡查看監聽的 port 在目前跑的容器中，使用 ss 查看目前系統的 socket 狀態，可以發現到其實在正常連結的情況下能偵測到連線(establish state) oracle server 的監聽。\n那麼當發生連線異常時(ORA-12518)，就可以當作是重啟的條件。\n1 2 3 4 5 6 7 8 livenessProbe: exec: command: - bash - -c - \u0026#34;ss -an | grep -q \u0026#39;EST.*:1521 *$\u0026#39;\u0026#34; initialDelaySeconds: 120 periodSeconds: 60 Reference https://jimmysong.io/kubernetes-handbook/guide/configure-pod-service-account.html https://stackoverflow.com/questions/49000280/monitor-and-take-action-based-on-pod-log-event https://stackoverflow.com/questions/57711963/kubernetes-liveness-probe-can-a-pod-monitor-its-own-stdout ","date":"2022-03-31T22:52:15Z","permalink":"http://localhost:1313/p/setup-liveness-probe-to-restart-when-pod-has-error-log/","title":"設定 liveness probe 監聽應用以重啟 pod"},{"content":"連接 k8s 的 api-server 有三種方式：\nKubernetes Node 通過 kubectl proxy 中轉連接 通過授權驗證直接連接，例如 kubectl 和各種 client 就是這種情況 容器內部通過 ServiceAccount 連接 本文以第三種情況作範例。\nKubernetes API Server 在 Kubernetes 集群被創建時，預設會在 default namespace 中創建 kubernetes 的服務，用於訪問 Kubernetes apiserver。因此，Pod 之間可以直接使用 kubernetes.default.svc 主機名來查詢 API server。\nService Account ServiceAccount 是給執行在 Pod 的程式使用的身份認證，給 Pod 容器的程式訪問 API Server 時使用；ServiceAccount 僅侷限它所在的 namespace，每個 namespace 建立時都會自動建立一個 default service account；建立 Pod 時，如果沒有指定 Service Account，Pod 則會使用 default Service Account。\nService Account Secret SA 對應的 Secret 會自動掛載到 Pod 的 /var/run/secrets/kubernetes.io/serviceaccount/ 目錄中(包含 token、ca.crt、namespace)。\n創建 Role \u0026amp; Role Binding 如果直接使用預設的 sa 訪問 api server 會遇到權限不足的問題\n此時需要建立角色開放存取 api 指定路徑的權限並綁定角色到 SA 上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # role\u0026amp;binding.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default-role namespace: converg-it rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - pods - pods/log verbs: - get - list --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-roldbinding namespace: converg-it subjects: - kind: ServiceAccount name: default roleRef: kind: Role name: default-role apiGroup: rbac.authorization.k8s.io 1 2 3 kubectl -n converg-it apply -f role\u0026amp;binding.yaml role.rbac.authorization.k8s.io/default-role created rolebinding.rbac.authorization.k8s.io/default-roldbinding created curl API 進入容器環境\n1 kubectl exec -it -n converg-it converg-it-adapter-oracle-8575f54dc6-7lnv6 -- bash get target API\n1 curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \u0026#34;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; -X GET https://kubernetes.default.svc/api/v1/namespaces/converg-it/pods/converg-it-adapter-oracle-8575f54dc6-7lnv6/log?sinceSeconds=300 Reference https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/ ","date":"2022-03-31T21:57:12Z","permalink":"http://localhost:1313/p/curl-kubernetes-api-server-within-pod/","title":"在容器裡 curl Kubernetes API server"},{"content":"手上有一個寫好的 API 要對外釋出，在設置完 istio 資源之後，curl istio ingress gateway/targetAPI 卻一直回傳 404 Not Found，照理來說這個 API 如果找不到資料回傳的 404 訊息應該是 {\u0026quot;error\u0026quot;:\u0026quot;Record Not Found, the serial number doesn't exist\u0026quot;}，用這篇文章記錄問題跟解決方式。\n問題排查 在相同的 istio ingress gateway 上的 API 皆正常運作，排除 istio 本身可能會有問題 用同樣的配置檔，部屬在另外一個 K8s 環境上的 istio，發現運作正常，排除配置檔有誤的問題 用其他 API 部屬，也一樣直接 404 Not Found，排除原先 API 本身可能有誤的問題 查看 istio ingress gateway 的 log 1 2 3 [2022-03-24T06:23:07.653Z] \u0026#34;GET /api/convergence/findRecord/TBCC32008806 HTTP/1.1\u0026#34; 200 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 9491 5 4 \u0026#34;10.1.5.32\u0026#34; \u0026#34;PostmanRuntime/7.29.0\u0026#34; \u0026#34;3655dd39-9c2c-9c14-ac5c-53fc7547a155\u0026#34; \u0026#34;10.1.5.41\u0026#34; \u0026#34;10.244.64.149:8080\u0026#34; outbound|8080||converg-api.converg-api.svc.cluster.local 10.244.128.48:42104 10.244.128.48:8080 10.1.5.32:39159 - http-Sbups --- [2022-03-24T06:26:32.759Z] \u0026#34;GET /api/convergence/grabreflow/TBCBB2039913 HTTP/1.1\u0026#34; 404 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 18 1 1 \u0026#34;10.1.5.32\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\u0026#34; \u0026#34;61998a8c-0c79-94a0-9c84-cb03a2f10390\u0026#34; \u0026#34;10.1.5.41\u0026#34; \u0026#34;10.244.128.19:8080\u0026#34; outbound|8080||reverseapi.converg-apipost.svc.cluster.local 10.244.128.48:36740 10.244.128.48:8080 10.1.5.32:15717 - http-XbhqV 上面 200 的是正常運作的 API，下面 404 是新設置的 API，所以其實 404 這個 istio gw \u0026amp; virtual service 其實是有運作的，得出來的結論就是，istio 找不到新設置的 API 去路由，極大可能是跟其他 URL 規則衝突。\n解決 終於發現前面最後一個設置的 API 沒有設 prefix，所以 istio 就直接監聽 /，導致後面怎麼設新的 API，都認不到!!!!!!!!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 kubectl get virtualservices.networking.istio.io -n converg-apipost converg-apipost-vs -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: creationTimestamp: \u0026#34;2021-08-04T10:04:00Z\u0026#34; generation: 9 labels: protocol: http name: converg-apipost-vs namespace: converg-apipost resourceVersion: \u0026#34;241532937\u0026#34; selfLink: /apis/networking.istio.io/v1beta1/namespaces/converg-apipost/virtualservices/converg-apipost-vs uid: 15049a96-f97f-4b00-bee9-001fafcdd2ff spec: gateways: - converg-apipost-gw hosts: - \u0026#39;*\u0026#39; http: - match: - uri: prefix: \u0026#34;\u0026#34; name: http-OuFqP route: - destination: host: reverseapi port: number: 8080 把 prefix 加上去後，原本後加的 API 就成功運作了 \u0026#x270c;\u0026#xfe0f;\n1 2 [root@k8sm1 ~]# curl 10.1.5.41/api/convergence/grabreflow/test {\u0026#34;error\u0026#34;:\u0026#34;Record Not Found, the serial number doesn\u0026#39;t exist\u0026#34;} ","date":"2022-03-24T21:52:00Z","permalink":"http://localhost:1313/p/istio-404-not-found/","title":"Istio 沒掛，但正確的設置 gateway 跟 virtual service 後，卻一直 404 not found"},{"content":"最近在刪除 namespace 的時候總是會卡在 Terminating 的狀態，一直不疑有他的直接使用網路上常看的解決方法將 spec.finalizers 清空。但因為每次刪、每次卡，就連完全無任何資源的命名空間也是卡！仔細看後才發現原來是有其他元件錯誤，進而造成影響。\n原因排查 先使用清空 finalizer 的方法強制刪除 namespace，\n1 2 kubectl proxy Starting to serve on 127.0.0.1:8001 另開一個 terminal\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026lt;\u0026lt;EOF | curl -X PUT \\ localhost:8001/api/v1/namespaces/test/finalize \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data-binary @- { \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;finalizers\u0026#34;: null } } EOF 會回傳下面結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 { \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;353766ea-be97-4ccf-9275-0b39bd651afe\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;1359585\u0026#34;, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2022-03-21T04:18:38Z\u0026#34;, \u0026#34;deletionTimestamp\u0026#34;: \u0026#34;2022-03-21T04:18:51Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;kubernetes.io/metadata.name\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;manager\u0026#34;: \u0026#34;kubectl-create\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-03-21T04:18:38Z\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:metadata\u0026#34;: { \u0026#34;f:labels\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:kubernetes.io/metadata.name\u0026#34;: {} } } } }, { \u0026#34;manager\u0026#34;: \u0026#34;kube-controller-manager\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:conditions\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceContentRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionContentFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionDiscoveryFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionGroupVersionParsingFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceFinalizersRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} } } } }, \u0026#34;subresource\u0026#34;: \u0026#34;status\u0026#34; } ] }, \u0026#34;spec\u0026#34;: {}, \u0026#34;status\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;Terminating\u0026#34;, \u0026#34;conditions\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionDiscoveryFailure\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:56Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;DiscoveryFailed\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Discovery failed for some groups, 1 failing: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently un able to handle the request\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionGroupVersionParsingFailure\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ParsedGroupVersions\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All legacy kube types successfully parsed\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionContentFailure\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentDeleted\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content successfully deleted, may be waiting on finalization \u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceContentRemaining\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentRemoved\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content successfully removed\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;NamespaceFinalizersRemaining\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-03-21T04:18:57Z\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentHasNoFinalizers\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content-preserving finalizers finished\u0026#34; } ] } } 可以看到最後的 Terminating 的區塊有解釋原因。\n解決方法 按照原因解決即可成功刪除 namespace，本文遇到的問題是因為安裝 metric server 時失敗，可以看到 apiservice 其中有出現 false 狀態。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 kubectl get apiservice NAME SERVICE AVAILABLE AGE v1. Local True 9d v1.admissionregistration.k8s.io Local True 9d v1.apiextensions.k8s.io Local True 9d v1.apps Local True 9d v1.authentication.k8s.io Local True 9d v1.authorization.k8s.io Local True 9d v1.autoscaling Local True 9d v1.batch Local True 9d v1.certificates.k8s.io Local True 9d v1.coordination.k8s.io Local True 9d v1.crd.projectcalico.org Local True 9d v1.discovery.k8s.io Local True 9d v1.events.k8s.io Local True 9d v1.monitoring.coreos.com Local True 3d23h v1.networking.k8s.io Local True 9d v1.node.k8s.io Local True 9d v1.policy Local True 9d v1.rbac.authorization.k8s.io Local True 9d v1.scheduling.k8s.io Local True 9d v1.storage.k8s.io Local True 9d v1alpha1.kafka.strimzi.io Local True 3h3m v1alpha1.monitoring.coreos.com Local True 3d23h v1beta1.batch Local True 9d v1beta1.discovery.k8s.io Local True 9d v1beta1.events.k8s.io Local True 9d v1beta1.flowcontrol.apiserver.k8s.io Local True 9d v1beta1.kafka.strimzi.io Local True 3h3m v1beta1.metrics.k8s.io kube-system/metrics-server False (MissingEndpoints) 3d1h v1beta1.node.k8s.io Local True 9d v1beta1.policy Local True 9d v1beta1.storage.k8s.io Local True 9d v1beta2.core.strimzi.io Local True 3h3m v1beta2.flowcontrol.apiserver.k8s.io Local True 9d v1beta2.kafka.strimzi.io Local True 3h3m v2.autoscaling Local True 9d v2beta1.autoscaling Local True 9d v2beta2.autoscaling Local True 9d 先暫時移除臨時裝的 metric server\n1 helm uninstall metrics-server --namespace kube-system 就可以成功 delete 了\n1 2 3 4 5 6 [root@node ~]# kubectl create ns test namespace/test created [root@node ~]# [root@node ~]# kubectl delete ns test namespace \u0026#34;test\u0026#34; deleted [root@node ~# 警告\n另外關於 metric-server 的 debug，紀錄在另外一篇 文章。\n","date":"2022-03-22T14:49:00Z","permalink":"http://localhost:1313/p/kubernetes-namespace-delete-terminating/","title":"Kubernetes namespace 一直 delete 不成功的原因 (卡在 terminating status)"},{"content":"Metrics Server 通過 kubelet（cAdvisor）獲取監控數據，主要作用是為 kube-scheduler、HPA(Horizontal Pod Autoscaler)等 k8s 核心組件，以及 kubectl top 命令和 Dashboard 等 UI 組件提供數據來源，可以用來看 node 或 pod 的資源 (CPU \u0026amp; Memory) 消耗。須注意的是，Metric Server 是 in memory 的 monitor，只可以查詢當前的度量數據，並不保存歷史數據。\n安裝 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm upgrade --install metrics-server metrics-server/metrics-server --namespace kube-system Release \u0026#34;metrics-server\u0026#34; does not exist. Installing it now. NAME: metrics-server LAST DEPLOYED: Mon Mar 21 16:32:14 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: *********************************************************************** * Metrics Server * *********************************************************************** Chart version: 3.8.2 App version: 0.6.1 Image tag: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 *********************************************************************** 連線失敗問題 可以發現使用預設值安裝完後 deployment 一直無法 ready，查看 deployment 資訊\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # kubectl describe deployments.apps -n kube-system metrics-server Name: metrics-server Namespace: kube-system CreationTimestamp: Mon, 21 Mar 2022 16:32:17 +0800 Labels: app.kubernetes.io/instance=metrics-server app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=metrics-server app.kubernetes.io/version=0.6.1 helm.sh/chart=metrics-server-3.8.2 Annotations: deployment.kubernetes.io/revision: 1 meta.helm.sh/release-name: metrics-server meta.helm.sh/release-namespace: kube-system Selector: app.kubernetes.io/instance=metrics-server,app.kubernetes.io/name=metrics-server Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app.kubernetes.io/instance=metrics-server app.kubernetes.io/name=metrics-server Service Account: metrics-server Containers: metrics-server: Image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 Port: 4443/TCP Host Port: 0/TCP Args: --secure-port=4443 --cert-dir=/tmp --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s Liveness: http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3 Environment: \u0026lt;none\u0026gt; Mounts: /tmp from tmp (rw) Volumes: tmp: Type: EmptyDir (a temporary directory that shares a pod\u0026#39;s lifetime) Medium: SizeLimit: \u0026lt;unset\u0026gt; Priority Class Name: system-cluster-critical Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: metrics-server-7d76b744cd (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 3m50s deployment-controller Scaled up replica set metrics-server-7d76b744cd to 1 查看 Pod log\n1 kubectl logs -f -n kube-system metrics-server-7d76b744cd-fv9ns 連線失敗問題修正 加上 --kubelet-insecure-tls 啟動參數\n1 kubectl patch -n kube-system deployment metrics-server --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/template/spec/containers/0/args/-\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;--kubelet-insecure-tls\u0026#34;}]\u0026#39; 測試 查看節點資源消耗\n1 2 3 4 5 [root@node ~]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node 312m 7% 2769Mi 36% rockyw 234m 2% 3335Mi 21% rockyw2 199m 2% 2727Mi 17% Reference 在找資料的時候發現以下兩篇針對 metric server 的介紹，值得拜讀。\nKubernetes Monitoring 101 — Core pipeline \u0026amp; Services Pipeline Kubernetes 自動彈性伸縮 ","date":"2022-03-21T21:28:00Z","permalink":"http://localhost:1313/p/kubernets-install-metrics-server-by-helm/","title":"使用 helm 安裝 Metrics Server"},{"content":"kubernetes 1.22 版之後，就不再支持 Docker 作為 container runtime 以及管理容器及鏡像的工具了。可以使用 containerd 取代 docker 的 container runtime；以及 crictl 作為 CRI(Container Runtime Interface)，另外 podman 也可以用來管理容器和鏡像。本篇記錄基於 containerd \u0026amp; crictl 使用 kubeadm 部屬 Kubernetes 集群的過程。\n系統環境配置 (所有節點) 最小系統資源需求 每台機器 4 GiB 以上 RAM master control plane 節點至少需要有兩個以上的 vCPU 集群中所有機器之間的完整網絡連接 (can be private or public) Server Type Hostname Spec master node.ulatest.com 4 vCPU, 8G RAM worker rockyw.ulatest.com 8 vCPU, 16G RAM worker rockyw2.ulatest.com 8 vCPU, 16G RAM 配置 /etc/hosts 1 2 3 4 5 6 7 8 9 10 cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.1.5.130 nexdata nexdata.ulatest.com 10.1.5.146 node node.ulatest.com 10.1.5.147 rockyw rockyw.ulatest.com 10.1.5.148 rockyw2 rockyw2.ulatest.com 10.1.5.130 nfs nfs.ulatest.com 更新軟體套件 1 yum update -y 系統配置 停用防火牆 1 2 systemctl stop firewalld systemctl disable firewalld 關閉 SELINUX 1 2 3 4 5 6 7 8 9 10 11 12 13 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted 關閉 swap 1 2 3 4 # Turn off swap swapoff -a # comment out the line of swap\u0026#39;s mount point sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab 配置 kernel module 自動加載 1 2 3 4 5 6 7 8 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # 執行以下命令使配置生效 modprobe overlay modprobe br_netfilter 調整 kernel 參數 Kubernetes 的核心是依靠 netfilter kernel module 來設定低級別的集群 IP 負載均衡，需要兩個關鍵的 module：IP轉發和橋接。\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysctl.d/kubernetes.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 EOF # 執行以下命令使配置生效 sysctl -p /etc/sysctl.d/kubernetes.conf 以上操作含意如下：\n開啟 iptables 對 bridge 的數據進行處理 開啟數據包轉發功能（實現 vxlan） 禁止使用 swap 空間，只有當系統 OOM 時才允許使用它 開啟 ipvs module Kube-Proxy 是 Kubernetes 用來控制 Service 轉發過程的一個元件，預設會使用 iptables 作為 Kubernetes Service 的底層實現方式，而此模式最主要的問題是在服務多的時候產生太多的 iptables 規則，大規模情況下有明顯的性能問題。可以透過參數變化的方式要求 Kube-Proxy 使用 ipvs。開啟 ipvs 的前提條件是加載以下的 kernal module：\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack 上面腳本創建了的 /etc/sysconfig/modules/ipvs.modules 文件，保證在節點重啟後能自動加載所需模塊。使用 lsmod | grep -e ip_vs -e nf_conntrack 命令查看是否已經正確加載所需的內核模塊。\n接下來還需要確保各個節點上已經安裝了 ipset 軟件包，以及管理工具 ipvsadm 便於查看 ipvs 的代理規則。\n1 yum install -y ipset ipvsadm 如果以上前提條件如果不滿足，則即使 kube-proxy 的配置開啟了 ipvs 模式，也會退回到 iptables 模式。\n安裝 containerd \u0026amp; crictl (所有節點) 安裝 containerd 1 2 3 4 5 yum install -y yum-utils # 使用 docker.ce 作為 containerd 的 repo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y containerd.io systemctl enable containerd 生成 containerd 的配置文件:\n1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 根據 Kubernetes 文檔 Container runtimes 中的內容，對於使用 systemd 作為 init system 的 Linux 發行版，使用 systemd 作為容器的 cgroup driver 可以確保服務器節點在資源緊張的情況更加穩定，因此這裡配置各個節點上 containerd 的 cgroup driver 為 systemd。\n如果檔案中[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] 區塊下，沒有 SystemdCgroup 的選項，下： 1 sed -i \u0026#39;s|\\(\\s\\+\\)\\[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options\\]|\\1\\[plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options\\]\\n\\1 SystemdCgroup = true|g\u0026#39; /etc/containerd/config.toml 如果檔案中[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options] 區塊下，有 SystemdCgroup = false 的選項，下： 1 sed -i \u0026#39;s/ SystemdCgroup = false/ SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml 修改完畢後 config 內容會如下：\n1 2 3 4 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true 重啟 containerd 已應用 config\n1 systemctl restart containerd 安裝 crictl 1 2 3 yum install -y wget tar wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.23.0/crictl-v1.23.0-linux-amd64.tar.gz tar zxvf crictl-v1.23.0-linux-amd64.tar.gz -C /usr/local/bin 設定 container runtime interface 為 containerd\n1 2 3 4 5 6 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF 測試\n1 2 crictl images IMAGE TAG IMAGE ID SIZE 如果上方 CRI 沒有指定的話，會出現以下錯誤\n安裝 kubernetes 套件 (所有節點) 新增 kubernetes repo 1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF 在更新 yum 源後，使用 yum makecache 生成緩存，將套件包訊息提前在本地 cache 一份，用來提高搜索安裝套件的速度。\n1 yum makecache -y 通過 yum list 命令可以查看當前源的穩定版本，目前的穩定版本是 1.23.4-0。安裝 kubeadm 便會將 kubelet、kubectl 等依賴一併安裝。\n1 yum list kubeadm 1 yum install -y kubeadm-1.23.4-0 配置命令參數自動補全功能 1 2 3 4 yum install -y bash-completion echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc echo \u0026#39;source \u0026lt;(kubeadm completion bash)\u0026#39; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc 啟動kubelet 服務 1 2 systemctl enable kubelet systemctl restart kubelet 配置節點 kubeadm 部署 master 節點 準備配置文件 1 2 kubeadm config print init-defaults \u0026gt; kubeadm-init.yaml vim kubeadm.yaml 更改以下配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.1.5.146 # 改為 master node IP bindPort: 6443 nodeRegistration: criSocket: unix:///run/containerd/containerd.sock # 改為 containerd Unix socket 地址 imagePullPolicy: IfNotPresent name: rockym # 指定節點名稱 taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: 1.23.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # 指定 pod 子網 cidr，在設定 calico 時會用到 scheduler: {} 叢集初始化 1 kubeadm init --config=kubeadm-init.yaml 完成後按照提示將 /etc/kubernetes/admin.conf 複製到 $HOME/.kube/config\n1 2 3 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 並複製下面那一串加入指令以便其他 node 加入 (兩個小時過期)\n1 2 kubeadm join 10.1.5.146:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fc56685aedecf323023637a6e02cc1584cfc88bfeb0690dc0e2a1feca278f008 或是之後使用 kubeadm token create \u0026ndash;print-join-command 建立新的。\n以上就完成 master 節點的部屬，可以使用 kubectl command 確認。\n因目前網路尚未設置，所以 coredns 狀態為 Pending 是正常的。\n安裝 calico 1 curl -s https://docs.projectcalico.org/manifests/calico.yaml | kubectl apply -f - 安裝完畢後就可以發現節點已經部屬完成了。\n加入工作節點 在各工作節點上直接輸入上方的 join command\n1 2 kubeadm join 10.1.5.146:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fc56685aedecf323023637a6e02cc1584cfc88bfeb0690dc0e2a1feca278f008 就可大功告成了~\n註釋\n後記\n有些截圖裡面可以發現原本的 master node 的 hostname 本來叫 rockym 的，可是在加入 master node 節點的時候的名字忘記改 (冏) 導致 master node 強迫改名為 node \u0026hellip; 求助谷歌大神，發現改節點名稱最乾淨且簡單的方式就是刪掉節點後重新加入，但不巧地是我要改的節點就是唯一一個的 master node =__= 只好折衷將錯就錯改 hostname，不知道後續會不會發生問題，先記錄一下。\nReference kube-proxy https://kubernetes.io/zh/docs/concepts/services-networking/service/#proxy-mode-ipvs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd ","date":"2022-03-13T14:56:00Z","permalink":"http://localhost:1313/p/install-kubernetes-123-on-rocky-linux/","title":"在 Rocky Linux 8 安裝 Kubernetes 1.23 (containerd as cri)"},{"content":"Kubernetes Dashboard 是由官方維護的 Kubernetes 集群 WEB UI 管理工具，能查看 Kubernetes Cluster 上資源分佈與使用狀況，也可以創建或者修改 Kubernetes 資源，讓使用者透過 Web UI 介面取代指令的管理 Kubernetes。\n安裝 安裝非常簡單，只要透過下面 command 即可部屬。\n1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml 確認安裝結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@ula ~]# kubectl get all -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-799d786dbf-zhvlc 1/1 Running 0 24s pod/kubernetes-dashboard-fb8648fd9-wd4t5 1/1 Running 0 24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 10.111.121.48 \u0026lt;none\u0026gt; 8000/TCP 24s service/kubernetes-dashboard ClusterIP 10.107.248.188 \u0026lt;none\u0026gt; 443/TCP 25s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dashboard-metrics-scraper 1/1 1 1 24s deployment.apps/kubernetes-dashboard 1/1 1 1 25s NAME DESIRED CURRENT READY AGE replicaset.apps/dashboard-metrics-scraper-799d786dbf 1 1 1 24s replicaset.apps/kubernetes-dashboard-fb8648fd9 1 1 1 24s 訪問 將 kubernetes-dashboard 服務暴露 NodePort\n1 kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard 將原本 type: ClusterIP 改成 type: NodePort。完成後就可以使用 https://NodeIP:nodePort 地址訪問 dashboard。\n1 2 3 [root@ula ~]# kubectl get svc -n kubernetes-dashboard kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.107.248.188 \u0026lt;none\u0026gt; 443:31302/TCP 44m 由於 Dashboard 默認使用 https，其證書不受瀏覽器信任，所以訪問時加上 https 強制跳轉就可以了。\n登入 登錄 Dashboard 支持 Kubeconfig 和 Token 兩種認證方式，Kubeconfig 中也依賴 token 字段，所以生成 token 這一步是必不可少的。下面紀錄使用 token 的方式登錄。\n建立 service account \u0026amp; role binding 準備 yaml 檔 sc-ula.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ula annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef: kind: ClusterRole name: cluster-admin # k8s 預設建立的角色 apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: ula namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: ula namespace: kube-system labels: kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile 使用 kubectl apply 建立\n1 kubectl apply -f sc-ula.yaml 取得 Server Account Token 查看 service account secret\n1 2 3 4 5 6 7 8 9 10 11 kubectl get sa ula -n kube-system -o=yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2022-03-14T03:12:41Z\u0026#34; name: ula namespace: kube-system resourceVersion: \u0026#34;332586\u0026#34; uid: f048e242-5947-4945-8310-432628e635b7 secrets: - name: ula-token-qwddr 取得 token 字段，並使用 base64 decode\n1 2 kubectl get secret ula-token-qwddr -o jsonpath={.data.token} -n kube-system | base64 -d eyJhbGciOiJSUzI1NiIsImtpZCI6IjVadWpvSGEtR2tMR2ZFMGVHaGloWjJNOFdRcn............... 然後在 dashboard 登錄頁面上直接使用上面得到的 token 字符串即可登錄，這樣就可以擁有管理員權限操作整個 kubernetes 集群的對象，也可以新建一個指定操作權限的用戶。\nReference https://github.com/kubernetes/dashboard https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/ ","date":"2022-03-13T14:46:00Z","permalink":"http://localhost:1313/p/kubernets-dashboard-installation/","title":"安裝 Kubernetes Dashboard - 單集群可視化管理"},{"content":"去年疫情悶了半年，十一月初就早早訂了 228 的司馬庫斯的櫻花祭旅遊。因為不想舟車勞頓的開三四個小時的山路，就決定邀請大姊男友家一起直接包八人小團出遊！每次出遊前一個禮拜就會忍不住每天都看好幾次的氣象預報，但結果是，不是前一天看的預報都不準 \u0026#x1f923;\n然超級幸運的是，原先連續兩個禮拜每天都在下雨的北部，竟然就這麼剛剛好的在連假前一天正式放晴 \u0026#x2600;\u0026#xfe0f; \u0026#x2600;\u0026#xfe0f; 行前每天都在祈禱大晴天，甚至還在回嘉的前一天按照網路上的不科學習俗畫了烏龜燒 XD 但儘管完全沒根據，還是很神奇的兩天都是一點雲都沒有的超級好天氣!!(故意沒畫雲) amazing~~ \u0026#x1f973; \u0026#x1f60e; 第一晚夜宿新竹市區 因為想多留一點時間在司馬庫斯，所以第一個晚上決定先上新竹住一晚。準時的約了旅行社安排的晚上七點，但連假國道塞好塞滿，所以最後十點才到飯店。城隍廟夜市幾乎都關了，所幸還能遇到開得很晚的碳烤，便買了幾道菜配酒，不亦樂乎！ 然後第一天晚上的旅館爛到不知道如何評論 \u0026#x1f643; 下次非車程很遠的旅行還是自助遊最好了，如今已經不像大學時的窮遊可以享受省錢的快樂啦 \u0026#x1f92a;\n合興車站 隔天一早八點半便從飯店出發，早上飯店附餐是很神奇的便當 =_=? 吃完便當配菜就跟姊們去附近的全家解決。路上買了司機推薦的新竹溫媽媽客家菜包跟草仔粿，米香很香，內餡也好吃！ 對比之下，我的構圖還是有待加強 TVT 辛苦我姊了 到第一個景點時，雲就都散了，天氣超好! 軍艦岩 前往軍艦岩前經過了一個觀景台，在馬里光部落附近。 在軍艦岩有兩座吊橋，春天大部分的樹葉都變綠了，但還能看到些許紅黃褐色的葉子穿插，呈現山巒的層次美。河水量充沛，且非常乾淨，好美哇~~ 司馬庫斯 一個小時後，大概一點就到司馬庫斯了！先吃個從合興車站帶上來的山豬肉便當(意外好吃)。在三點入住前到處在部落逛逛，趁著前一批遊客離開、下一批遊客到來前，在大道跟部落入口拍照，提早來真的是太對了 \u0026#x1f97a; \u0026#x1f607; 大概十分鐘到上面的觀景台，路途上有一隻可愛的黃牛 \u0026#x1f42e;，觀景台可以看到整個司馬庫斯部落。 然後三點準時跟司機 check-in 小木屋。這次因為沒有馬上就訂好，所以只能住靠近大門的伯特利屋 雅房，但慶幸的是住在二樓，而且衛浴就房間在旁邊！大家在房間內各自休息，嗑了一些帶來的零食跟早上沒吃完的菜包充飢，還睡了一小覺補眠，夕陽灑進來，剛剛好的溫度，太舒服啦 \u0026#x1f606; 從二樓拍出去的風景，紀錄司馬庫斯太陽下山的光影變化。 七點準時到也是木頭搭建的餐廳吃提前一個禮拜跟旅行社點好的晚餐，拔還買了水蜜桃酒跟米酒配。 吃飽出來天色就完全暗了，一抬頭是滿天的星辰大海，美得誇張 \u0026#x2b50;\u0026#x2b50;\u0026#x2b50; 用二姊的 11 pro 夜間模式拍出來的星星還能這麼明顯 \u0026#x1f97a; 讚嘆 iPhone！ 洗了個水壓超強的舒服熱水澡之後就迅速的睡覺了 \u0026#x1f923; 隔壁還能聽到拔的打呼聲哈哈哈 \u0026#x1f923;\n隔天起床，凌晨太陽還沒出來的時候，超·級·冷！直接把所有衣服套在身上 \u0026#x1f62c; 吃完早餐後太陽整個出來又把早上穿的衣服又脫回去 XDD 溫差超大。 早餐是七點，吃清粥小菜跟吐司還有蘋果(本來後來要帶去爬山吃，殊不知沒吃到還直接忘記，完整的帶四顆回台北 \u0026#x1f602; \u0026#x1f643; ))\n巨木群登山步道 大概快八點開始登山，整條步道來回大概十一公里，路途上很平緩，因為上山已經用車爬三四個小時了嘛 XDDD 路上風景也很漂亮，能經過一整排的櫻花林跟竹子林，最後到達巨木群。幸虧這兩天的好天氣，讓泥土大部分都乾了，保住了我跟姊為了這兩天買的新鞋 \u0026#x1f923; 一大早出發的好處就是登山口都沒人，拍了超級漂亮的認證照 \u0026#x1f607; 路上只有我們的櫻花林，實在太美了。 回顧照片仍舊會讚嘆的美景。 抵達巨木群終點 (題外話：到終點的時候剛好遇到比我們早先出發的團，拍照就無法那麼悠閒的拍了，決定快走到下一座巨木，不然桑屍就追上來啦 -v-) 謝家照片合集 拔麻的意境照，太喜歡第二張櫻花步道的背影照了，回來直接設為手機封面!!! 大姊 二姊 本妹 ","date":"2022-02-28T21:00:00+08:00","image":"http://localhost:1313/p/smangus-2022/cover_hu8184729626239843418.jpg","permalink":"http://localhost:1313/p/smangus-2022/","title":"司馬庫斯兩日遊🤎"},{"content":"問題描述 nginx ingress 從原本 deprecated 的 stable/nginx-ingress helm chart 改為 ingress-nginx/ingress-nginx chart 後，發現 ingress resource 的 nginx 網頁 404 not found，查看 ingress nginx controller log 發現有 ingress does not contain a valid IngressClass 的錯誤。\n完整 log 如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [root@testm terraform]# kubectl logs -f -n ingress-nginx ingress-nginx-controller-5c5bf8c854-7pcf7 ------------------------------------------------------------------------------- NGINX Ingress controller Release: v1.1.1 Build: a17181e43ec85534a6fea968d95d019c5a4bc8cf Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.19.9 ------------------------------------------------------------------------------- W0209 05:43:07.359176 7 client_config.go:615] Neither --kubeconfig nor --master was specified. Using thesterConfig. This might not work. I0209 05:43:07.359883 7 main.go:223] \u0026#34;Creating API client\u0026#34; host=\u0026#34;https://10.96.0.1:443\u0026#34; I0209 05:43:07.379993 7 main.go:267] \u0026#34;Running in Kubernetes cluster\u0026#34; major=\u0026#34;1\u0026#34; minor=\u0026#34;20\u0026#34; git=\u0026#34;v1.20.15\u0026#34; \u0026#34;clean\u0026#34; commit=\u0026#34;8f1e5bf0b9729a899b8df86249b56e2c74aebc55\u0026#34; platform=\u0026#34;linux/amd64\u0026#34; I0209 05:43:07.644539 7 main.go:104] \u0026#34;SSL fake certificate created\u0026#34; file=\u0026#34;/etc/ingress-controller/ssl/defake-certificate.pem\u0026#34; I0209 05:43:07.675915 7 ssl.go:531] \u0026#34;loading tls certificate\u0026#34; path=\u0026#34;/usr/local/certificates/cert\u0026#34; key=\u0026#34;/ual/certificates/key\u0026#34; I0209 05:43:07.727278 7 nginx.go:255] \u0026#34;Starting NGINX Ingress controller\u0026#34; I0209 05:43:07.828276 7 event.go:282] Event(v1.ObjectReference{Kind:\u0026#34;ConfigMap\u0026#34;, Namespace:\u0026#34;ingress-nginxe:\u0026#34;ingress-nginx-controller\u0026#34;, UID:\u0026#34;c6943575-18fe-471a-aa44-5f251af7a277\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;104FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;CREATE\u0026#39; ConfigMap ingress-nginx/ingress-nginx-controller I0209 05:43:08.929877 7 nginx.go:297] \u0026#34;Starting NGINX process\u0026#34; I0209 05:43:08.930002 7 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-cler-leader... I0209 05:43:08.930504 7 nginx.go:317] \u0026#34;Starting validation webhook\u0026#34; address=\u0026#34;:8443\u0026#34; certPath=\u0026#34;/usr/local/icates/cert\u0026#34; keyPath=\u0026#34;/usr/local/certificates/key\u0026#34; I0209 05:43:08.930926 7 controller.go:155] \u0026#34;Configuration changes detected, backend reload required\u0026#34; I0209 05:43:08.945063 7 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-controllder I0209 05:43:08.945130 7 status.go:84] \u0026#34;New leader elected\u0026#34; identity=\u0026#34;ingress-nginx-controller-5c5bf8c854- I0209 05:43:09.015595 7 controller.go:172] \u0026#34;Backend successfully reloaded\u0026#34; I0209 05:43:09.015708 7 controller.go:183] \u0026#34;Initial sync, sleeping for 1 second\u0026#34; I0209 05:43:09.015766 7 event.go:282] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;ingress-nginx\u0026#34;, Namress-nginx-controller-5c5bf8c854-7pcf7\u0026#34;, UID:\u0026#34;f84b09e4-7d7d-40bf-ae66-f2eb72ab7a59\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceV:\u0026#34;104846\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;RELOAD\u0026#39; NGINX reload triggered due to a change in configurat W0209 05:43:26.721706 7 controller.go:988] Error obtaining Endpoints for Service \u0026#34;monitoring/prometheus-or-prometheus\u0026#34;: no object matching key \u0026#34;monitoring/prometheus-operator-prometheus\u0026#34; in local store W0209 05:43:26.721831 7 controller.go:1083] Service \u0026#34;monitoring/prometheus-operator-grafana\u0026#34; does not havactive Endpoint. W0209 05:43:26.722981 7 controller.go:988] Error obtaining Endpoints for Service \u0026#34;monitoring/prometheus-or-alertmanager\u0026#34;: no object matching key \u0026#34;monitoring/prometheus-operator-alertmanager\u0026#34; in local store I0209 05:43:26.781310 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.06s renderingIngressLength:1 renderingIngressTime:0.001s admissionTime:18.0kBs testedConfiguraze:0.061} I0209 05:43:26.781370 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorinetheus-ingress\u0026#34; I0209 05:43:26.784116 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.063s renderingIngressLength:1 renderingIngressTime:0s admissionTime:18.0kBs testedConfiguratio0.063} I0209 05:43:26.784155 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorinana-ingress\u0026#34; I0209 05:43:26.785125 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.063s renderingIngressLength:1 renderingIngressTime:0.002s admissionTime:18.1kBs testedConfigurize:0.065} I0209 05:43:26.785171 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;monitorintmanager-ingress\u0026#34; I0209 05:43:26.785238 7 admission.go:149] processed ingress via admission controller {testedIngressLengthtedIngressTime:0.064s renderingIngressLength:1 renderingIngressTime:0s admissionTime:18.0kBs testedConfiguratio0.064} I0209 05:43:26.785259 7 main.go:101] \u0026#34;successfully validated configuration, accepting\u0026#34; ingress=\u0026#34;istio-sysali-ingress\u0026#34; I0209 05:43:26.931088 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/grafana-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; I0209 05:43:26.931132 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/prometheus-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; I0209 05:43:26.931149 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;monitoring/alertmanager-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; I0209 05:43:26.945691 7 store.go:420] \u0026#34;Ignoring ingress because of error while validating ingress class\u0026#34; s=\u0026#34;istio-system/kiali-ingress\u0026#34; error=\u0026#34;ingress does not contain a valid IngressClass\u0026#34; 原 Ingress 配置 以 grafana-ing.tf 為範例 (使用 terraform 自動部署工具安裝)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;grafana_ingress\u0026#34; { count = var.package.prometheus == true ? 1 : 0 metadata { name = \u0026#34;grafana-ingress\u0026#34; namespace = \u0026#34;monitoring\u0026#34; } spec { rule { host = \u0026#34;${var.prometheus-options.host_grafana}.${var.domain}\u0026#34; http { path { backend { service_name = \u0026#34;prometheus-operator-grafana\u0026#34; service_port = 80 } path = \u0026#34;/\u0026#34; } } } } depends_on = [ helm_release.ingress-nginx, ] } 發生原因 原因是在 ingress v1.0.0 版之後，ingress 需要加上 ingress class，請參考 github 的 #7341 pull request，如果沒有，controller 會丟 Ignoring ingress because of error while validating ingress class\u0026quot; ingress=\u0026quot;k8sNamespace/ingressResourceName\u0026quot; error=\u0026quot;ingress does not contain a valid IngressClass\u0026quot; 的錯誤。\n資訊\nAn Ingress Class is basically a category which specify who needs to serve and manage the Ingress, this is necessary since in a cluster you can have more than one Ingress controller, each one with its rules and configurations.\n解決方法 在 ingress resource 中的 metadata 欄位加上 annotations: kubernetes.io/ingress.class: \u0026quot;nginx\u0026quot; 即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;grafana_ingress\u0026#34; { count = var.package.prometheus == true ? 1 : 0 metadata { name = \u0026#34;grafana-ingress\u0026#34; namespace = \u0026#34;monitoring\u0026#34; annotations = { \u0026#34;kubernetes.io/ingress.class\u0026#34; = \u0026#34;nginx\u0026#34; } } spec { rule { host = \u0026#34;${var.prometheus-options.host_grafana}.${var.domain}\u0026#34; http { path { backend { service_name = \u0026#34;prometheus-operator-grafana\u0026#34; service_port = 80 } path = \u0026#34;/\u0026#34; } } } } depends_on = [ helm_release.ingress-nginx, ] } 注意 上述方法僅適用於 kubernetes 1.22 版以前。k8s 1.22 版以後請使用 ingressClassName 於 spec 區塊下。請參考官網說明。\nReference https://forum.linuxfoundation.org/discussion/859965/exercise-7-nginx-update-requires-change-to-yaml https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/ingress https://stackoverflow.com/questions/65979766/ingress-with-nginx-controller-not-working-address-missing ","date":"2022-02-11T06:20:00Z","permalink":"http://localhost:1313/p/kubernets-ingress-invalid-ingressclass/","title":"Ingress does not contain a valid IngressClass"},{"content":"在下 kubectl 時出現 Unable to connect to the server: x509: certificate has expired or is not yet valid 的錯誤，原因是 kubernetes apiserver 證書已過期，kubernetes 的 apiServer 與 kubelet 的訪問授權證書是一年，官方表示通過這種方式，讓用戶不斷的升級版本。\n目前有幾種解決方式：\n重新生成證書取代過期的證書 (本次作法) 升級集群以自動更新證書 部屬一套新的環境，將業務遷移過去 去掉證書驗證功能 (不安全且不科學，需要自己改 source code) 查看證書的有效日期 透過 openssl 直接查證書內容\n1 2 3 $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | grep Not Not Before: Nov 17 04:48:20 2020 GMT Not After : Nov 17 04:48:20 2021 GMT 或是透過 kubeadm 檢查 Kubernetes 環境證書\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W1118 09:51:35.880390 7092 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no apiserver Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; ca no apiserver-etcd-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no apiserver-kubelet-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; ca no controller-manager.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no etcd-healthcheck-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-peer Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-server Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; etcd-ca no front-proxy-client Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; front-proxy-ca no scheduler.conf Nov 17, 2021 04:48 UTC \u0026lt;invalid\u0026gt; no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Nov 15, 2030 04:48 UTC 8y no etcd-ca Nov 15, 2030 04:48 UTC 8y no front-proxy-ca Nov 15, 2030 04:48 UTC 8y no 經查看 k8s master 組件證書都過期了。\n更新證書 備份舊有的配置文件與證書 1 $ cp -rf /etc/kubernetes /etc/kubernets.bak 更新證書 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ kubeadm alpha certs renew all [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [renew] Error reading configuration from the Cluster. Falling back to default configuration W1118 11:11:52.322016 26585 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed 重新生成配置文件\n這些配置文件中包含證書，所以需要重新生成\n1 2 $ rm -rf /etc/kubernetes/*.conf $ kubeadm init phase kubeconfig all --apiserver-advertise-address 10.1.5.21 更新配置身份認證的 $HOME/.kube/config 檔案\n將重新生成於 /etc/kubernetes 下的 admin.conf 檔案覆蓋原先的 ~/.kube/config 1 2 $ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ chown $(id -u):$(id -g) $HOME/.kube/config 重新啟動 kubelet \u0026amp; docker service 1 2 $ systemctl restart kubelet $ systemctl restart docker 重新使用 kubectl 訪問集群 1 2 3 4 5 6 7 8 $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8sm1 Ready master 365d v1.17.13 k8sm2 Ready master 365d v1.17.13 k8sm3 Ready master 365d v1.17.13 k8sw1 Ready \u0026lt;none\u0026gt; 365d v1.17.13 k8sw2 Ready \u0026lt;none\u0026gt; 365d v1.17.13 k8sw3 Ready \u0026lt;none\u0026gt; 365d v1.17.13 如果是多 master，上面的步驟在每個 master 都要做 Reference https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired https://cloud.tencent.com/developer/article/1832411 ","date":"2021-11-21T17:36:00Z","permalink":"http://localhost:1313/p/kubernets-ca-expired/","title":"Unable to connect to the server: x509: certificate has expired or is not yet valid"},{"content":"下載 Istio 下載資源 用自動化工具下載並提取最新版本（Linux 或 macOS）：\n1 $ curl -L https://istio.io/downloadIstio | sh - 或是用指定參數下載指定的、不同處理器體系的版本。例如，下載 x86_64 架構的、1.6.8 版本的 Istio ，運行：\n1 $ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.8 TARGET_ARCH=x86_64 sh - 進入 Istio 包目錄 1 $ cd istio-1.11.4 安裝目錄包含：\nsamples/ 目錄下的示例應用程序 bin/ 目錄下的 [istioctl](https://www.bookstack.cn/read/istio-1.11-zh/af90c7c768b11bf8.md) 客户端二進制文件 設定 istioctl 將 istioctl 客户端加入執行路径（Linux or macOS）:\n1 $ export PATH=$PWD/bin:$PATH 部署 Istio Operator 1 $ istioctl operator init 此命令運行 Operator 在 istio-operator 命名空間中創建以下資源：\nOperator 自定義資源定義（CRD） Operator 控制器的 deployment 對象 一個用來訪問 Operator 指標的服務 Istio Operator 運行必須的 RBAC 規則 查看創建的資源\n1 kubectl get all -n istio-operator 安裝 Istio 可以依據 profile 安裝指定的 istio 套件\n1 2 3 4 5 6 7 8 9 10 $ kubectl create ns istio-system $ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istiocontrolplane spec: profile: demo EOF 各種 profile 的 yaml 檔放置在 ./manifests/profiles 下，可依照需求改內容參數。並使用 $ kubectl apply -f xxx.yaml 部屬。\n安裝 Addons Component 不同於以往 1.6 以前的版本的 istio yaml 檔可直接指定 addonComponents，在 1.11 版如果要安裝 Kiali、Jaeger 等 addon component，則需要另外部屬。 1 2 3 4 5 # 一次部屬所有 addons $ kubectl apply -f samples/addons # 單獨指定套件部屬 $ kubectl apply -f samples/addons/kiali.yaml Resource https://preliminary.istio.io/latest/zh/docs/setup/install/operator/ https://istio.io/latest/docs/setup/getting-started/ ","date":"2021-10-26T20:27:00Z","permalink":"http://localhost:1313/p/kubernets-install-istio/","title":"使用 istio operator 安裝 Istio v1.11"},{"content":"原先使用 k8s-at-home 的 helm chart 部屬，但完成後發現 node 安裝後會 deploy 異常，懷疑是 persistence 設定問題，但又不想花時間深究，所以就直接自己寫 yaml 部屬比較快。\n準備 node-red.yaml 檔 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nodered-pvc namespace: node-red spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: nfs --- apiVersion: apps/v1 kind: Deployment metadata: name: node-red namespace: node-red spec: replicas: 1 selector: matchLabels: app: node-red template: metadata: labels: app: node-red spec: containers: - name: node-red image: nodered/node-red:2.1.2 ports: - containerPort: 1880 name: node-red protocol: TCP imagePullPolicy: IfNotPresent volumeMounts: - name: nodered-data mountPath: /data volumes: - name: nodered-data persistentVolumeClaim: claimName: nodered-pvc --- apiVersion: v1 kind: Service metadata: name: node-red namespace: node-red spec: type: NodePort selector: app: node-red ports: - protocol: TCP port: 1880 nodePort: 31880 部屬 1 2 kubectl create ns node-red kubectl -n node-red apply -f node-red.yaml 完成 1 kubectl get all -n node-red 訪問 service 使用 NodePort，則直接使用 control plane 的 IP 以及 NodePort 指定的 port 31880 訪問 Node-RED 即可。 ","date":"2021-10-26T09:44:00Z","permalink":"http://localhost:1313/p/install-nodered-on-kubernetes/","title":"[Node-RED] Deploy on Kubernetes"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # note the old IPs allocated to the services kubectl get svc # edit config kubectl edit cm -n metallb-system config # delete the metallb pods kubectl -n metallb-system delete pod --all # watch the pods come back up kubectl -n metallb-system get pods -w # inspect new IPs of services kubectl get svc or\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026lt;\u0026lt; EOF \u0026gt; new_config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 10.0.0.10-10.0.0.19 EOF # delete the old configmap kubectl -n metallb-system delete cm config # apply the new configmap kubectl apply -f new_config.yaml why we need to delete all metallb pod MetalLB rejects new configurations if the new configuration would break existing services. Your new configuration does not allow existing services to continue existing, so MetalLB ignored it. There are log entries in the controller and speaker pods about this.\nTo force MetalLB to accept an unsafe configuration, delete all the controller and speaker pods. When they restart, they\u0026rsquo;ll accept the new configuration and change all your services. kubectl delete po -n metallb-system --all\nReference https://github.com/metallb/metallb/issues/308 https://github.com/metallb/metallb/issues/348 ","date":"2021-09-16T10:30:00Z","permalink":"http://localhost:1313/p/kubernets-metallb/","title":"Change MetalLB IP Range"},{"content":"minikube 是一個由 Google 發布的部署單節點的 Kubernetes Cluster 的工具，可以安裝在本機上，支援 Windows 與 Mac Minikube 只有一個 Node (節點)。對於本地實驗可以避免節點不足的困擾；讓開發者可以在本機上輕易架設一個 Kubernetes Cluster，快速上手 Kubernetes 的指令與環境。\n運作原理就是會在本機上建立一個 virtual machine，並且在這 VM 建立一個 signle-node Kubernetes Cluster。\nminikube 適合用於開發環境測試，不會把它用在實際生產環境中。\n下載與部屬 Minikube 支援 Windows、MacOS、Linux，在這三種平台的本機端都可以安裝並執行 Minikube 。安裝及執行步驟，請參考官網。\n整體步驟如下：\n安裝Virtualization Software，如 VirtualBox 安裝 kubectl 套件，用以和 K8S 集群交互溝通 從 Github 下載 Minikube 套件 啟動 minikube 及 K8s 集群 使用 kubectl 操作集群及應用 官網跟其他教學文寫得很詳細，在這裡就不一一列示了。\nReference https://ithelp.ithome.com.tw/articles/10192490 ","date":"2021-09-05T22:17:00Z","permalink":"http://localhost:1313/p/kubernets-minikube/","title":"Minikube"},{"content":"Problem 在嘗試更新 Kubernetes 時，下了下面的 command 取得目前集群的組件狀態：\n1 $ kubectl get cs 發現 controller-mamager 和 scheduler 有 unhealthy 的狀態：\n1 2 3 4 NAME STATUS MESSAGE ERROR controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused scheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34; Reason 這兩個 pod 的非安全端口沒有開啟，健康檢查時報錯，但是由於本身服務是正常的，只是健康檢查的端口沒啟，所以不影響正常使用。\nSolution 在所有 Master nodes 上修改下面檔案:\n1 2 $ vim /etc/kubernetes/manifests/kube-scheduler.yaml $ vim /etc/kubernetes/manifests/kube-controller-manager.yaml 刪掉或註釋掉 - --port=0 (spec-\u0026gt;containers-\u0026gt;command) 這行\n1 $ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml 重啟 kubelet 服務\n1 $ sudo systemctl restart kubelet.service 這時10251，10252端口就開啟了，健康檢查狀態也正常了。\n1 2 3 [root@master1 ~]# netstat -tulpn | grep \u0026#39;10251\\|10252\u0026#39; tcp6 0 0 :::10251 :::* LISTEN 11863/kube-schedule tcp6 0 0 :::10252 :::* LISTEN 11902/kube-controll Reference https://www.cnblogs.com/wuliping/p/13780147.html ","date":"2021-08-31T21:15:00Z","permalink":"http://localhost:1313/p/kubernets-scheduler-controller-manager-unhealthy/","title":"解決 scheduler and controller-manager unhealthy state"},{"content":"上次將 K8s 集群從 1.7 升級到 1.20 之後，在創建 pvc 時，發現狀態會一直停留在 Pending，詳細資訊如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@master1 telegraf]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Pending nfs 5s [root@master1 telegraf]# kubectl describe pvc test Name: test Namespace: default StorageClass: nfs Status: Pending Volume: Labels: \u0026lt;none\u0026gt; Annotations: volume.beta.kubernetes.io/storage-provisioner: cluster.local/nfs-sc-0-nfs-client-provisioner Finalizers: [kubernetes.io/pvc-protection] Capacity: Access Modes: VolumeMode: Filesystem Used By: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ExternalProvisioning 10s (x5 over 50s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \u0026#34;cluster.local/nfs-sc-0-nfs-client-provisioner\u0026#34; or manually created by system administrator 查了原因可能是因為 selfLink 為空，無法建立參考。 Kubernetes v1.20 開始，根據 change log 說明，默認刪除了 metadata.selfLink 字段，然而部分應用仍然依賴於此功能，例如 nfs-client-provisioner。如果仍然要繼續使用這些應用，需要重新啟用selfLink。\n解決方式 修改 kube-apiserver 文件\n1 vim /etc/kubernetes/manifests/kube-apiserver.yaml 在 command 下的參數加上 --feature-gates=RemoveSelfLink=false\n1 2 3 4 5 6 7 8 9 10 11 spec: containers: - command: - kube-apiserver - --advertise-address=10.1.5.141 - --allow-privileged=true - --au... . . . **- --feature-gates=RemoveSelfLink=false** 儲存編輯後，kube-apiserver 便會自動重啟。\n","date":"2021-08-29T21:47:00Z","permalink":"http://localhost:1313/p/kubernets-nfs-client-provisioner-pending-in-creating-pvc/","title":"k8s v1.20 nfs-client-provisioner 創建 pvc 時停在 Pending"},{"content":"紀錄在現有 kubernetes 1.17 集群升級到 1.20 的過程。\n環境說明 集群配置 主機名 系統 IP Address vip Server Load Balancer (SLB) 10.1.5.140 master1 CentOS 7.8 10.1.5.141 Worker1 CentOS 7.8 10.1.5.142 當前運行版本 組件 版本 kubeadm v1.17.13 kubelet v1.17.13 kubectl v1.17.13 Container Runtime - Docker v20.10.7 etcd V3.4.3-0 kube-apiserver v1.17.17 kube-controller-manager v1.17.17 kube-proxy v1.17.17 kube-scheduler v1.17.17 coredns v1.6.5 pause v3.1 查看版本的命令如下\n1 2 3 4 kubeadm version kubelet --version kubectl version kubectl get node master1 -o yaml Kubernetes Non-Active Branch History 目前使用的 1.17 版已在 2021-01-13 EOL，final patch release 在 1.17.17，請參考官方 repo 說明。\n準備升級 版本升級的必要 對於 Kubernetes 集群的使用者： 更新的版本能有更新的功能、更加全面的安全補丁以及諸多的bugfix。\n對於 Kubernetes 集群的運維者： 通過集群升級功能可以拉齊所管理的集群版本，減少集群版本的碎片化，從而減少管理成本和維護成本。\n升級注意事項 升級僅支持一個小版本號。也就是說，只能從 1.7 升級到 1.8 的最新版本，或是從 1.17.13 升級到 1.17.17，而不能從 1.7 直接升級到 1.9。 一次更新一個節點，確保 Kubernetes 功能不會因為更新而中斷。 升級後所有容器都會被重啟，避免服務中斷，需確保應用程式使用進階的 Kubernetes API 建立，如 Deployment，或是利用副本機制。 kubeadm upgrade 不會影響工作負載，只會涉及Kubernetes 內部的組件，但為保險起見，仍然可以先備份 etcd 的狀態。 備份 etcd Snapshot 又稱為快照，就像照相一樣，在某個時間點，將硬碟目前的整個狀態儲存起來，以作為將來還原的備份依據。 Snapshot 是幾乎所有的儲存服務設備都會提供的功能，就像是幫你硬碟上的資料照張像一樣，把這個目前的狀態記錄下來，以備將來還原之用。\netcd 的備份有兩種方式：\n使用 etcdctl snapshot save 進行備份 1 $ kubectl -n kube-system exec -it etcd-master1 -- sh -c \u0026#34;ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key etcdctl --endpoints=https://127.0.0.1:2379 snapshot save /var/lib/etcd/snapshot1.db\u0026#34; 使用 etcdctl snapshot status查看備份\n1 $ kubectl -n kube-system exec -it etcd-master1 -- sh -c \u0026#34;ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key etcdctl --endpoints=https://127.0.0.1:2379 snapshot status -w table /var/lib/etcd/snapshot.db\u0026#34; 檢視備份檔\n1 2 3 4 5 6 $ ls /var/lib/etcd/ member snapshot.db $ mkdir ~/backup $ cp /var/lib/etcd/snapshot.db ~/backup/\t# 備份 etcd 核心資料檔案 $ cp -r /etc/kubernetes/pki/etcd $HOME/backup/ 使用 docker etcd image 連線進入 etcd 內部下 command 1 2 3 4 5 6 7 8 9 10 11 $ mkdir -vp /data/backup $ docker run --rm \\ -v /data/backup:/backup \\ -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \\ --env ETCDCTL_API=3 \\ registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0 \\ /bin/sh -c \u0026#34;etcdctl --endpoints=https://192.168.12.226:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \\ --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\ snapshot save /backup/etcd-snapshot-1.19.13.db\u0026#34; 開始升級 1.17.13 ~ 1.18.20 主節點 確認可升級版本與升級方案\n1 $ yum list --showduplicates kubeadm --disableexcludes=kubernetes 通過以上命令查詢到 1.18 當前最新版是 1.18.20-0 版本。master 如有三個節點可先從 master3 節點開始升級。\nyum 升级 kubernetes 插件\n1 $ yum install kubeadm-1.18.20-0 kubelet-1.18.20-0 kubectl-1.18.20-0 --disableexcludes=kubernetes 騰空節點檢查集群是否可升級\n1 2 $ kubectl drain master1 --ignore-daemonsets $ kubeadm upgrade plan 升級\n1 $ kubeadm upgrade apply v1.18.20 重啟 kubelet 取消節點保護\n1 2 3 4 $ systemctl daemon-reload $ systemctl restart kubelet $ kubectl uncordon master1 $ kubectl get nodes 經測試後雖然測試環境的集群的 master 只有一個，但升級單一這個 master 後好像不會造成 Pod 的影響。\n工作節點 切換到 worker node，yum升级kubernetes插件\n1 2 3 # 在 worker node 節點操作 $ ssh worker $ yum install kubeadm-1.18.20-0 kubelet-1.18.20-0 kubectl-1.18.20-0 --disableexcludes=kubernetes 將節點標記為不可調度並逐出工作負載。\n1 2 3 4 5 6 7 # master1 $ kubectl drain worker1 --ignore-daemonsets # 可以看見與下類似的輸出： node/ip-172-31-85-18 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-dj7d7, kube-system/weave-net-z65qx node/ip-172-31-85-18 drained 升級\n1 2 # worker1 $ kubeadm upgrade node 重啟kubelet 並取消對節點的保護\n1 2 3 4 5 6 7 # worker1 $ systemctl daemon-reload $ systemctl restart kubelet # master1 $ kubectl uncordon worker1 # 通過將節點標記為可調度，讓節點重新上線，如果有做保護節點的步驟的話在做。 $ kubectl uncordon worker1 從 1.18.20 升級到 1.19.12 重複以上動作\n從 1.19.12 升級到 1.20.10 重複以上動作\nReference https://cloud.tencent.com/developer/article/1848150 https://v1-18.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ https://xie.infoq.cn/article/6e47d96d4c4a4c6d47852adc8 https://www.796t.com/article.php?id=260536 for etcd backup ","date":"2021-08-22T21:29:00Z","permalink":"http://localhost:1313/p/k8s-upgrade/","title":"Kubernetes 升級紀錄"},{"content":"ConfigMap ConfigMap 以 key-vaule 的方式用來描述系統相關設定，所有與應用程式相關的非敏感性未加密的資訊可放在 ConfigMap 內。而如有敏感性資料，則需透過 Secret。\n主要目的 主要目的是將應用程式與設定解耦，ConfigMap 與 Pod 將個別單獨存在於 k8s 叢集中，當 Pod 需要使用 ConfigMap 時才需要將 ConfigMap 掛載到 Pod 內使用。解耦的好處有：\n便於管理 彈性高，可掛載不同的 ConfigMap 到 Pod 內使用；或是同一個 ConfigMap 掛載到多個 Pod。 用法 Kubernetes 的 ConfigMap 透過 kubectl create 或 kubectl apply 來建立。\n1 2 $ kubectl create configmap [資源名字] [來源參數] $ kubectl apply condfigmap.yaml 使用 kubectl create 建立 使用 kubectl create 可以從檔案路徑、檔案或是 literal value 來建立 configMap。\n\u0026ndash;from-file 1 2 3 4 5 6 7 8 # 建立名為 myConf、資料來源是某路徑下所有檔案的 configMap $ kubectl create configmap myConf --from-file=/path/for/config/file/ # 建立名為 myConf、資料來源是一個檔案的 configMap $ kubectl create configmap myConf --from-file=/path/to/app.properties # 建立名為 myConf、資料來源是多個檔案的 configMap $ kubectl create configmap myConf --from-file=/path/of/app1.properties --from-file=/path/of/app2.properties 註釋\n如果是來源是檔案的話，則 configMap 中的 key 就會是檔名，value 則是檔案內容。\n\u0026ndash;from-literal 1 2 3 4 # 建立名為 myConf、包含指定鍵值對的 configMap $ kubectl create configmap myConf --from-literal=key1=config1 $ kubectl create configmap myConf --from-literal=key1=config1 --from-literal=key2=config2 兩個共用 1 2 3 $ kubectl create configmap myConf --from-file=/path/of/config.conf \\ --from-literal=key1=config1 \\ --from-literal=key2=config2 \u0026ndash;from-env-file 使用環境變數表示的檔案。 警告\n請注意，value 如果有 \u0026quot;\u0026quot; 則會視為是值的一部份。且如果在同個 create 中使用多個 \u0026ndash;from-env-file 則指會應用最後一個。\n使用 kubectl apply yaml 檔案建立 準備 yaml 檔\n1 2 3 4 5 6 7 8 9 10 11 12 --- apiVersion: v1 kind: ConfigMap metadata: name: myConf data: key1: config1 key2: config2 app.properties: | property.1 = value1 property.2 = value2 property.3 = value3 佈署\n1 $ kubectl apply -f configmap.yaml 查看 ConfigMap 建立完成後可以使用 kubectl get 或是 kubectl describe 的擷取 configMap 的內容。\n1 $ kubectl get configmaps myConf -o yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2021-01-04T18:52:05Z name: myConf namespace: default resourceVersion: \u0026#34;516\u0026#34; uid: b4952dc3-d670-11e5-8cd0-68f728db1985 data: key1: config1 key2: config2 app.properties: | property.1 = value1 property.2 = value2 property.3 = value3 將 ConfigMap 掛載到 pod 使用 當成環境變數使用 pod 的 yaml 檔如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: testenv spec: containers: - name: test image: tomcat:8 imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $(KEY1_ENV)\u0026#34; ] env: - name: KEY1_ENV valueFrom: configMapKeyRef: name: myConf key: key1 將 pod 跑起來後，ConfigMap myConf 中的 key1 的 value 就會做為環境變數 KEY1_ENV 的值。\n掛載成 volume pod 的 yaml 檔如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: testvolume spec: containers: - name: test image: tomcat: 8 imagePullPolicy: IfNotPresent command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/keys\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: myConf 註釋\n使用 volume 將 ConfigMap 作為文件或目錄直接掛載，ConfigMap 中每一個 key-value 鍵值對都會生成一個文件，key 為文件名，value 為內容。\n另一種方式，只掛載某個 key，並指定相對路徑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: testvolume spec: containers: - name: test image: tomcat:8 imagePullPolicy: IfNotPresent volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: myConf items: - key: key1 path: /path/to/key1 # key1 會放在 mountPath /etc/config/path/to 下。 - key: app.properties path: app.properties # 如果 path 與 key 相同，則會直接把 app.properties 文件放在 mountPath 下。 Reference https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ https://www.cnblogs.com/pu20065226/p/10690628.html ","date":"2021-01-08T14:26:00Z","permalink":"http://localhost:1313/p/kubernets-configmap/","title":"ConfigMap 建立及掛載"},{"content":"K8s Obejcts 常用的基本 objects Pod\nPod 有兩種類型：普通 Pod 和靜態 Pod (static pod)。靜態 Pod 即不通過 K8S 調度和創建，直接在某個具體的 Node 機器上通過具體的文件來啟動。普通 Pod 則是由 K8S 創建、調度，同時數據存放在 etcd 中。 Service\n可以認為是 pod 的反向代理，負責接收客戶端請求，把請求轉給 pod。因為每個 pod 都有自己的內部 ip，但是 deployment 的 pod 的 ip 是有可能變的 (pod 掛掉或複製)，所以需要 service 來做類似中間者的抽象存在。Service 挑選、關聯 Pod 的方式為基於 Label Selector 進行定義。通過 type 在 ServiceSpec中指定可以以不同的方式公開服務： ClusterIP (default)：只有內部 IP，只能從集群內訪問服務。 NodePort：工作於每個節點的主機 IP 之上，可以從集群外部訪問服務。ClusterIP 的超集。 LoadBalancer：在當前集群中創建一個外部負載平衡器，把外部請求負載均衡至多個Node 主機 IP 的 NodePort 之上，為該服務分配一個固定的外部 IP。NodePort 的超集。 ExternalName-externalName：通過在 Cluster 的 DNS Server 添加一筆 CName Record，使用指定名稱（在 yaml 中設定）公開服務，不使用代理 (kube-proxy)，而是透過 kube-dns。此類型在 kubernetes 1.7 版本有提供，但是要 kube-dns version 要在 1.14.9 以上，否則會遇到 Resolve External Name issue。主要是為了讓不同 namespace 中的 Service 可以利用 ExternalName 設定的外部名稱連到其它的 namespace 中的 Service。 Label\n標籤用於區分對象，使用標籤引用對象而不再是 IP。Label 以鍵值對的形式存在，每個對象可以有多個標籤，通過標籤可以關聯對象。 Volume\n共享 Pod 中使用的數據。 Namespace\n可以抽象理解為一群對象的集合。 High-level objects (Controllers) 建立在基本對象的基礎上，提供了附加的功能和便利性：\nReplicaSet：確保運行指定數量的 pod，官方建議使用 Deployment 來自動管理。 Deployment：最常見的部屬 Pod 的方法，可以用來創建 replicaSet。支持版本記錄、rolling update/back、暫停升級等高級特性。 StatefulSet：用於管理具有持久性存儲的有狀態應用程序。 有序部屬、有序擴展：即 Pod 是有順序的，在部署或者擴展的時候要依據定義的順序依次依次進行（即從 0 到 N-1 ，在下一個 Pod 運行之前所有之前的 Pod 必須都是 Running 和 Ready 狀態），基於 init containers 來實現。所以 pod name 固定且會帶一個有序的序號(app-0, app-1\u0026hellip;)。 持久性儲存：基於 pvc 實現，pod 重新調度後仍會跟 volume 保持關聯， volume 不會隨 pod 刪除而刪除。 穩定網路標誌：基於 Headless Service（即沒有 Cluster IP 的 Service）來實現，Pod 重新調度後其 PodName 和 HostName 不變。 DaemonSet：保證在每個 Node 上都運行一個容器副本，常用來部署一些集群的日誌、監控或者其他系統管理應用。 Job：一次性任務，運行完後Pod銷毀，不再自動重建。 CronJob：定時任務。 Resource https://www.cnblogs.com/ccbloom/p/11311286.html#%E5%9F%BA%E7%A1%80%E5%AF%B9%E8%B1%A1 https://www.cnblogs.com/baoshu/p/13124881.html ","date":"2021-01-07T17:38:00Z","permalink":"http://localhost:1313/p/kubernets-object/","title":"Kubernetes Object 基本對象介紹"},{"content":"長大以後都沒有過的全家出國旅行，在這次終於成行了！從六月決議玩開始，我就擔任導遊的角色，著手規劃了整個旅程。來來回回修改不下十次，甚至連小手冊都做了，雖然最後只有自己在看 XD\n這幾天天氣都極好，第一天抵達的時候多雲、第二天早上飄雨外其餘的都是豔陽高照的超級好天氣，真的很幸運，因為高中朋友早我一個禮拜去剛好遇到每天都在下雨的颱風(QQ)，出發前一個禮拜每天都不時的看四五個不同的氣象預報呢，萬幸萬幸。\nDAY1 第一天坐早上六點半的樂桃航空，我坐凌晨的客運到機場，本來打算在機場睡一下的，但冷爆。拔昨晚從大陸飛到桃園等，麻跟二姊則從大林開車北上順道載；完全沒睡，真名符其實的紅眼班機丫 \u0026#x1f62a; 最喜歡準備出發的時候，興奮得難以言喻，然後就載換行李箱的時候把手機忘載報到櫃台了哈哈哈超級烏龍。幸好在時間有餘的起飛前發現了，有驚無險地拿回來。\n日本比台灣快一個小時，所以當地時間九點抵達那霸機場，原本以為已經做好萬全準備了，但抵達時還是發現忘了提錢買好景點套票，在機場補買票時，姊他們去買必吃的豬肉蛋飯糰，吃了明太子跟山苦瓜口味，真的就如評價說的好吃，大家都很喜歡，拔還說回程可以再來買 \u0026#x1f601; 租車 吃完後去 ORIX 接駁的地點坐去營業處取車（提前在 TABIRAI 日文網預定的，還比較划算呢 \u0026#x1f44d;）租的車是 NISSAN 的五人座休旅車，比家裡的 outlander 小一點，但在沖繩來說可是超級大車啊！放眼望去幾乎七成的車都是正方形屁股的小車。出發後先去隔壁的 OTS 買便宜的第四天的玉泉洞門票後就正式出發了。沒開過右駕的拔首次上路也是手忙腳亂的，切方向燈都會不小心開到雨刷 \u0026#x1f602; 而且停車的時候前兩天也是很不習慣(但後面愈停愈順)，然後固定副駕駛導遊的我到第四天也都還是心驚膽跳的，怕習慣左偏因此擦撞到路邊道路(雖然還是有擦到路邊的三角錐跟不小心開上路邊的坎，但幸虧都沒有刮傷，可喜可賀可歌可泣 \u0026#x1f913;) 古宇利島 古宇利島在北邊，這四天計畫由北慢慢玩回來。所以去的車程大概有一個半小時，這也是唯一開高速公路的路程，中間有路過到一個大型休息站休息，看得到一點點海景，沖繩真的沒有一處不美的 \u0026#x1f97a; 因為比較偏僻，所以蚊蟲特別多，但令人意外的是公廁非常乾淨，也終於體會到日本整潔的公共場所。後半程換我開車，不過左右相反真的超難適應的啊啊啊，戰戰兢兢地開的半小時後就到了南詰展望所。\n這邊是小停車場，從這可以看到整座延伸到對岸的古宇利大橋。可以的是早上的烏雲還沒散，還飄了點毛毛雨；幸好出發前有在蝦皮買了三頂漁夫帽，還帶了 NIKE 棒球帽，戴起來也都很讚 \u0026#x1f60e; 因為才剛在機場吃飯糰，所以中餐就直奔最有名的蝦蝦飯。正好天氣也沒有很熱，不然原先是打算找一間餐廳，不過計劃總是趕不是變化，來就是要當個稱職的觀光客啊!!!連點餐也不管評價多花了幾百塊買辣味，結果真的只多了辣粉，超不值 XDDDD (事後還把兩罐用剩的辣粉待在車上到第四天還是完好如初的扔了哈哈哈哈哈哈哈) 不過蝦蝦飯果然還是不愧其名的好吃，加點的牛肉也蠻嫩的，幸好有來，因為麻很期待吃(小阿姨也是不久前全家來玩過有特地跟她提過蝦蝦飯 \u0026#x1f923; )，一起坐在二樓的觀景階梯，還能看到古宇利橋的海景，大家都很滿意 \u0026#x1f496; \u0026#x1f496; 接著來到古宇利海洋塔，從入口處就可以看到頂樓的幸福鐘。雖然雲還沒開，但都這麼遠的開車來名護了，就算評價說是滿滿商人的包裝，還是要來看一下的。坐無人車繞著塔一圈圈的上塔頂，很新鮮有趣，沿途都是各種各樣整得漂亮的海島植物，一邊還能聽拔講解 XD。下車後先是參觀琳瑯滿目的貝殼館，在走道三四樓的觀景台看風景、敲敲幸福鐘。還參考 google maps 評價的建議買了好吃的南瓜可頌和泡芙吃 \u0026#x1f950; \u0026#x1f60b; 開上古宇利橋準備離島時，天空終於完全放晴了!!! \u0026#x2600;\u0026#xfe0f; 能在橋上看到藍天白雲、配上筆直寬闊的大橋，真的超級超級幸運! \u0026#x1f64c; 美麗海水族館 第一天的最後一站是原先沒有要去的美麗海水族館，但幸好行前臨時把 ORION 酒廠換掉了，水族館果然是個百看不膩又療癒的地方。因為入場已經四點了，所以還能退星光票的價差，超級划算!!五點有一場鯨鯊餵食秀，坐在有位置的劇場裡面，但往前走才發現有更大的景觀玻璃可以觀賞，拔麻都說被解說員給騙坐了 \u0026#x1f923; \u0026#x1f923; 不過能舒服的坐著還有一些專業的解說也是可以的啦。出館之後緊接著到室外的海豚劇場，五點半的太陽還是超大，但比中下午好很多了，而且傍晚的場次人也不會超爆，安排得很完美 \u0026#x1f973; 海豚有很多才藝，音樂也都下得恰到好處，整場表演都沒冷場，非常精采！拔麻還說比之前去北海道看的海豚表演還精彩呢 \u0026#x1f633; 不過水族館比想像中小很多，所以看到原本應該自由的在大海優游的鯨鯊和可愛的海豚們，在讚嘆和掌聲下還是不由自主地希望他們能開開心心的，下輩子如果還是魚，那冀望牠們都能遨遊在廣闊的大海中 \u0026#x1f97a; D1 晚餐 \u0026amp; 民宿 晚餐原先是想去附近很有名的島豚家吃很厲害的沖繩燒肉麵，但安排的時候忘了注意他們只營業到中午！太可惜了。只好先上路去第一晚的民宿，為了隔天早起浮潛方便，所以第一晚的落腳處直接選在恩納，車程需要一個半小時，中途我還看錯導航 \u0026#x1f613;，才終於到。超！餓！不過附近比較偏僻，所以晚餐只好吃附近的吉野家，雖然比起台灣的有豐富的選擇，但跟原先計畫的燒肉麵有很大的落差啊 \u0026#x1f62d; 吃飽後還誤闖隔壁外觀看起來很 fancy 的小鋼珠店，裡面真的超誇張，大概有上百台機檯而且座無虛席，完全大開眼界。第一晚的民宿是三晚下來最便宜的，便宜到我訂到四人房都不知道 \u0026#x1f974;\nDAY2 隔天原本計畫好要五點半左右起床，殊不知起來的時候已經六點半了，急忙地收拾行李換裝，拍蜜粉在沒洗頭的油田上 \u0026#x1f92a; \u0026#x1f60f; 再抽空跑去看拔麻起床了沒，結果也沒有哈哈哈 \u0026#x1f602; 約莫快七點才好不容易到大廳退房吃早餐，民宿有提供免費早餐(河粉、法國麵包夾肉、粥)，令人意外的是，雖然看起來簡單卻非常美味(全家一致認同)！不過邊吃還邊在擔心要不要打給浮潛的店家說要延後一個梯次，但幸好備餐速度很快，我們也迅速地解決後出發了～\n海底漫步 車程僅十分鐘（昨天忍著餓花時間開夜車是對的 \u0026#x1f918;）抵達時正好八點，很幸運地沒有真的遲到。放好東西並著裝完畢後，先由一位台灣教練帶我們到出海口，一邊講解海底漫步的注意事項，再由兩位日本教練陪同出海。早上天氣沒有太陽還有毛毛雨，不過幸好是水上活動，所以不受影響 \u0026#x270c;\u0026#xfe0f; 坐著快艇(吧?)，被大海及海風環繞，整著人都心曠神怡了起來，也能感受到大家的好心情。大約十分鐘就到了目的地，下去前麻超級緊張，但還是鼓起勇氣下去了 \u0026#x1f4aa;\n海底漫步是用很重的頭盔罩住頭，在頭盔裡有空氣可以呼吸，腰上會綁很重的石頭避免身體太輕。下去後真的是被滿坑滿谷的魚圍繞住，尤其是拿著麵包在餵魚的時候!!!超級神奇。雖然台灣教練有說他們覺得海底漫步很不值得 XD 但看大家笑得很開心就覺得超級值回票價，而且這個梯次的海底漫步只有我們~~無敵棒 \u0026#x1f970; 拍了幾張認證照之後我們就結束上半場了，上來的時候因為壓力轉換還會有點耳鳴跟頭暈，在回岸的路上教練還倒了麥茶給我們喝。之後走回到潛水店等下半場浮潛，很幸運地在出發前天空開始放晴了，而且還是藍天白雲大太陽的好天氣！在船上教練還說這可是睽違兩周能下水並進到青之洞窟的第一天呢，上兩周因為颱風所以完全不能玩水上活動，旅運大爆發。這次坐著快一點的船(應該就是快艇了XD)還配著蝦趴的音樂出海，船上滿滿的遊客在同梯次，不像海底漫步那時候爽爽的包船 \u0026#x1f923;，不一會便到了青之洞窟前。因為浮潛的深度跟海底漫步差很多，大概有二三十公尺深了，麻實在太害怕，不過拔也都一直待在旁邊，讚讚。第一次浮潛，往下看各式各樣的魚還有珊瑚礁，美不勝收！剛開始的時候還有點抓不到訣竅，不過後來就慢慢習慣了。這時段的遊客很多，游的時候還會互相踢到，但能下水並且進到青洞已經是很滿足的事了！結束前還餵了一次魚，教練也直接把防水相機丟給我們拍（雖然畫質很爛，所以青之洞窟裡的照片很多都超糊 \u0026#x1f605;）不過最重要的是能讓拔麻在 60 歲前留下難忘的浮潛回憶，這三千塊一個人的費用真的超值回票價的！結束浮潛後，上來用最迅速的速度洗澡，同梯次的日本人和中國女生竟然上來的時候頭髮是乾的，可見我們玩得多徹底\u0026#x1f606; 琉球の牛 中餐排了附近我期待已久的和牛燒烤啦～網路上說琉球の牛和第三天晚上要吃的阿古豬火鍋（食彩酒房）很難排，必須要先訂位，無意間在看評論的時候知道了信用卡秘書的服務，便在出發前一個禮拜使用，打得還稍微有點晚了，因為預約結果要等到三到五個工作天，幸虧出發兩天前收到訂位成功的通知 \u0026#x1f97a; 從浮潛店到餐廳只要車程五分鐘，一進店內要脫鞋，把鞋子放在特製的以木牌當鑰匙的木鞋櫃。吃了兩份午餐特餐、三份牛五花、一份牛舌；的確是有入口即化感，有機會要真的試試看 A5 和牛。一個人大概快五千日幣，讓拔請客，心滿意足 \u0026#x1f913; AEON MALL 下午的行程是 AEON MALL，不過果然跟想像中一樣，因為拔麻太累了就幾乎待在休息區休息，只有我跟二姊逛，但怕她們等太久就很快結束了。所以在這邊的戰利品就很少，只有在 Uniqlo 買外套、二姊買了雙鞋子，還喝了星巴克，就結束這回合了。 D2 晚餐 \u0026amp; 民宿 第二晚的住的地方在美國村，飯店就如同期待的一樣漂亮！幸好我們早早入住，還有機會看著太陽下山，從房間看出去的海景超漂亮，還跟姊一起拍了超多網美照 \u0026#x1f60e; 幸好當初在挑飯店的時候換了很多家，才又下定決心多花一點訂這間海景房！舒服的休息了一下，便出發去吃晚餐了～本來晚餐要安排在附近有名的迴轉壽司或拉麵，但都不巧的公休。變換成台灣也有的藏壽司，不過我們也都還沒吃過 \u0026#x1f44f; 從飯店走過去要 20 分鐘，美國村真的就像網路上說的一樣完全沒東西 \u0026#x1f923; 我們就慢慢散步過去，體驗一般日本的街道也是不錯。原本有特別在網路上訂位，但太悠閒地出發了，所以錯過時間，幸好不用等太久就換我們了。第一次玩有名的藏壽司扭蛋很好玩，雖然扭蛋只中一次。不過還是很開心，一起喝了冰涼的啤酒 \u0026#x1f37a;，一起乾杯 \u0026#x1f942; 回去地路上順道去了大樹藥妝店，逛了一下後先讓拔麻回飯店休息，我跟姊又多大買特買一兩小時 \u0026#x1f4b8; DAY3 第三天的行程比較悠閒，吃完飯店準備地超簡易早餐（可頌果汁咖啡）後就出發去中城城了。\n中城城 \u0026amp; 首里城 抵達後座接駁車到最高點，再讓遊客慢慢走下來。一早觀光地人潮不多，站在石牆上可以看到溪邊的中國南海和東邊的太平洋，景觀非常壯麗。而且石牆配上一片草地也是很好拍照的景點。\n緊接著的首里城也是沖繩必到的古蹟之一，但在這裡比較可惜的是我排的行程根本沒走完啊啊啊，實在是太熱了，而且早上飯店的早餐實在是太過簡便(主要是我跟姊太晚起床，拔麻又太早吃XD)，拔就說想吃午餐了，便草草結束這個景點去牧志市場跟國際通了。另外小插曲是我們九月才剛造訪首里城，十月就馬上有新聞說被燒毀，真的很可惜，也很慶幸我們有走過路過沒有錯過。 牧志市場 \u0026amp; 午餐 跟二姊都吃過一蘭拉麵，但拔麻沒吃過，在國際通邊邊找停車場停好車後便直直地往一蘭去。用自動販賣機點餐，一人吃一碗拉麵跟溫泉蛋，還是跟第一次在台灣吃的一樣好吃 \u0026#x1f60b; 吃飽後去逛市場跟國際通，吃了事先規劃好地塩屋冰淇淋，顧名思義就是把各式各樣的鹽撒在牛奶冰淇淋上配著吃，鹽的口味有番紅花、巧克力、抹茶等等，酷～ 波上宮 這個也是我期待已久的景點，不過抵達後麻因為肚子很痛(應該是吃了重口味一蘭又吃了冰可能還有點中暑，出去玩餐點也是得好好排的呢)，全家著急地找廁所 \u0026#x1f923; 還好有驚無險的渡過了。事先有先做功課按照教學的進行御手洗，在進殿門參拜，很迅速的就結束了。因為今天天氣太熱，跳過了幾個行程，所以結束得很早(我們實在太奢侈了XD早一個禮拜來的朋友哪兒都去不了)。時間才快三點，便直接去飯店辦理入住，還好可以直接 Check in。 D3 晚餐 \u0026amp; 飯店 第三間飯店因為坐落在那霸市區，所以規劃的時候也是猶豫了好久，儘管跟第二天的一樣貴，但是空間小很多，房外市景也很普，不過因為是連鎖的，所以整體來說還是很乾淨，麻也說不會過敏，睡得很舒服 \u0026#x1f44d; 因為接著沒有行程，所以在房間睡了覺，然後再去大廳喝個飯店免費提供的清酒跟小點心，過沒多久拔麻也一起下來。雖然當下不是在外頭觀光，但如此悠閒愜意的時光也是很享受的~ 約五點準備一下就再次出發去吃也是期待已久的阿古豬火鍋 \u0026#x1f37d;\u0026#xfe0f;\n食彩酒房其實在 google 評論裡的台灣評論不多，但每個評價都超高。而且阿古豬也是在沖繩必吃的美食之一，那時候在安排行程的時候因為這頓也是要一人五千日幣，傳在群組裡問要選和牛還是阿古豬，結果拔很霸氣的直接說都吃 \u0026#x1f389; 整間店內只有我們是台灣人，店員很仔細地介紹各種吃法，前半部幫忙涮肉、我們邊吃，桌邊服務超到位！前菜還有吃到到處看到的海葡萄，再加上一整盤肉跟一些青菜，拔姊跟我一人還配一杯 ORION 啤酒，就八九分飽了，等吃完肉之後店員會將剩下的湯煮成粥做完美的結尾。店員非常不浪我們的飯錢的把所有食物塞到我們的碗裡，真的是飽到頭頂 \u0026#x1f635; 總結來說真的好吃！肉看起來很多油花，但吃起來不會很容易膩，尤其搭配柚子醬油很清爽。最重要的是飽得很超值 \u0026#x1f974;\n吃飽後拔麻一樣回飯店休息，我跟二姊又再次出征到國際通購物，買了紀念磁鐵，其它因為在美國村買得夠多了，所以也沒有多買什麼。回程太累就坐計程車回飯店了，日本的計程車是自動門，開關門都是司機操控，很新奇。另外計程車司機真的不分國界，都超衝，還差點在一個路口撞上突然鑽出來的機車，幸好平平安安的！ DAY4 玉泉洞 早上吃飯店很豐盛又很貴的自助吧早餐後再悠閒地出發去玉泉洞。很幸運地趕上十點半的沖繩傳統太鼓表演，是免費的還很精采！超划算。洞穴裡的鐘乳石也很壯觀，走在裡面還會被水滴到，感覺鐘乳石依舊無時無刻的在生成長大中，不由得讚嘆大自然的鬼斧神工～大約走了半小時，出洞穴後點了兩碗剉冰消暑，再參觀王國村，終於體會到網路上說的商業感很重了，值得看的東西並不多，有印象的只有種滿各式各樣的水果跟植物（拔的最愛 XD）跟陳年蛇酒。 奧武島 \u0026amp; 午餐 因為奧武島就在旁邊，所以就把這個貓島之稱的景點排進去了，順道去吃個有名的天婦羅當午餐，點了幾項炸物和旁邊的兩碗沖繩麵。不過還是太想看炎熱的天氣，吃飽就啟程了，只看到兩三隻貓 \u0026#x1f602; 瀨長島 接著因為還很早，便決定到機場附近的 OUTLET 走走，不過這邊不樣 AEON 那樣，而是真的一間間一層樓的 OUTLET，沒有想買東西的我們走了一圈後就回到車上睡覺了，紮實的睡了一覺後就出發到瀨長島吃幸福鬆餅啦~\n瀨長島距離幾分鐘就到了，這裡真的是座很漂亮的小島，藍天白雲大海在配上全白的建築，有種異於日本的歐式風情。幸福鬆餅那時候再前兩周預約的時候還沒注意到我們要吃的是周末，還蓄勢待發的在電腦前就位搶，結果才發現周末不開放預約，太搞笑了 \u0026#x1f92a;\n趕緊登記後就在店外等候，幸好雖然來客很多，但大約半小時後就等到了室外的座位，我們點了水果跟錫蘭紅茶口味。前面幾口睛豔好吃，超軟超 Q 彈，口感非常特別，大家也覺得好吃 \u0026#x270c;\u0026#xfe0f; 還好遵守網路上的建議，只點了兩個不然吃到後面也會有點膩。 回程 吃飽喝足後便正式的結束了沖繩的行程，去附近還完車後便搭接駁車去機場了。首次嘗試自駕，沒有任何擦傷，能安全的結束，這是最棒、最值得感恩的事了。到機場還很早，決定再用飯糰結束這完美的行程！！但原來飯糰店就在我們報到(三樓)的正下方一樓的地方，我跟姊還大老遠的先走到右邊下樓在走回左邊，幸好回程時有恍然大悟哈哈哈哈。晚餐一人吃一個飯糰，拔吃山苦瓜、麻吃明太子、姊吃炸蝦、我吃島豆腐，順便把麻那邊換剩的一千塊日幣花完，我這邊換剩的日幣也僅剩幾百塊，算得超級剛好 \u0026#x1f919;\u0026#x1f919;\u0026#x1f919; 幸福的吃完之後就差不多去報到了，完美完美 \u0026#x1f44f;\n從六月知道要全家（少了剛去玩大阪回來的大姊 \u0026gt;\u0026lt;）一起出國玩，就用了很(上)多(班)時間排行程、看飯店。排完美的行程出來覺得成就感好大喔，看大家也都玩得很開心就覺得心滿意足。有記憶以來的全家出國玩是國小的香港迪士尼。之後爸媽努力工作、而我們求學；如今我們也都有工作了，爸媽也該到退休享福的年紀。希望以後能常常帶他們出國玩 \u0026#x1f496; 長愈大愈能感受家裡的好，和家人一起出去玩也是最最最好、最棒！\u0026#x1f4aa;\n","date":"2020-11-08T19:00:00+08:00","permalink":"http://localhost:1313/p/okinawa-2019/","title":"四天三夜沖繩遊記✨"}]