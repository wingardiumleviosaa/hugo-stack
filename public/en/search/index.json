[{"content":"TL; DR The current Kubernetes HA (High Availability) architecture on my lab utilizes Nginx as a Load Balancer. When Kubernetes is deployed by Kubespray, the API server endpoint can be configured to an external load balancer.\nArchitecture Overview In this setup, Nginx serves as the load balancer for API server requests, which distributes traffic across multiple master nodes to ensure high availability. This configuration requires modifications to the Kubespray deployment files to specify the load balancer details.\nConfiguration Locate the all.yml file in the sample inventory provided by Kubespray:\ninventory/sample/group_vars/all/all.yml\n1 2 3 4 5 ## External LB example config ## apiserver_loadbalancer_domain_name: \u0026#34;elb.some.domain\u0026#34; # loadbalancer_apiserver: # address: 1.2.3.4 # port: 1234 Therefore, you need to modify group_vars more before deploying ansible playbook.\n1 vi inventory/mycluster/group_vars/all/all.yml Uncomment the example configuration for the external load balancer and adjust the values to fit your setup. An example configuration is provided below:\n1 2 3 4 5 apiserver_loadbalancer_domain_name: \u0026#34;k8s.sdsp-dev.com\u0026#34; # Configure the Nginx IP address and forwarding port here loadbalancer_apiserver: address: 172.20.37.19 port: 6443 Additional information Nginx stream config for Kubernetes API Servers.\nReference https://www.youtube.com/watch?v=u_1f3WyvtQE ","date":"2024-06-10T11:36:21+08:00","permalink":"http://localhost:1313/en/p/k8s-kubespray-exteranl-loadbalancer/","title":"Kubespray Setup for External Load Balancer"},{"content":"Introduction In addition to Kafka Broker, the Kafka ecosystem also includes Kafka Connect, Kafka Bridge, Mirror Maker, etc. Setting up Kafka in Kubernetes through an Operator is easier than writing multiple manifests or installing multiple helm charts. This article selects Strimzi, which has the most stars in the Kafka Operator project on GitHub. The following table provides a list of resources that Stimzi can deploy.\nStrimzi Operator Deployment Best Practice Installing the Strimzi Operator in a different namespace from the Kafka Cluster and other Kafka components it manages to ensure clear separation of resources and configuration. A Kubernetes installs only a single Strimzi Operator to manage all Kafka instances. Updated Strimzi Operator and supported Kafka versions to reflect the latest features and enhancements. Operator Installation 1 2 kubectl create ns kafka kubectl create -f \u0026#39;https://strimzi.io/install/latest?namespace=kafka\u0026#39; -n kafka Kafka Cluster Deployment 1 2 3 4 5 wget https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.41.0/strimzi-0.41.0.tar.gz tar zxvf strimzi-0.41.0.tar.gz cd strimzi-0.41.0 cp examples/kafka/kraft/kafka.yaml . vi kafka.yaml Modify the deployment file. Deployment of Kafka cluster in KRaft mode requires the use of KafkaNodePool resources, so the yaml of the above two node pools are resources that must be deployed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: controller labels: strimzi.io/cluster: my-cluster spec: replicas: 3 roles: - controller storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi kraftMetadata: shared deleteClaim: false class: ceph-csi-rbd-hdd # Replace with existing storage class --- apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: broker labels: strimzi.io/cluster: my-cluster spec: replicas: 3 roles: - broker storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi kraftMetadata: shared deleteClaim: false class: ceph-csi-rbd-hdd # Replace with existing storage class --- apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata: name: my-cluster annotations: strimzi.io/node-pools: enabled strimzi.io/kraft: enabled spec: kafka: version: 3.7.1 metadataVersion: 3.7-IV4 listeners: - name: plain port: 9092 type: internal tls: false - name: external port: 9094 type: nodeport # Added external nodeport service tls: false configuration: bootstrap: nodePort: 32100 # Specify the nodeport occupied by bootstrap. If the broker is not specified one by one, the Operator will automatically assign it. brokers: - broker: 0 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8091 - broker: 1 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8092 - broker: 2 advertisedHost: kafka.sdsp-dev.com advertisedPort: 8093 config: offsets.topic.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 default.replication.factor: 3 min.insync.replicas: 2 entityOperator: topicOperator: {} userOperator: {} Start deployment\n1 kubectl -n kafka apply -f kafka.yaml View kafka and kafka node pool customized resource\nLoad Balance for Kafka The load forwarding rules of Nginx Load Balance must include the Node Ports of all Worker Nodes occupied by Bootstrap and Brokers.\nThe following is an example of setting up the Bootstrap port:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 upstream tcp9094 { server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; server 172.20.37.42:32100 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } Deployment Architecture Note\nWhen setting the external settings, you need to also specify the nodeport of each broker. Otherwise, when the client exists outside the network segment, you will encounter the error Disconnected from node 1 due to timeout.\nFor example:\n1 2 3 4 5 [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 2 disconnected. [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Cancelled in-flight API_VERSIONS request with correlation id 2 due to node 2 being disconnected (elapsed time since creation: 7ms, elapsed time since send: 7ms, request timeout: 3600000ms) [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 1 disconnected. [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Cancelled in-flight API_VERSIONS request with correlation id 3 due to node 1 being disconnected (elapsed time since creation: 4ms, elapsed time since send: 4ms, request timeout: 3600000ms) [kafka-admin-client-thread | flink-kafka-quickstart-enumerator-admin-client] INFO org.apache.kafka.clients.NetworkClient - [AdminClient clientId=flink-kafka-quickstart-enumerator-admin-client] Node 0 disconnected. Or another example:\n1 2 3 4 5 6 7 8 9 10 11 [2024-09-18 16:28:17,666] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 4 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 5 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 6 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,669] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 7 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:17,670] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 8 on topic-partition mocktest-0, retrying (2147483646 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:28:47,774] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 12 on topic-partition mocktest-0, retrying (2147483645 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) [2024-09-18 16:29:17,883] WARN [Producer clientId=perf-producer-client] Got error produce response with correlation id 15 on topic-partition mocktest-0, retrying (2147483644 attempts left). Error: REQUEST_TIMED_OUT. Error Message: Disconnected from node 1 due to timeout (org.apache.kafka.clients.producer.internals.Sender) org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation org.apache.kafka.common.errors.TimeoutException: Expiring 15 record(s) for mocktest-0:120000 ms has passed since batch creation ","date":"2024-06-01T12:45:10+08:00","permalink":"http://localhost:1313/en/p/k8s-strimzi-kafka-kraft-cluster/","title":"[Kubernetes] Install Kafka Kraft Cluster through Strizi"},{"content":"TL; DR This article records the use of bitnami helm chart to install kafka Kraft mode external cluster.\nPrepare values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 global: storageClass: ceph-csi-rbd-hdd heapOpts: \u0026#34;-Xmx6g -Xms6g\u0026#34; listeners: interbroker: name: INTERNAL containerPort: 9092 protocol: PLAINTEXT controller: name: CONTROLLER containerPort: 9093 protocol: PLAINTEXT client: name: CLIENT containerPort: 9095 protocol: PLAINTEXT external: containerPort: 9094 protocol: PLAINTEXT name: EXTERNAL controller: replicaCount: 3 persistence: size: 50Gi broker: replicaCount: 3 persistence: size: 300Gi externalAccess: enabled: true controller: forceExpose: false service: type: NodePort ports: external: 9094 nodePorts: - 30494 - 30594 - 30694 useHostIPs: true broker: service: type: NodePort ports: external: 9094 nodePorts: - 30194 - 30294 - 30394 useHostIPs: true volumePermissions: enabled: true rbac: create: true kraft: clusterId: M2VhY2Q3NGQ0NGYzNDg2YW Deploy 1 helm upgrade --install -name kafka bitnami/kafka --namespace kafka -f values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 Kafka can be accessed by consumers via port 9095 on the following DNS name from within your cluster: kafka.kafka.svc.cluster.local Each Kafka broker can be accessed by producers via port 9095 on the following DNS name(s) from within your cluster: kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-1.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9095 kafka-broker-0.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-1.kafka-broker-headless.kafka.svc.cluster.local:9095 kafka-broker-2.kafka-broker-headless.kafka.svc.cluster.local:9095 Set Kafka Load Balance You can see that both broker and controller are exposed its\u0026rsquo; service via NodePort after the deployment. And we can now set load balance for Kafka to provide the unified entrypoint to external client\nCurrently, an Nginx has been established for the cluster outside Kubernetes. We can use this server directly to also set up load forwarding for Kafka\u0026rsquo;s TCP traffic. The configuration file is as follows. All Worker Node nodes that will be opened to the Node Port are set up at once:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 upstream tcp9094 { server 172.20.37.36:30194 max_fails=3 fail_timeout=30s; server 172.20.37.36:30294 max_fails=3 fail_timeout=30s; server 172.20.37.36:30394 max_fails=3 fail_timeout=30s; server 172.20.37.36:30494 max_fails=3 fail_timeout=30s; server 172.20.37.36:30594 max_fails=3 fail_timeout=30s; server 172.20.37.36:30694 max_fails=3 fail_timeout=30s; server 172.20.37.37:30194 max_fails=3 fail_timeout=30s; server 172.20.37.37:30294 max_fails=3 fail_timeout=30s; server 172.20.37.37:30394 max_fails=3 fail_timeout=30s; server 172.20.37.37:30494 max_fails=3 fail_timeout=30s; server 172.20.37.37:30594 max_fails=3 fail_timeout=30s; server 172.20.37.37:30694 max_fails=3 fail_timeout=30s; server 172.20.37.38:30194 max_fails=3 fail_timeout=30s; server 172.20.37.38:30294 max_fails=3 fail_timeout=30s; server 172.20.37.38:30394 max_fails=3 fail_timeout=30s; server 172.20.37.38:30494 max_fails=3 fail_timeout=30s; server 172.20.37.38:30594 max_fails=3 fail_timeout=30s; server 172.20.37.38:30694 max_fails=3 fail_timeout=30s; server 172.20.37.39:30194 max_fails=3 fail_timeout=30s; server 172.20.37.39:30294 max_fails=3 fail_timeout=30s; server 172.20.37.39:30394 max_fails=3 fail_timeout=30s; server 172.20.37.39:30494 max_fails=3 fail_timeout=30s; server 172.20.37.39:30594 max_fails=3 fail_timeout=30s; server 172.20.37.39:30694 max_fails=3 fail_timeout=30s; server 172.20.37.40:30194 max_fails=3 fail_timeout=30s; server 172.20.37.40:30294 max_fails=3 fail_timeout=30s; server 172.20.37.40:30394 max_fails=3 fail_timeout=30s; server 172.20.37.40:30494 max_fails=3 fail_timeout=30s; server 172.20.37.40:30594 max_fails=3 fail_timeout=30s; server 172.20.37.40:30694 max_fails=3 fail_timeout=30s; server 172.20.37.41:30194 max_fails=3 fail_timeout=30s; server 172.20.37.41:30294 max_fails=3 fail_timeout=30s; server 172.20.37.41:30394 max_fails=3 fail_timeout=30s; server 172.20.37.41:30494 max_fails=3 fail_timeout=30s; server 172.20.37.41:30594 max_fails=3 fail_timeout=30s; server 172.20.37.41:30694 max_fails=3 fail_timeout=30s; server 172.20.37.42:30194 max_fails=3 fail_timeout=30s; server 172.20.37.42:30294 max_fails=3 fail_timeout=30s; server 172.20.37.42:30394 max_fails=3 fail_timeout=30s; server 172.20.37.42:30494 max_fails=3 fail_timeout=30s; server 172.20.37.42:30594 max_fails=3 fail_timeout=30s; server 172.20.37.42:30694 max_fails=3 fail_timeout=30s; } server { listen 9094; proxy_pass tcp9094; proxy_connect_timeout 300s; proxy_timeout 300s; } Deploy Kafka UI Prepare values.yaml file:\n1 2 3 4 5 6 7 8 9 10 11 yamlApplicationConfig: kafka: clusters: - name: platform bootstrapServers: kafka-broker-headless:9092 auth: type: disabled management: health: ldap: enabled: false And then deploy the UI through helm chart.\n1 helm install -name kafka-ui kafka-ui/kafka-ui -f values.yaml --namespace kafka ","date":"2024-06-01T11:35:00+08:00","permalink":"http://localhost:1313/en/p/k8s-bitnami-kafka-kraft-helm-chart/","title":"[Kubernetes] Install Kafka Kraft Cluster by Bitnami Helm Chart"},{"content":"TL; DR Official Definition:\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\nâ€• offical site Kubernetes, also known as k8s, was a project originally developed by Google using golang and then released. Used to operate automated containers, including deployment, scheduling, and scaling across node clusters.\nArchitecture The picture above shows a simple Kubernetes Cluster. Usually there are multiple Masters in a Cluster as backup, but for simplicity we only show one.\nOperation Method When the user creates a Pod through User Command (kubectl), after authenticating the user identity, the user passes the command to the API Server in the Master Node, and the API Server will back up the command to etcd. Next, the controller-manager will receive a message from the API Server that a new Pod needs to be created, and check if the resources are allowed, then create a new Pod. Finally, when the Scheduler regularly accesses the API Server, it will ask the controller-manager whether a new Pod has been created. If a newly created Pod is found, the Scheduler will be responsible for distributing the Pod to the most suitable Node.\nNodes and Components Master Node It is the console of the Kubernetes cluster, responsible for managing the cluster and coordinating all activities. It contains the following components:\nAPI Server API Server manages all API interfaces of Kubernetes and is used to communicate and operate with each node in the cluster.\nScheduler Scheduler is a Pods scheduler that monitors newly created Pods that have not yet been assigned a Worker Node to run on, and coordinates the most suitable object to be placed for the Pod based on the resources on each Node.\nController Manager The component responsible for managing and running the Kubernetes controller. The controller is a number of Processes responsible for monitoring the status of the Cluster. It can be divided into the following different types:\nNode controller - Responsible for notifying and responding to the status of nodes Replication controller - Responsible for maintaining the set number of Pods in each replication system End-Point controller - Responsible for service publishing of endpoints Service Account \u0026amp; Token controller - Responsible for creating service accounts and API access tokens for newly generated Namespaces etcd It is used to store Kubernetes Cluster data as a backup. When the Master fails for some reason, we can use etcd to help us restore the state of Kubernetes.\nWorker Node It is the runtime execution environment of Kubernetes and includes the following components:\nPod Pod is the smallest unit managed by Kubernetes, which contains one or more containers and can be regarded as the logical host of an application. Containers in the same Pod share the same resources and network, and communicate with each other through local port numbers. Pod runs on a private and isolated network. By default, it is visible in other pods and services in the same cluster, but is not visible to the outside. It needs to be exposed to the outside through services.\nKubelet Kubelet accepts commands from the API server to start pods and monitor status to ensure that all containers are running. It provides a heartbeat to the master node every few seconds. If the replication controller does not receive this message, the node is marked as unhealthy.\nKube Proxy Perform network connection forwarding, responsible for forwarding requests to the correct container.\nResource https://blog.sensu.io/how-kubernetes-works https://medium.com/@C.W.Hu/kubernetes-basic-concept-tutorial-e033e3504ec0 https://ithelp.ithome.com.tw/articles/10202135 ","date":"2022-09-07T21:13:00Z","permalink":"http://localhost:1313/en/p/kubernets-basic/","title":"Kubernetes Introduction"}]